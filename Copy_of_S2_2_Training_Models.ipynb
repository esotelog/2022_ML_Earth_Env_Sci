{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of S2_2_Training_Models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esotelog/2022_ML_Earth_Env_Sci/blob/main/Copy_of_S2_2_Training_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Chapter 4 â€“ Training Models**\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td align=middle>\n",
        "    <a target=\"_blank\" href=\"https://github.com/ageron/handson-ml2/blob/master/04_training_linear_models.ipynb\"> Open the original notebook <br><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "5Tt5C4PoIRl0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's begin like in the last notebook: importing a few common modules, ensuring MatplotLib plots figures inline and preparing a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so once again we strongly recommend you use Python 3 instead), as well as Scikit-Learn â‰¥0.20.\n",
        "\n",
        "You don't need to worry about understanding everything that is written in this section."
      ],
      "metadata": {
        "id": "8HQ31GpXuKr-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "S_OXSp49IOF2"
      },
      "outputs": [],
      "source": [
        "# Python â‰¥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Is this notebook running on Colab or Kaggle?\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "# Scikit-Learn â‰¥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# To make this notebook's output stable across runs\n",
        "rnd_seed = 42\n",
        "rnd_gen = np.random.default_rng(rnd_seed)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"classification\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we will be working with the [*Iris Flower Dataset*](https://en.wikipedia.org/wiki/Iris_flower_data_set), in which the length and width of both the sepals and petals of three types of Iris flowes were recorded. For reference, these are pictures of the three flowers: <br>\n",
        "\n",
        "<center> In order: Iris Setosa,  Iris Versicolor, and Iris Virginica </center>\n",
        "\n",
        "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/5/56/Kosaciec_szczecinkowaty_Iris_setosa.jpg/360px-Kosaciec_szczecinkowaty_Iris_setosa.jpg' height=300 >\n",
        "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Iris_versicolor_3.jpg/640px-Iris_versicolor_3.jpg' height=300></img>\n",
        "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Iris_virginica.jpg/590px-Iris_virginica.jpg' height=300></img>\n",
        "\n",
        "Photo Credits:[Kosaciec szczecinkowaty Iris setosa](https://en.wikipedia.org/wiki/File:Kosaciec_szczecinkowaty_Iris_setosa.jpg) by [Radomil Binek](https://commons.wikimedia.org/wiki/User:Radomil) licensed under [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/deed.en); [Blue flag flower close-up (Iris versicolor)](https://en.wikipedia.org/wiki/File:Iris_versicolor_3.jpg)by Danielle Langlois licensed under [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/deed.en); [image of Iris virginica shrevei](https://en.wikipedia.org/wiki/File:Iris_virginica.jpg) by [Frank Mayfield](https://www.flickr.com/photos/33397993@N05) licensed under [CC BY-SA 2.0](https://creativecommons.org/licenses/by-sa/2.0/deed.en).\n",
        "<br><br>\n",
        "\n",
        "As you can imagine, this dataset is normally used to train *multiclass*/*multinomial* classification algorithms and not *binary* classification algorithms, since there *are* more than 2 classes. \n",
        "\n",
        "\"*Three classes, even!*\" - an observant TA\n",
        "\n",
        "For this exercise, however, we will be implemented the binary classification algorithm referred to as the *logistic regression* algorithm (also called logit regression)."
      ],
      "metadata": {
        "id": "wKsvLXdmzqD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's load the Iris Dataset\n",
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# Print out some information about the data\n",
        "print(f'Keys in Iris dictionary: \\n{list(iris.keys())}\\n\\n')\n",
        "print(iris.DESCR)\n",
        "\n",
        "# And load the petal lengths and widths as our input data\n",
        "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
        "y = iris[\"target\"]\n",
        "\n",
        "print(iris['data_module'])\n",
        "\n",
        "# The target data labels Setosa as 0, Versicolor as 1, and Virginica as 2. For \n",
        "# this exercise we will be using only the Versicolor and Virgina sets.\n",
        "bin_indices = np.logical_or(y==1,y==2)\n",
        "bin_X = X[bin_indices]\n",
        "bin_y = (y[bin_indices]==2).astype(np.uint8) # convert to binary"
      ],
      "metadata": {
        "id": "emWru72owjEI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc7ce4ad-2db5-42fd-c363-70c2db20add7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys in Iris dictionary: \n",
            "['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module']\n",
            "\n",
            "\n",
            ".. _iris_dataset:\n",
            "\n",
            "Iris plants dataset\n",
            "--------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 150 (50 in each of three classes)\n",
            "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
            "    :Attribute Information:\n",
            "        - sepal length in cm\n",
            "        - sepal width in cm\n",
            "        - petal length in cm\n",
            "        - petal width in cm\n",
            "        - class:\n",
            "                - Iris-Setosa\n",
            "                - Iris-Versicolour\n",
            "                - Iris-Virginica\n",
            "                \n",
            "    :Summary Statistics:\n",
            "\n",
            "    ============== ==== ==== ======= ===== ====================\n",
            "                    Min  Max   Mean    SD   Class Correlation\n",
            "    ============== ==== ==== ======= ===== ====================\n",
            "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
            "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
            "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
            "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
            "    ============== ==== ==== ======= ===== ====================\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "    :Class Distribution: 33.3% for each of 3 classes.\n",
            "    :Creator: R.A. Fisher\n",
            "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
            "    :Date: July, 1988\n",
            "\n",
            "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
            "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
            "Machine Learning Repository, which has two wrong data points.\n",
            "\n",
            "This is perhaps the best known database to be found in the\n",
            "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
            "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
            "data set contains 3 classes of 50 instances each, where each class refers to a\n",
            "type of iris plant.  One class is linearly separable from the other 2; the\n",
            "latter are NOT linearly separable from each other.\n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
            "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
            "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
            "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
            "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
            "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
            "     Structure and Classification Rule for Recognition in Partially Exposed\n",
            "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
            "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
            "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
            "     on Information Theory, May 1972, 431-433.\n",
            "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
            "     conceptual clustering system finds 3 classes in the data.\n",
            "   - Many, many more ...\n",
            "sklearn.datasets.data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bin_X.shape\n",
        "bin_y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0Bef___oNXK",
        "outputId": "4f81c4b7-98b4-4529-a43d-ed88ad52f3f2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have a set of binary classification data we can use to train an algorithm.\n",
        "\n",
        "As we saw during our reading, we need to define three things in order to train our algorithm: the type of algorithm we will train, the cost function (which will tell us how close our prediction is to the truth), and a method for updating the parameters in our model according to the value of the cost function (e.g., the gradient descent method). \n",
        "\n",
        "Let's begin by defining the type of algorithm we will use. We will train a logistic regression model to differentiate between two classes. A reminder of how the logistic regression algorithm works is given below.\n",
        "<br><br><br>\n",
        "The logistic regression algorithm will thus take an input $t$ that is a linear combination of the features:\n",
        "\n",
        "<a name=\"logit\"></a>\n",
        "\n",
        "<center> $t_{\\small{n}} = \\beta_{\\small{0}} + \\beta_{\\small{1}} \\cdot X_{1,n} + \\beta_{\\small{2}} \\cdot X_{2,n}$ </center>\n",
        "\n",
        "where \n",
        "* $n$ is the ID of the sample \n",
        "* $X_{\\small{0}}$ represents the petal length\n",
        "* $X_{\\small{1}}$ represents the petal width\n",
        "\n",
        "This input is then fed into the logistic function, $\\sigma$:\n",
        "\\begin{align} \n",
        "\\sigma: t\\mapsto \\dfrac{1}{1+e^ {-t}}\n",
        "\\end{align}\n",
        "\n",
        "Let's plot it below to remember the shape of the function"
      ],
      "metadata": {
        "id": "jvNBaOWZ9fXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = np.arange(-4,4,.1)\n",
        "def logistic(in_val):\n",
        "    # Return the value of the logistic function\n",
        "    return 1/(1 + np.exp(- in_val))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.axvline(0, c='black', alpha=1)\n",
        "ax.axhline(0, c='black', alpha=1)\n",
        "\n",
        "[ax.axhline(y_val, c='black', alpha=0.5, linestyle='dotted') for y_val in (0.5,1)]\n",
        "\n",
        "plt.autoscale(axis='x', tight=True)\n",
        "\n",
        "ax.plot(t, logistic(t));\n",
        "ax.set_xlabel('$t$')\n",
        "ax.set_ylabel('$\\\\sigma\\\\  \\\\left(t\\\\right)$')\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "lgt9dI6b9Zwa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "367bbdb7-1852-49fd-a737-41054ee553d2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the logistic function, we define inputs resulting in $\\sigma\\geq.5$ as belonging to the ***one*** class, and any value below that is considered to belong to the ***zero*** class.\n",
        "\n",
        "We now have a function which lets us map the value of the petal length and width to the class to which the observation belongs (i.e., whether the length and width correspond to Iris Versicolor or Iris Virginica). However, there is a parameter vector **$\\theta$** with a number of parameters that we do not have a value for: <br> $\\theta = [ \\beta_{\\small{0}}, \\beta_{\\small{1}}$, $\\beta_{\\small{2}} ]$\n",
        "\n",
        "**Q1) Set up an array of random numbers between 0 and 1 representing the $\\theta$ vector.**\n",
        "\n",
        "Hint:  Use `rnd_gen`! If you're not sure how to use it, consult the `default_rng` documentation [at this link](https://numpy.org/doc/stable/reference/random/generator.html). For instance, you may use the `random` method of `rnd_gen`."
      ],
      "metadata": {
        "id": "0Ll1PKpjxqLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "rng = np.random.default_rng()\n",
        "theta_init = rng.random( size=3)\n",
        "print(theta_init)"
      ],
      "metadata": {
        "id": "-Vk05y1C2VBs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c9ad944-cac3-48a4-d160-203e73940962"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.20693996 0.43319825 0.30702669]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to determine whether a set of $\\beta$ values is better than the other, we need to quantify well the values are able to predict the class. This is where the cost function comes in.\n",
        "\n",
        "The cost function, $c$, will return a value close to zero when the prediction, $\\hat{p}$, is correct and a large value when it is wrong. In a binary classification problem, we can use the log loss function. For a single prediction and truth value, it is given by:\n",
        "\\begin{align}\n",
        "        \\text{c}(\\hat{p},y) = \\left\\{\n",
        "        \\begin{array}{cl}\n",
        "        -\\log(\\hat{p})& \\text{if}\\; y=1\\\\\n",
        "        -\\log(1-\\hat{p}) & \\text{if}\\; y=0\n",
        "        \\end{array}\n",
        "        \\right.\n",
        "    \\end{align}\n",
        "\n",
        "However, we want to apply the cost function to an n-dimensional set of predictions and truth values. Thankfully, we can find the average value of the log loss function $J$ for an an-dimensional set of $\\hat{y}$ & $y$ as follows:\n",
        "\n",
        "\\begin{align}\n",
        "        \\text{J}(\\mathbf{\\hat{p}},y) = - \\dfrac{1}{n} \\sum_{i=1}^{n} \n",
        "        \\left[ y_i\\cdot \\log\\left( \\hat{p}_i \\right) \\right] + \n",
        "        \\left[ \\left( 1 - y_i \\right) \\cdot \\log\\left( 1-\\hat{p}_i \\right) \\right]\n",
        "    \\end{align}\n",
        "\n",
        "We now have a formula that can be used to calculate the average cost over the training set of data.\n",
        "\n",
        "**Q2) Define a log_loss function that takes in an arbitrarily large set of prediction and truths**\n",
        "\n",
        "Hint 1: You need to encode the function $J$ above, for which Numpy's functions may be quite convenient (e.g., [`log`](https://numpy.org/doc/stable/reference/generated/numpy.log.html), [`mean`](https://numpy.org/doc/stable/reference/generated/numpy.mean.html), etc.)\n",
        "\n",
        "Hint 2: Asserting the dimensions of the vector is a good way to check that your function is working correctly. [Here's a tutorial on how to use `assert`](https://swcarpentry.github.io/python-novice-inflammation/10-defensive/index.html#assertions). For instance, to assert that two vectors `X` and `Y` have the same dimension, you may use:\n",
        "```\n",
        "assert X.shape==Y.shape\n",
        "```"
      ],
      "metadata": {
        "id": "s8KM_CeF2Ven"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's code ðŸ’»"
      ],
      "metadata": {
        "id": "XBLxwlSWMoo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_loss(p_hat, y, epsilon=1e-7):\n",
        "  # Write your code here. \n",
        "  # We can also run into problems if p_hat = 0, so add an _epsilon_ term\n",
        "  # when evaluating log(p_hat).\n",
        "  log_p_hat =  np.log10(p_hat) +epsilon\n",
        "  n=y.shape[0]\n",
        "  Jarray=y*log_p_hat + (1-y)*np.log10(1-p_hat)\n",
        "  J = -np.sum(Jarray)*1/n\n",
        "\n",
        "  # After calculating J, assert that J has the same shape as p_hat and y\n",
        "  assert Jarray.shape==p_hat.shape\n",
        "  assert Jarray.shape==y.shape\n",
        "\n",
        "  return J"
      ],
      "metadata": {
        "id": "H5fDeL36EauO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have a way of quantifying how good our predictions are. The final thing needed for us to train our algorithm is figuring out a way to update the parameters in a way that improves the average quality of our predictions. \n",
        "\n",
        "<br><br>**Warning**: we'll go into a bit of math below <br><br>\n",
        "\n",
        "Let's look at the change in a single parameter within $\\theta$: $\\beta_1$ (given $X_{1,i} = X_1$, $\\;\\hat{p}_{i} = \\hat{p}$, $\\;y_{i} = y$). If we want to know what the effect of changing the value of $\\beta_1$ will have on the log loss function we can find this with the partial derivative:\n",
        "<center>$\n",
        "        \\dfrac{\\partial J}{\\partial \\beta_1}\n",
        "$</center>\n",
        "\n",
        "This may not seem very helpful by itself - after all, $\\beta_1$ isn't even in the expression of $J$. But if we use the chain rule, we can rewrite the expression as:\n",
        "<center>\n",
        "        $\\dfrac{\\partial J}{\\partial \\hat{p}} \\cdot\n",
        "        \\dfrac{\\partial \\hat{p}}{\\partial \\theta} \\cdot\n",
        "        \\dfrac{\\partial \\theta}{\\partial \\beta_1}$\n",
        "</center>\n",
        "\n",
        "We'll spare you the math (feel free to verify it youself, however!):\n",
        "\n",
        "<center>$\\dfrac{\\partial J}{\\partial \\hat{p}} =  \\dfrac{\\hat{p} - y}{\\hat{p}(1-\\hat{p})}, \\quad\n",
        "        \\dfrac{\\partial \\hat{p}}{\\partial \\theta} = \\hat{p} (1-\\hat{p}), \\quad\n",
        "        \\dfrac{\\partial \\theta}{\\partial \\beta_1} = X_1 $\n",
        "</center>\n",
        "\n",
        "and thus \n",
        "<center>$\n",
        "        \\dfrac{\\partial J}{\\partial \\beta_1} = (\\hat{p} - y) \\cdot X_1\n",
        "$</center>\n",
        "\n",
        "We can calculate the partial derivative for each parameter in $\\theta$ which, as you may have realized, is simply the $\\theta$ gradient of $J$: $\\nabla_{\\theta}(J)$\n",
        "\n",
        "With all of this information, we can now write $\\nabla_{\\theta} J$ in terms of the error, the feature vector, and the number of samples we're training on!\n",
        "\n",
        "<a name=\"grad_eq\"></a>\n",
        "\n",
        "<center>$\\nabla_{\\mathbf{\\theta}^{(k)}} \\, J(\\mathbf{\\theta^{(k)}}) = \\dfrac{1}{n} \\sum\\limits_{i=1}^{n}{ \\left ( \\hat{p}^{(k)}_{i} - y_{i} \\right ) \\mathbf{X}_{i}}$</center>\n",
        "\n",
        "Note that here $k$ represents the iteration of the parameters we are currently on.\n",
        "\n",
        "We now have a gradient we can calculate and use in the batch gradient descent method! The updated parameters will thus be:\n",
        "\n",
        "<a name=\"grad_descent\"></a>\n",
        "\n",
        "\\begin{align} \n",
        "{\\mathbf{\\theta}^{(k+1)}} = {\\mathbf{\\theta}^{(k)}} - \\eta\\,\\nabla_{\\theta^{(k)}}J(\\theta^{(k)})\n",
        "\\end{align}\n",
        "\n",
        "Where $\\eta$ is the learning rate parameter. It's also worth pointing out that $\\;\\hat{p}^{(k)}_i = \\sigma\\left(\\theta^{(k)}, X_i\\right) $"
      ],
      "metadata": {
        "id": "aO4Bkm1gFV3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to easily calculate the input to the logistic regression, we'll multiply the $\\theta$ vector with the X data, and as we have a non-zero bias  $\\beta_0$ we'd like to have an X matrix whose first column is filled with ones.\n",
        "\n",
        "\\begin{align}\n",
        "    X_{\\small{with\\ bias}} = \\begin{pmatrix}\n",
        "        1 & X_{1,0} & X_{2,0}\\\\\n",
        "        1 & X_{1,1} & X_{2,1}\\\\\n",
        "        &...&\\\\\n",
        "        1 & X_{1,n} & X_{2,n} \n",
        "        \\end{pmatrix}\n",
        "\\end{align}\n",
        "<br>\n",
        "**Q3) Prepare the `X_with_bias` matrix (remember to use the `bin_X` data and not just `X`). Write a function called `predict` that takes in the parameter vector $\\theta$ and the `X_with_bias` matrix and evaluates the logistic function for each of the samples.**\n",
        "\n",
        "Hint 1: You recently learned how to initialize arrays in the `Numpy` notebook [at this link](https://nbviewer.org/github/tbeucler/2022_ML_Earth_Env_Sci/blob/main/Lab_Notebooks/S1_2_Numpy.ipynb). There are many ways to add a columns of 1 to `bin_X`, for instance using [`np.concatenate`](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html) or [`np.append`](https://numpy.org/doc/stable/reference/generated/numpy.append.html).\n",
        "\n",
        "Hint 2:  To clarify, the function `predict` calculates $\\hat{p}$ from $\\beta$ and $\\boldsymbol{X}$.\n",
        "\n",
        "Hint 3: In practice, to calculate the logistic function for each sample, you may follow the equations [higher up in the notebook](#logit) and (1) calculate $t$ from $\\beta$ and $\\boldsymbol{X_{\\mathrm{with\\ bias}}}$ before (2) applying the logistic function $\\sigma$ to $t$. "
      ],
      "metadata": {
        "id": "ML4uik7sbdMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the X_with_bias matrix\n",
        "ones=np.ones(bin_X.shape[0])\n",
        "X_with_bias= np.concatenate(( np.transpose([ones]),bin_X), axis=1)"
      ],
      "metadata": {
        "id": "3b2oOJ5WKn5m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your function predict here\n",
        "def predict(X,theta_model):\n",
        "  t=np.sum(theta_model*X,axis=1)\n",
        "  p_hat= logistic(t)\n",
        "  y_hat= p_hat>=0.5\n",
        "  return y_hat.astype(int)\n",
        "\n"
      ],
      "metadata": {
        "id": "tBLryApsbatR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4) Now that you have a `predict` function, write a `gradient_calc` function that calculates the gradient for the logistic function.**\n",
        "\n",
        "Hint 1: You'll have to feed `theta`, `X`, and `y` to the `gradient_calc` function.\n",
        "\n",
        "Hint 2: You can use [this equation](#grad_eq) to calculate the gradient of the cost function."
      ],
      "metadata": {
        "id": "p6cPbu4LvVES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "def gradient_calc(p_hat,y,X):\n",
        "  n=y.size\n",
        "  grad_matrix=(p_hat-y)*np.transpose(X)\n",
        "  gradJ=1/n*np.sum(np.transpose(grad_matrix), axis=0)\n",
        "  return gradJ\n"
      ],
      "metadata": {
        "id": "BtnANN5WvVuy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now write a function that will train a logistic regression algorithm!\n",
        "\n",
        "Your `logistic_regression` function needs to:\n",
        "* Take in a set of training input/output data, validation input/output data, a number of iterations to train for, a set of initial parameters $\\theta$, and a learning rate $\\eta$\n",
        "* At each iteration:\n",
        " * Generate a set of predictions on the training data. Hint: You may use your function `predict` on inputs `X_train` from the training set.\n",
        " * Calculate and store the loss function for the training data at each iteration. Hint: You may use your function `log_loss` on inputs `X_train` and outputs `y_train` from the training set.\n",
        " * Calculate the gradient. Hint: You may use your function `grad_calc`.\n",
        " * Update the $\\theta$ parameters. Hint: You need to implement [this equation](#grad_descent).\n",
        " * Generate a set of predictions on the validation data using the updated parameters. Hint: You may use your function `predict` on inputs `X_valid` from the validation set. \n",
        " * Calculate and store the loss function for the validation data. Hint: You may use your function `log_loss` on inputs `X_valid` and outputs `y_valid` from the validation set. \n",
        " * Bonus: Calculate and store the accuracy of the model on the training and validation data as a metric!\n",
        "* Return the final set of parameters $\\theta$ & the stored training/validation loss function values (and the accuracy, if you did the bonus)\n",
        "\n",
        "**Q5) Write the `logistic_regression` function**"
      ],
      "metadata": {
        "id": "PU4A5HVKuAGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "def logistic_regression(eta,theta,max_iter,X_train,y_train,X_test,y_test):\n",
        "\n",
        "  #intializing variables\n",
        "  tol=1e-8\n",
        "  n_iter=1\n",
        "\n",
        "  #training set\n",
        "  t=np.sum(theta*X_train,axis=1)\n",
        "  p_hat= logistic(t)\n",
        "  loss=log_loss(p_hat, y_train, epsilon=1e-7)\n",
        "  lossar0=np.array(loss) \n",
        "\n",
        "  #test_set\n",
        "  t_test=np.sum(theta*X_test,axis=1)\n",
        "  p_hat_test= logistic(t_test)\n",
        "  loss_test=log_loss(p_hat_test, y_test, epsilon=1e-7)\n",
        "  lossar1=np.array(loss_test) \n",
        "\n",
        "#train\n",
        "  while ( n_iter < max_iter and loss>tol ):\n",
        "    n_iter =n_iter+1\n",
        "    print('iteration',n_iter)\n",
        "    #calculating the gradient\n",
        "    gradJ=gradient_calc(p_hat,y_train,X_train)\n",
        "    #new theta values\n",
        "    theta=theta -eta*gradJ\n",
        "    #evaluating p_hat\n",
        "    t=np.sum(theta*X_train,axis=1)\n",
        "    p_hat= logistic(t)   \n",
        "    #new loss function\n",
        "    loss=log_loss(p_hat, y_train, epsilon=1e-7)    \n",
        "    print('loss_train',loss)\n",
        "    lossar0 =np.append(lossar0,loss)\n",
        "\n",
        "    #Evaluate the test set\n",
        "    t_test=np.sum(theta*X_test,axis=1)\n",
        "    p_hat_test=logistic(t_test)\n",
        "    loss_test=log_loss(p_hat_test, y_test, epsilon=1e-7) \n",
        "    lossar1 =np.append(lossar1,loss_test)\n",
        "\n",
        "  return np.concatenate(([lossar0],[lossar1]), axis=0), theta\n"
      ],
      "metadata": {
        "id": "HDsR5TxPt-0Y"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Â¡Â¡Â¡Important Note!!!**\n",
        "\n",
        "The notebook assumes that you will return \n",
        "1. a Losses list, where Losses[0] is the training loss and Losses[1] is the validation loss\n",
        "2. a tuple with the 3 final coefficients ($\\beta_0$, $\\beta_1$, $\\beta_2$)\n",
        "\n",
        "The code for visualizing the bonus accuracy is not included - but it should be simple enough to do in a way similar to that which is done with the losses.\n",
        "\n",
        "---------------------"
      ],
      "metadata": {
        "id": "EWMDLk7wFB0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our logistic regression function, we're all set to train our algorithm! Or are we?\n",
        "\n",
        "There's an important data step that we've neglected up to this point - we need to split the data into the train, validation, and test datasets."
      ],
      "metadata": {
        "id": "2ep5FQYBmqG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_ratio = 0.2\n",
        "validation_ratio = 0.2\n",
        "total_size = len(X_with_bias)\n",
        "\n",
        "test_size = int(total_size * test_ratio)\n",
        "validation_size = int(total_size * validation_ratio)\n",
        "train_size = total_size - test_size - validation_size\n",
        "\n",
        "rnd_indices = rnd_gen.permutation(total_size)\n",
        "\n",
        "X_train = X_with_bias[rnd_indices[:train_size]]\n",
        "y_train = bin_y[rnd_indices[:train_size]]\n",
        "X_valid = X_with_bias[rnd_indices[train_size:-test_size]]\n",
        "y_valid = bin_y[rnd_indices[train_size:-test_size]]\n",
        "X_test = X_with_bias[rnd_indices[-test_size:]]\n",
        "y_test = bin_y[rnd_indices[-test_size:]]"
      ],
      "metadata": {
        "id": "CVrXzjYA2iil"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we're ready! \n",
        "\n",
        "**Q6) Train your logistic regression algorithm. Use 5000 iterations, $\\eta$=0.1**\n",
        "\n",
        "Hint: It's time to use the `logistic_regression` function you defined in Q5. "
      ],
      "metadata": {
        "id": "33IhRpME8LOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete the code\n",
        "max_iter=5000\n",
        "eta=0.1\n",
        "theta=theta_init\n",
        "\n",
        "losses, coeffs =  logistic_regression(eta,theta,max_iter,X_train,y_train,X_test,y_test)"
      ],
      "metadata": {
        "id": "dWAr0ORYEYi2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3169f810-495f-4b67-e552-13d80ba1500c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "iteration 2501\n",
            "loss_train 0.1320256000226267\n",
            "iteration 2502\n",
            "loss_train 0.13200902990066943\n",
            "iteration 2503\n",
            "loss_train 0.1319924684054566\n",
            "iteration 2504\n",
            "loss_train 0.13197591552956867\n",
            "iteration 2505\n",
            "loss_train 0.13195937126559518\n",
            "iteration 2506\n",
            "loss_train 0.13194283560613465\n",
            "iteration 2507\n",
            "loss_train 0.1319263085437948\n",
            "iteration 2508\n",
            "loss_train 0.1319097900711924\n",
            "iteration 2509\n",
            "loss_train 0.1318932801809532\n",
            "iteration 2510\n",
            "loss_train 0.13187677886571206\n",
            "iteration 2511\n",
            "loss_train 0.13186028611811274\n",
            "iteration 2512\n",
            "loss_train 0.13184380193080816\n",
            "iteration 2513\n",
            "loss_train 0.13182732629646007\n",
            "iteration 2514\n",
            "loss_train 0.1318108592077393\n",
            "iteration 2515\n",
            "loss_train 0.13179440065732567\n",
            "iteration 2516\n",
            "loss_train 0.13177795063790776\n",
            "iteration 2517\n",
            "loss_train 0.13176150914218332\n",
            "iteration 2518\n",
            "loss_train 0.13174507616285883\n",
            "iteration 2519\n",
            "loss_train 0.13172865169264986\n",
            "iteration 2520\n",
            "loss_train 0.1317122357242806\n",
            "iteration 2521\n",
            "loss_train 0.1316958282504844\n",
            "iteration 2522\n",
            "loss_train 0.13167942926400325\n",
            "iteration 2523\n",
            "loss_train 0.13166303875758814\n",
            "iteration 2524\n",
            "loss_train 0.13164665672399878\n",
            "iteration 2525\n",
            "loss_train 0.1316302831560038\n",
            "iteration 2526\n",
            "loss_train 0.13161391804638056\n",
            "iteration 2527\n",
            "loss_train 0.13159756138791526\n",
            "iteration 2528\n",
            "loss_train 0.13158121317340277\n",
            "iteration 2529\n",
            "loss_train 0.13156487339564688\n",
            "iteration 2530\n",
            "loss_train 0.13154854204746003\n",
            "iteration 2531\n",
            "loss_train 0.13153221912166338\n",
            "iteration 2532\n",
            "loss_train 0.1315159046110869\n",
            "iteration 2533\n",
            "loss_train 0.1314995985085691\n",
            "iteration 2534\n",
            "loss_train 0.13148330080695744\n",
            "iteration 2535\n",
            "loss_train 0.13146701149910783\n",
            "iteration 2536\n",
            "loss_train 0.13145073057788498\n",
            "iteration 2537\n",
            "loss_train 0.1314344580361621\n",
            "iteration 2538\n",
            "loss_train 0.13141819386682124\n",
            "iteration 2539\n",
            "loss_train 0.131401938062753\n",
            "iteration 2540\n",
            "loss_train 0.1313856906168564\n",
            "iteration 2541\n",
            "loss_train 0.13136945152203938\n",
            "iteration 2542\n",
            "loss_train 0.13135322077121828\n",
            "iteration 2543\n",
            "loss_train 0.13133699835731794\n",
            "iteration 2544\n",
            "loss_train 0.13132078427327196\n",
            "iteration 2545\n",
            "loss_train 0.13130457851202232\n",
            "iteration 2546\n",
            "loss_train 0.1312883810665196\n",
            "iteration 2547\n",
            "loss_train 0.13127219192972284\n",
            "iteration 2548\n",
            "loss_train 0.1312560110945997\n",
            "iteration 2549\n",
            "loss_train 0.13123983855412616\n",
            "iteration 2550\n",
            "loss_train 0.1312236743012869\n",
            "iteration 2551\n",
            "loss_train 0.1312075183290748\n",
            "iteration 2552\n",
            "loss_train 0.13119137063049133\n",
            "iteration 2553\n",
            "loss_train 0.13117523119854646\n",
            "iteration 2554\n",
            "loss_train 0.13115910002625847\n",
            "iteration 2555\n",
            "loss_train 0.13114297710665407\n",
            "iteration 2556\n",
            "loss_train 0.13112686243276836\n",
            "iteration 2557\n",
            "loss_train 0.13111075599764493\n",
            "iteration 2558\n",
            "loss_train 0.1310946577943356\n",
            "iteration 2559\n",
            "loss_train 0.1310785678159006\n",
            "iteration 2560\n",
            "loss_train 0.13106248605540852\n",
            "iteration 2561\n",
            "loss_train 0.1310464125059362\n",
            "iteration 2562\n",
            "loss_train 0.13103034716056902\n",
            "iteration 2563\n",
            "loss_train 0.13101429001240036\n",
            "iteration 2564\n",
            "loss_train 0.1309982410545321\n",
            "iteration 2565\n",
            "loss_train 0.1309822002800743\n",
            "iteration 2566\n",
            "loss_train 0.13096616768214533\n",
            "iteration 2567\n",
            "loss_train 0.13095014325387183\n",
            "iteration 2568\n",
            "loss_train 0.13093412698838863\n",
            "iteration 2569\n",
            "loss_train 0.13091811887883886\n",
            "iteration 2570\n",
            "loss_train 0.13090211891837372\n",
            "iteration 2571\n",
            "loss_train 0.13088612710015277\n",
            "iteration 2572\n",
            "loss_train 0.13087014341734368\n",
            "iteration 2573\n",
            "loss_train 0.13085416786312234\n",
            "iteration 2574\n",
            "loss_train 0.13083820043067268\n",
            "iteration 2575\n",
            "loss_train 0.130822241113187\n",
            "iteration 2576\n",
            "loss_train 0.13080628990386542\n",
            "iteration 2577\n",
            "loss_train 0.13079034679591653\n",
            "iteration 2578\n",
            "loss_train 0.13077441178255683\n",
            "iteration 2579\n",
            "loss_train 0.13075848485701094\n",
            "iteration 2580\n",
            "loss_train 0.13074256601251158\n",
            "iteration 2581\n",
            "loss_train 0.13072665524229957\n",
            "iteration 2582\n",
            "loss_train 0.13071075253962375\n",
            "iteration 2583\n",
            "loss_train 0.13069485789774105\n",
            "iteration 2584\n",
            "loss_train 0.13067897130991643\n",
            "iteration 2585\n",
            "loss_train 0.13066309276942278\n",
            "iteration 2586\n",
            "loss_train 0.1306472222695412\n",
            "iteration 2587\n",
            "loss_train 0.13063135980356055\n",
            "iteration 2588\n",
            "loss_train 0.1306155053647779\n",
            "iteration 2589\n",
            "loss_train 0.13059965894649814\n",
            "iteration 2590\n",
            "loss_train 0.13058382054203413\n",
            "iteration 2591\n",
            "loss_train 0.13056799014470674\n",
            "iteration 2592\n",
            "loss_train 0.13055216774784476\n",
            "iteration 2593\n",
            "loss_train 0.13053635334478494\n",
            "iteration 2594\n",
            "loss_train 0.13052054692887188\n",
            "iteration 2595\n",
            "loss_train 0.13050474849345808\n",
            "iteration 2596\n",
            "loss_train 0.1304889580319039\n",
            "iteration 2597\n",
            "loss_train 0.13047317553757776\n",
            "iteration 2598\n",
            "loss_train 0.1304574010038557\n",
            "iteration 2599\n",
            "loss_train 0.13044163442412174\n",
            "iteration 2600\n",
            "loss_train 0.13042587579176768\n",
            "iteration 2601\n",
            "loss_train 0.1304101251001932\n",
            "iteration 2602\n",
            "loss_train 0.13039438234280587\n",
            "iteration 2603\n",
            "loss_train 0.1303786475130208\n",
            "iteration 2604\n",
            "loss_train 0.13036292060426125\n",
            "iteration 2605\n",
            "loss_train 0.1303472016099579\n",
            "iteration 2606\n",
            "loss_train 0.13033149052354945\n",
            "iteration 2607\n",
            "loss_train 0.1303157873384823\n",
            "iteration 2608\n",
            "loss_train 0.13030009204821044\n",
            "iteration 2609\n",
            "loss_train 0.13028440464619578\n",
            "iteration 2610\n",
            "loss_train 0.1302687251259079\n",
            "iteration 2611\n",
            "loss_train 0.13025305348082405\n",
            "iteration 2612\n",
            "loss_train 0.1302373897044292\n",
            "iteration 2613\n",
            "loss_train 0.13022173379021598\n",
            "iteration 2614\n",
            "loss_train 0.13020608573168466\n",
            "iteration 2615\n",
            "loss_train 0.1301904455223433\n",
            "iteration 2616\n",
            "loss_train 0.13017481315570745\n",
            "iteration 2617\n",
            "loss_train 0.13015918862530038\n",
            "iteration 2618\n",
            "loss_train 0.13014357192465298\n",
            "iteration 2619\n",
            "loss_train 0.13012796304730376\n",
            "iteration 2620\n",
            "loss_train 0.13011236198679882\n",
            "iteration 2621\n",
            "loss_train 0.13009676873669176\n",
            "iteration 2622\n",
            "loss_train 0.13008118329054394\n",
            "iteration 2623\n",
            "loss_train 0.13006560564192413\n",
            "iteration 2624\n",
            "loss_train 0.1300500357844087\n",
            "iteration 2625\n",
            "loss_train 0.13003447371158158\n",
            "iteration 2626\n",
            "loss_train 0.13001891941703422\n",
            "iteration 2627\n",
            "loss_train 0.13000337289436562\n",
            "iteration 2628\n",
            "loss_train 0.12998783413718223\n",
            "iteration 2629\n",
            "loss_train 0.129972303139098\n",
            "iteration 2630\n",
            "loss_train 0.12995677989373447\n",
            "iteration 2631\n",
            "loss_train 0.12994126439472048\n",
            "iteration 2632\n",
            "loss_train 0.12992575663569245\n",
            "iteration 2633\n",
            "loss_train 0.12991025661029418\n",
            "iteration 2634\n",
            "loss_train 0.12989476431217709\n",
            "iteration 2635\n",
            "loss_train 0.12987927973499974\n",
            "iteration 2636\n",
            "loss_train 0.12986380287242832\n",
            "iteration 2637\n",
            "loss_train 0.1298483337181364\n",
            "iteration 2638\n",
            "loss_train 0.1298328722658048\n",
            "iteration 2639\n",
            "loss_train 0.12981741850912196\n",
            "iteration 2640\n",
            "loss_train 0.12980197244178343\n",
            "iteration 2641\n",
            "loss_train 0.12978653405749233\n",
            "iteration 2642\n",
            "loss_train 0.12977110334995903\n",
            "iteration 2643\n",
            "loss_train 0.1297556803129012\n",
            "iteration 2644\n",
            "loss_train 0.12974026494004398\n",
            "iteration 2645\n",
            "loss_train 0.12972485722511962\n",
            "iteration 2646\n",
            "loss_train 0.12970945716186794\n",
            "iteration 2647\n",
            "loss_train 0.12969406474403575\n",
            "iteration 2648\n",
            "loss_train 0.12967867996537738\n",
            "iteration 2649\n",
            "loss_train 0.1296633028196543\n",
            "iteration 2650\n",
            "loss_train 0.12964793330063534\n",
            "iteration 2651\n",
            "loss_train 0.12963257140209647\n",
            "iteration 2652\n",
            "loss_train 0.12961721711782095\n",
            "iteration 2653\n",
            "loss_train 0.1296018704415993\n",
            "iteration 2654\n",
            "loss_train 0.1295865313672292\n",
            "iteration 2655\n",
            "loss_train 0.12957119988851556\n",
            "iteration 2656\n",
            "loss_train 0.12955587599927046\n",
            "iteration 2657\n",
            "loss_train 0.1295405596933132\n",
            "iteration 2658\n",
            "loss_train 0.1295252509644702\n",
            "iteration 2659\n",
            "loss_train 0.12950994980657513\n",
            "iteration 2660\n",
            "loss_train 0.12949465621346873\n",
            "iteration 2661\n",
            "loss_train 0.12947937017899888\n",
            "iteration 2662\n",
            "loss_train 0.12946409169702064\n",
            "iteration 2663\n",
            "loss_train 0.12944882076139616\n",
            "iteration 2664\n",
            "loss_train 0.12943355736599468\n",
            "iteration 2665\n",
            "loss_train 0.12941830150469255\n",
            "iteration 2666\n",
            "loss_train 0.1294030531713733\n",
            "iteration 2667\n",
            "loss_train 0.12938781235992727\n",
            "iteration 2668\n",
            "loss_train 0.12937257906425212\n",
            "iteration 2669\n",
            "loss_train 0.12935735327825257\n",
            "iteration 2670\n",
            "loss_train 0.1293421349958401\n",
            "iteration 2671\n",
            "loss_train 0.12932692421093364\n",
            "iteration 2672\n",
            "loss_train 0.12931172091745874\n",
            "iteration 2673\n",
            "loss_train 0.12929652510934822\n",
            "iteration 2674\n",
            "loss_train 0.1292813367805418\n",
            "iteration 2675\n",
            "loss_train 0.1292661559249862\n",
            "iteration 2676\n",
            "loss_train 0.1292509825366351\n",
            "iteration 2677\n",
            "loss_train 0.12923581660944924\n",
            "iteration 2678\n",
            "loss_train 0.12922065813739622\n",
            "iteration 2679\n",
            "loss_train 0.12920550711445059\n",
            "iteration 2680\n",
            "loss_train 0.12919036353459393\n",
            "iteration 2681\n",
            "loss_train 0.12917522739181467\n",
            "iteration 2682\n",
            "loss_train 0.1291600986801081\n",
            "iteration 2683\n",
            "loss_train 0.12914497739347658\n",
            "iteration 2684\n",
            "loss_train 0.1291298635259292\n",
            "iteration 2685\n",
            "loss_train 0.12911475707148204\n",
            "iteration 2686\n",
            "loss_train 0.12909965802415807\n",
            "iteration 2687\n",
            "loss_train 0.12908456637798701\n",
            "iteration 2688\n",
            "loss_train 0.12906948212700547\n",
            "iteration 2689\n",
            "loss_train 0.12905440526525708\n",
            "iteration 2690\n",
            "loss_train 0.12903933578679203\n",
            "iteration 2691\n",
            "loss_train 0.12902427368566752\n",
            "iteration 2692\n",
            "loss_train 0.12900921895594744\n",
            "iteration 2693\n",
            "loss_train 0.12899417159170265\n",
            "iteration 2694\n",
            "loss_train 0.12897913158701063\n",
            "iteration 2695\n",
            "loss_train 0.12896409893595578\n",
            "iteration 2696\n",
            "loss_train 0.12894907363262909\n",
            "iteration 2697\n",
            "loss_train 0.12893405567112856\n",
            "iteration 2698\n",
            "loss_train 0.12891904504555873\n",
            "iteration 2699\n",
            "loss_train 0.12890404175003103\n",
            "iteration 2700\n",
            "loss_train 0.12888904577866347\n",
            "iteration 2701\n",
            "loss_train 0.12887405712558095\n",
            "iteration 2702\n",
            "loss_train 0.12885907578491498\n",
            "iteration 2703\n",
            "loss_train 0.12884410175080374\n",
            "iteration 2704\n",
            "loss_train 0.12882913501739227\n",
            "iteration 2705\n",
            "loss_train 0.12881417557883207\n",
            "iteration 2706\n",
            "loss_train 0.12879922342928143\n",
            "iteration 2707\n",
            "loss_train 0.12878427856290534\n",
            "iteration 2708\n",
            "loss_train 0.12876934097387538\n",
            "iteration 2709\n",
            "loss_train 0.12875441065636975\n",
            "iteration 2710\n",
            "loss_train 0.1287394876045734\n",
            "iteration 2711\n",
            "loss_train 0.12872457181267774\n",
            "iteration 2712\n",
            "loss_train 0.1287096632748809\n",
            "iteration 2713\n",
            "loss_train 0.1286947619853876\n",
            "iteration 2714\n",
            "loss_train 0.12867986793840908\n",
            "iteration 2715\n",
            "loss_train 0.12866498112816332\n",
            "iteration 2716\n",
            "loss_train 0.1286501015488747\n",
            "iteration 2717\n",
            "loss_train 0.12863522919477427\n",
            "iteration 2718\n",
            "loss_train 0.12862036406009966\n",
            "iteration 2719\n",
            "loss_train 0.12860550613909488\n",
            "iteration 2720\n",
            "loss_train 0.12859065542601067\n",
            "iteration 2721\n",
            "loss_train 0.12857581191510412\n",
            "iteration 2722\n",
            "loss_train 0.128560975600639\n",
            "iteration 2723\n",
            "loss_train 0.1285461464768855\n",
            "iteration 2724\n",
            "loss_train 0.12853132453812027\n",
            "iteration 2725\n",
            "loss_train 0.12851650977862655\n",
            "iteration 2726\n",
            "loss_train 0.12850170219269397\n",
            "iteration 2727\n",
            "loss_train 0.12848690177461863\n",
            "iteration 2728\n",
            "loss_train 0.1284721085187031\n",
            "iteration 2729\n",
            "loss_train 0.1284573224192565\n",
            "iteration 2730\n",
            "loss_train 0.1284425434705942\n",
            "iteration 2731\n",
            "loss_train 0.12842777166703817\n",
            "iteration 2732\n",
            "loss_train 0.12841300700291666\n",
            "iteration 2733\n",
            "loss_train 0.12839824947256442\n",
            "iteration 2734\n",
            "loss_train 0.1283834990703226\n",
            "iteration 2735\n",
            "loss_train 0.1283687557905387\n",
            "iteration 2736\n",
            "loss_train 0.12835401962756662\n",
            "iteration 2737\n",
            "loss_train 0.12833929057576665\n",
            "iteration 2738\n",
            "loss_train 0.1283245686295054\n",
            "iteration 2739\n",
            "loss_train 0.1283098537831558\n",
            "iteration 2740\n",
            "loss_train 0.1282951460310973\n",
            "iteration 2741\n",
            "loss_train 0.12828044536771546\n",
            "iteration 2742\n",
            "loss_train 0.12826575178740238\n",
            "iteration 2743\n",
            "loss_train 0.12825106528455624\n",
            "iteration 2744\n",
            "loss_train 0.1282363858535818\n",
            "iteration 2745\n",
            "loss_train 0.12822171348888983\n",
            "iteration 2746\n",
            "loss_train 0.12820704818489756\n",
            "iteration 2747\n",
            "loss_train 0.12819238993602855\n",
            "iteration 2748\n",
            "loss_train 0.12817773873671245\n",
            "iteration 2749\n",
            "loss_train 0.12816309458138536\n",
            "iteration 2750\n",
            "loss_train 0.12814845746448947\n",
            "iteration 2751\n",
            "loss_train 0.1281338273804733\n",
            "iteration 2752\n",
            "loss_train 0.12811920432379156\n",
            "iteration 2753\n",
            "loss_train 0.12810458828890528\n",
            "iteration 2754\n",
            "loss_train 0.12808997927028157\n",
            "iteration 2755\n",
            "loss_train 0.12807537726239382\n",
            "iteration 2756\n",
            "loss_train 0.1280607822597216\n",
            "iteration 2757\n",
            "loss_train 0.12804619425675065\n",
            "iteration 2758\n",
            "loss_train 0.12803161324797296\n",
            "iteration 2759\n",
            "loss_train 0.12801703922788665\n",
            "iteration 2760\n",
            "loss_train 0.12800247219099592\n",
            "iteration 2761\n",
            "loss_train 0.12798791213181124\n",
            "iteration 2762\n",
            "loss_train 0.1279733590448492\n",
            "iteration 2763\n",
            "loss_train 0.1279588129246324\n",
            "iteration 2764\n",
            "loss_train 0.12794427376568976\n",
            "iteration 2765\n",
            "loss_train 0.12792974156255613\n",
            "iteration 2766\n",
            "loss_train 0.12791521630977265\n",
            "iteration 2767\n",
            "loss_train 0.12790069800188636\n",
            "iteration 2768\n",
            "loss_train 0.12788618663345055\n",
            "iteration 2769\n",
            "loss_train 0.1278716821990245\n",
            "iteration 2770\n",
            "loss_train 0.12785718469317364\n",
            "iteration 2771\n",
            "loss_train 0.12784269411046936\n",
            "iteration 2772\n",
            "loss_train 0.12782821044548912\n",
            "iteration 2773\n",
            "loss_train 0.12781373369281657\n",
            "iteration 2774\n",
            "loss_train 0.12779926384704118\n",
            "iteration 2775\n",
            "loss_train 0.12778480090275857\n",
            "iteration 2776\n",
            "loss_train 0.12777034485457037\n",
            "iteration 2777\n",
            "loss_train 0.12775589569708423\n",
            "iteration 2778\n",
            "loss_train 0.12774145342491378\n",
            "iteration 2779\n",
            "loss_train 0.1277270180326786\n",
            "iteration 2780\n",
            "loss_train 0.1277125895150043\n",
            "iteration 2781\n",
            "loss_train 0.1276981678665225\n",
            "iteration 2782\n",
            "loss_train 0.1276837530818707\n",
            "iteration 2783\n",
            "loss_train 0.12766934515569234\n",
            "iteration 2784\n",
            "loss_train 0.127654944082637\n",
            "iteration 2785\n",
            "loss_train 0.12764054985735998\n",
            "iteration 2786\n",
            "loss_train 0.12762616247452263\n",
            "iteration 2787\n",
            "loss_train 0.1276117819287922\n",
            "iteration 2788\n",
            "loss_train 0.12759740821484178\n",
            "iteration 2789\n",
            "loss_train 0.1275830413273505\n",
            "iteration 2790\n",
            "loss_train 0.12756868126100332\n",
            "iteration 2791\n",
            "loss_train 0.12755432801049102\n",
            "iteration 2792\n",
            "loss_train 0.12753998157051036\n",
            "iteration 2793\n",
            "loss_train 0.1275256419357639\n",
            "iteration 2794\n",
            "loss_train 0.1275113091009602\n",
            "iteration 2795\n",
            "loss_train 0.1274969830608135\n",
            "iteration 2796\n",
            "loss_train 0.12748266381004392\n",
            "iteration 2797\n",
            "loss_train 0.1274683513433775\n",
            "iteration 2798\n",
            "loss_train 0.12745404565554608\n",
            "iteration 2799\n",
            "loss_train 0.1274397467412873\n",
            "iteration 2800\n",
            "loss_train 0.12742545459534457\n",
            "iteration 2801\n",
            "loss_train 0.12741116921246723\n",
            "iteration 2802\n",
            "loss_train 0.12739689058741024\n",
            "iteration 2803\n",
            "loss_train 0.12738261871493456\n",
            "iteration 2804\n",
            "loss_train 0.12736835358980667\n",
            "iteration 2805\n",
            "loss_train 0.1273540952067991\n",
            "iteration 2806\n",
            "loss_train 0.12733984356068986\n",
            "iteration 2807\n",
            "loss_train 0.12732559864626297\n",
            "iteration 2808\n",
            "loss_train 0.12731136045830801\n",
            "iteration 2809\n",
            "loss_train 0.12729712899162046\n",
            "iteration 2810\n",
            "loss_train 0.12728290424100133\n",
            "iteration 2811\n",
            "loss_train 0.12726868620125759\n",
            "iteration 2812\n",
            "loss_train 0.12725447486720157\n",
            "iteration 2813\n",
            "loss_train 0.12724027023365175\n",
            "iteration 2814\n",
            "loss_train 0.127226072295432\n",
            "iteration 2815\n",
            "loss_train 0.1272118810473719\n",
            "iteration 2816\n",
            "loss_train 0.12719769648430687\n",
            "iteration 2817\n",
            "loss_train 0.12718351860107782\n",
            "iteration 2818\n",
            "loss_train 0.12716934739253144\n",
            "iteration 2819\n",
            "loss_train 0.12715518285351998\n",
            "iteration 2820\n",
            "loss_train 0.12714102497890153\n",
            "iteration 2821\n",
            "loss_train 0.12712687376353954\n",
            "iteration 2822\n",
            "loss_train 0.1271127292023033\n",
            "iteration 2823\n",
            "loss_train 0.12709859129006762\n",
            "iteration 2824\n",
            "loss_train 0.12708446002171297\n",
            "iteration 2825\n",
            "loss_train 0.12707033539212548\n",
            "iteration 2826\n",
            "loss_train 0.12705621739619674\n",
            "iteration 2827\n",
            "loss_train 0.12704210602882407\n",
            "iteration 2828\n",
            "loss_train 0.12702800128491024\n",
            "iteration 2829\n",
            "loss_train 0.12701390315936373\n",
            "iteration 2830\n",
            "loss_train 0.12699981164709848\n",
            "iteration 2831\n",
            "loss_train 0.12698572674303404\n",
            "iteration 2832\n",
            "loss_train 0.12697164844209552\n",
            "iteration 2833\n",
            "loss_train 0.12695757673921354\n",
            "iteration 2834\n",
            "loss_train 0.1269435116293243\n",
            "iteration 2835\n",
            "loss_train 0.1269294531073694\n",
            "iteration 2836\n",
            "loss_train 0.12691540116829614\n",
            "iteration 2837\n",
            "loss_train 0.12690135580705722\n",
            "iteration 2838\n",
            "loss_train 0.12688731701861086\n",
            "iteration 2839\n",
            "loss_train 0.12687328479792084\n",
            "iteration 2840\n",
            "loss_train 0.12685925913995633\n",
            "iteration 2841\n",
            "loss_train 0.126845240039692\n",
            "iteration 2842\n",
            "loss_train 0.12683122749210807\n",
            "iteration 2843\n",
            "loss_train 0.12681722149219007\n",
            "iteration 2844\n",
            "loss_train 0.12680322203492922\n",
            "iteration 2845\n",
            "loss_train 0.126789229115322\n",
            "iteration 2846\n",
            "loss_train 0.12677524272837035\n",
            "iteration 2847\n",
            "loss_train 0.12676126286908168\n",
            "iteration 2848\n",
            "loss_train 0.1267472895324689\n",
            "iteration 2849\n",
            "loss_train 0.1267333227135502\n",
            "iteration 2850\n",
            "loss_train 0.12671936240734924\n",
            "iteration 2851\n",
            "loss_train 0.1267054086088951\n",
            "iteration 2852\n",
            "loss_train 0.1266914613132223\n",
            "iteration 2853\n",
            "loss_train 0.12667752051537057\n",
            "iteration 2854\n",
            "loss_train 0.1266635862103852\n",
            "iteration 2855\n",
            "loss_train 0.1266496583933168\n",
            "iteration 2856\n",
            "loss_train 0.12663573705922132\n",
            "iteration 2857\n",
            "loss_train 0.12662182220316007\n",
            "iteration 2858\n",
            "loss_train 0.12660791382019976\n",
            "iteration 2859\n",
            "loss_train 0.12659401190541236\n",
            "iteration 2860\n",
            "loss_train 0.1265801164538752\n",
            "iteration 2861\n",
            "loss_train 0.126566227460671\n",
            "iteration 2862\n",
            "loss_train 0.1265523449208877\n",
            "iteration 2863\n",
            "loss_train 0.12653846882961867\n",
            "iteration 2864\n",
            "loss_train 0.12652459918196243\n",
            "iteration 2865\n",
            "loss_train 0.12651073597302295\n",
            "iteration 2866\n",
            "loss_train 0.12649687919790942\n",
            "iteration 2867\n",
            "loss_train 0.1264830288517362\n",
            "iteration 2868\n",
            "loss_train 0.12646918492962322\n",
            "iteration 2869\n",
            "loss_train 0.12645534742669534\n",
            "iteration 2870\n",
            "loss_train 0.12644151633808295\n",
            "iteration 2871\n",
            "loss_train 0.12642769165892148\n",
            "iteration 2872\n",
            "loss_train 0.12641387338435175\n",
            "iteration 2873\n",
            "loss_train 0.12640006150951974\n",
            "iteration 2874\n",
            "loss_train 0.1263862560295767\n",
            "iteration 2875\n",
            "loss_train 0.12637245693967908\n",
            "iteration 2876\n",
            "loss_train 0.12635866423498854\n",
            "iteration 2877\n",
            "loss_train 0.12634487791067198\n",
            "iteration 2878\n",
            "loss_train 0.12633109796190142\n",
            "iteration 2879\n",
            "loss_train 0.12631732438385426\n",
            "iteration 2880\n",
            "loss_train 0.1263035571717128\n",
            "iteration 2881\n",
            "loss_train 0.12628979632066487\n",
            "iteration 2882\n",
            "loss_train 0.1262760418259031\n",
            "iteration 2883\n",
            "loss_train 0.1262622936826255\n",
            "iteration 2884\n",
            "loss_train 0.12624855188603526\n",
            "iteration 2885\n",
            "loss_train 0.1262348164313406\n",
            "iteration 2886\n",
            "loss_train 0.12622108731375498\n",
            "iteration 2887\n",
            "loss_train 0.1262073645284969\n",
            "iteration 2888\n",
            "loss_train 0.1261936480707901\n",
            "iteration 2889\n",
            "loss_train 0.12617993793586335\n",
            "iteration 2890\n",
            "loss_train 0.12616623411895053\n",
            "iteration 2891\n",
            "loss_train 0.12615253661529074\n",
            "iteration 2892\n",
            "loss_train 0.12613884542012807\n",
            "iteration 2893\n",
            "loss_train 0.12612516052871164\n",
            "iteration 2894\n",
            "loss_train 0.1261114819362959\n",
            "iteration 2895\n",
            "loss_train 0.12609780963814013\n",
            "iteration 2896\n",
            "loss_train 0.12608414362950876\n",
            "iteration 2897\n",
            "loss_train 0.12607048390567138\n",
            "iteration 2898\n",
            "loss_train 0.12605683046190247\n",
            "iteration 2899\n",
            "loss_train 0.12604318329348174\n",
            "iteration 2900\n",
            "loss_train 0.1260295423956937\n",
            "iteration 2901\n",
            "loss_train 0.12601590776382818\n",
            "iteration 2902\n",
            "loss_train 0.1260022793931798\n",
            "iteration 2903\n",
            "loss_train 0.12598865727904834\n",
            "iteration 2904\n",
            "loss_train 0.12597504141673857\n",
            "iteration 2905\n",
            "loss_train 0.12596143180156022\n",
            "iteration 2906\n",
            "loss_train 0.12594782842882807\n",
            "iteration 2907\n",
            "loss_train 0.1259342312938619\n",
            "iteration 2908\n",
            "loss_train 0.1259206403919864\n",
            "iteration 2909\n",
            "loss_train 0.12590705571853128\n",
            "iteration 2910\n",
            "loss_train 0.12589347726883127\n",
            "iteration 2911\n",
            "loss_train 0.12587990503822602\n",
            "iteration 2912\n",
            "loss_train 0.1258663390220601\n",
            "iteration 2913\n",
            "loss_train 0.1258527792156832\n",
            "iteration 2914\n",
            "loss_train 0.12583922561444968\n",
            "iteration 2915\n",
            "loss_train 0.12582567821371907\n",
            "iteration 2916\n",
            "loss_train 0.12581213700885577\n",
            "iteration 2917\n",
            "loss_train 0.12579860199522905\n",
            "iteration 2918\n",
            "loss_train 0.1257850731682131\n",
            "iteration 2919\n",
            "loss_train 0.12577155052318714\n",
            "iteration 2920\n",
            "loss_train 0.12575803405553512\n",
            "iteration 2921\n",
            "loss_train 0.12574452376064596\n",
            "iteration 2922\n",
            "loss_train 0.12573101963391362\n",
            "iteration 2923\n",
            "loss_train 0.1257175216707367\n",
            "iteration 2924\n",
            "loss_train 0.1257040298665187\n",
            "iteration 2925\n",
            "loss_train 0.12569054421666825\n",
            "iteration 2926\n",
            "loss_train 0.12567706471659862\n",
            "iteration 2927\n",
            "loss_train 0.12566359136172786\n",
            "iteration 2928\n",
            "loss_train 0.12565012414747911\n",
            "iteration 2929\n",
            "loss_train 0.1256366630692802\n",
            "iteration 2930\n",
            "loss_train 0.12562320812256383\n",
            "iteration 2931\n",
            "loss_train 0.12560975930276758\n",
            "iteration 2932\n",
            "loss_train 0.12559631660533369\n",
            "iteration 2933\n",
            "loss_train 0.12558288002570944\n",
            "iteration 2934\n",
            "loss_train 0.12556944955934674\n",
            "iteration 2935\n",
            "loss_train 0.12555602520170236\n",
            "iteration 2936\n",
            "loss_train 0.12554260694823793\n",
            "iteration 2937\n",
            "loss_train 0.1255291947944198\n",
            "iteration 2938\n",
            "loss_train 0.1255157887357191\n",
            "iteration 2939\n",
            "loss_train 0.12550238876761177\n",
            "iteration 2940\n",
            "loss_train 0.12548899488557852\n",
            "iteration 2941\n",
            "loss_train 0.12547560708510475\n",
            "iteration 2942\n",
            "loss_train 0.12546222536168078\n",
            "iteration 2943\n",
            "loss_train 0.12544884971080145\n",
            "iteration 2944\n",
            "loss_train 0.12543548012796657\n",
            "iteration 2945\n",
            "loss_train 0.12542211660868058\n",
            "iteration 2946\n",
            "loss_train 0.12540875914845254\n",
            "iteration 2947\n",
            "loss_train 0.12539540774279648\n",
            "iteration 2948\n",
            "loss_train 0.12538206238723096\n",
            "iteration 2949\n",
            "loss_train 0.12536872307727934\n",
            "iteration 2950\n",
            "loss_train 0.12535538980846964\n",
            "iteration 2951\n",
            "loss_train 0.1253420625763345\n",
            "iteration 2952\n",
            "loss_train 0.12532874137641153\n",
            "iteration 2953\n",
            "loss_train 0.12531542620424269\n",
            "iteration 2954\n",
            "loss_train 0.1253021170553748\n",
            "iteration 2955\n",
            "loss_train 0.12528881392535943\n",
            "iteration 2956\n",
            "loss_train 0.12527551680975252\n",
            "iteration 2957\n",
            "loss_train 0.125262225704115\n",
            "iteration 2958\n",
            "loss_train 0.1252489406040122\n",
            "iteration 2959\n",
            "loss_train 0.12523566150501428\n",
            "iteration 2960\n",
            "loss_train 0.125222388402696\n",
            "iteration 2961\n",
            "loss_train 0.1252091212926366\n",
            "iteration 2962\n",
            "loss_train 0.12519586017042014\n",
            "iteration 2963\n",
            "loss_train 0.12518260503163522\n",
            "iteration 2964\n",
            "loss_train 0.12516935587187505\n",
            "iteration 2965\n",
            "loss_train 0.1251561126867375\n",
            "iteration 2966\n",
            "loss_train 0.12514287547182495\n",
            "iteration 2967\n",
            "loss_train 0.1251296442227445\n",
            "iteration 2968\n",
            "loss_train 0.12511641893510772\n",
            "iteration 2969\n",
            "loss_train 0.12510319960453078\n",
            "iteration 2970\n",
            "loss_train 0.1250899862266346\n",
            "iteration 2971\n",
            "loss_train 0.12507677879704437\n",
            "iteration 2972\n",
            "loss_train 0.1250635773113901\n",
            "iteration 2973\n",
            "loss_train 0.12505038176530622\n",
            "iteration 2974\n",
            "loss_train 0.12503719215443182\n",
            "iteration 2975\n",
            "loss_train 0.12502400847441042\n",
            "iteration 2976\n",
            "loss_train 0.1250108307208902\n",
            "iteration 2977\n",
            "loss_train 0.12499765888952365\n",
            "iteration 2978\n",
            "loss_train 0.12498449297596813\n",
            "iteration 2979\n",
            "loss_train 0.12497133297588527\n",
            "iteration 2980\n",
            "loss_train 0.12495817888494125\n",
            "iteration 2981\n",
            "loss_train 0.12494503069880684\n",
            "iteration 2982\n",
            "loss_train 0.12493188841315725\n",
            "iteration 2983\n",
            "loss_train 0.12491875202367221\n",
            "iteration 2984\n",
            "loss_train 0.12490562152603589\n",
            "iteration 2985\n",
            "loss_train 0.12489249691593703\n",
            "iteration 2986\n",
            "loss_train 0.12487937818906886\n",
            "iteration 2987\n",
            "loss_train 0.12486626534112891\n",
            "iteration 2988\n",
            "loss_train 0.12485315836781935\n",
            "iteration 2989\n",
            "loss_train 0.12484005726484675\n",
            "iteration 2990\n",
            "loss_train 0.12482696202792215\n",
            "iteration 2991\n",
            "loss_train 0.12481387265276106\n",
            "iteration 2992\n",
            "loss_train 0.12480078913508334\n",
            "iteration 2993\n",
            "loss_train 0.12478771147061338\n",
            "iteration 2994\n",
            "loss_train 0.12477463965508\n",
            "iteration 2995\n",
            "loss_train 0.12476157368421634\n",
            "iteration 2996\n",
            "loss_train 0.12474851355376008\n",
            "iteration 2997\n",
            "loss_train 0.12473545925945324\n",
            "iteration 2998\n",
            "loss_train 0.12472241079704231\n",
            "iteration 2999\n",
            "loss_train 0.12470936816227804\n",
            "iteration 3000\n",
            "loss_train 0.12469633135091582\n",
            "iteration 3001\n",
            "loss_train 0.1246833003587152\n",
            "iteration 3002\n",
            "loss_train 0.12467027518144015\n",
            "iteration 3003\n",
            "loss_train 0.12465725581485912\n",
            "iteration 3004\n",
            "loss_train 0.12464424225474488\n",
            "iteration 3005\n",
            "loss_train 0.12463123449687451\n",
            "iteration 3006\n",
            "loss_train 0.12461823253702957\n",
            "iteration 3007\n",
            "loss_train 0.12460523637099583\n",
            "iteration 3008\n",
            "loss_train 0.12459224599456349\n",
            "iteration 3009\n",
            "loss_train 0.1245792614035271\n",
            "iteration 3010\n",
            "loss_train 0.1245662825936855\n",
            "iteration 3011\n",
            "loss_train 0.12455330956084189\n",
            "iteration 3012\n",
            "loss_train 0.1245403423008038\n",
            "iteration 3013\n",
            "loss_train 0.12452738080938307\n",
            "iteration 3014\n",
            "loss_train 0.1245144250823958\n",
            "iteration 3015\n",
            "loss_train 0.12450147511566247\n",
            "iteration 3016\n",
            "loss_train 0.12448853090500786\n",
            "iteration 3017\n",
            "loss_train 0.12447559244626095\n",
            "iteration 3018\n",
            "loss_train 0.12446265973525518\n",
            "iteration 3019\n",
            "loss_train 0.12444973276782802\n",
            "iteration 3020\n",
            "loss_train 0.12443681153982152\n",
            "iteration 3021\n",
            "loss_train 0.12442389604708179\n",
            "iteration 3022\n",
            "loss_train 0.12441098628545923\n",
            "iteration 3023\n",
            "loss_train 0.12439808225080859\n",
            "iteration 3024\n",
            "loss_train 0.12438518393898881\n",
            "iteration 3025\n",
            "loss_train 0.12437229134586304\n",
            "iteration 3026\n",
            "loss_train 0.12435940446729875\n",
            "iteration 3027\n",
            "loss_train 0.12434652329916769\n",
            "iteration 3028\n",
            "loss_train 0.12433364783734564\n",
            "iteration 3029\n",
            "loss_train 0.12432077807771286\n",
            "iteration 3030\n",
            "loss_train 0.12430791401615364\n",
            "iteration 3031\n",
            "loss_train 0.1242950556485565\n",
            "iteration 3032\n",
            "loss_train 0.12428220297081438\n",
            "iteration 3033\n",
            "loss_train 0.12426935597882413\n",
            "iteration 3034\n",
            "loss_train 0.124256514668487\n",
            "iteration 3035\n",
            "loss_train 0.12424367903570828\n",
            "iteration 3036\n",
            "loss_train 0.12423084907639767\n",
            "iteration 3037\n",
            "loss_train 0.12421802478646879\n",
            "iteration 3038\n",
            "loss_train 0.12420520616183965\n",
            "iteration 3039\n",
            "loss_train 0.12419239319843228\n",
            "iteration 3040\n",
            "loss_train 0.12417958589217291\n",
            "iteration 3041\n",
            "loss_train 0.12416678423899201\n",
            "iteration 3042\n",
            "loss_train 0.12415398823482417\n",
            "iteration 3043\n",
            "loss_train 0.12414119787560798\n",
            "iteration 3044\n",
            "loss_train 0.12412841315728641\n",
            "iteration 3045\n",
            "loss_train 0.1241156340758064\n",
            "iteration 3046\n",
            "loss_train 0.12410286062711913\n",
            "iteration 3047\n",
            "loss_train 0.12409009280717978\n",
            "iteration 3048\n",
            "loss_train 0.12407733061194773\n",
            "iteration 3049\n",
            "loss_train 0.12406457403738651\n",
            "iteration 3050\n",
            "loss_train 0.12405182307946364\n",
            "iteration 3051\n",
            "loss_train 0.12403907773415085\n",
            "iteration 3052\n",
            "loss_train 0.12402633799742398\n",
            "iteration 3053\n",
            "loss_train 0.12401360386526282\n",
            "iteration 3054\n",
            "loss_train 0.12400087533365142\n",
            "iteration 3055\n",
            "loss_train 0.1239881523985778\n",
            "iteration 3056\n",
            "loss_train 0.12397543505603412\n",
            "iteration 3057\n",
            "loss_train 0.12396272330201659\n",
            "iteration 3058\n",
            "loss_train 0.12395001713252543\n",
            "iteration 3059\n",
            "loss_train 0.12393731654356503\n",
            "iteration 3060\n",
            "loss_train 0.12392462153114374\n",
            "iteration 3061\n",
            "loss_train 0.12391193209127398\n",
            "iteration 3062\n",
            "loss_train 0.12389924821997225\n",
            "iteration 3063\n",
            "loss_train 0.12388656991325907\n",
            "iteration 3064\n",
            "loss_train 0.12387389716715899\n",
            "iteration 3065\n",
            "loss_train 0.12386122997770058\n",
            "iteration 3066\n",
            "loss_train 0.12384856834091643\n",
            "iteration 3067\n",
            "loss_train 0.12383591225284318\n",
            "iteration 3068\n",
            "loss_train 0.12382326170952146\n",
            "iteration 3069\n",
            "loss_train 0.12381061670699589\n",
            "iteration 3070\n",
            "loss_train 0.12379797724131514\n",
            "iteration 3071\n",
            "loss_train 0.12378534330853182\n",
            "iteration 3072\n",
            "loss_train 0.12377271490470253\n",
            "iteration 3073\n",
            "loss_train 0.12376009202588795\n",
            "iteration 3074\n",
            "loss_train 0.12374747466815263\n",
            "iteration 3075\n",
            "loss_train 0.12373486282756516\n",
            "iteration 3076\n",
            "loss_train 0.12372225650019801\n",
            "iteration 3077\n",
            "loss_train 0.12370965568212777\n",
            "iteration 3078\n",
            "loss_train 0.12369706036943486\n",
            "iteration 3079\n",
            "loss_train 0.1236844705582037\n",
            "iteration 3080\n",
            "loss_train 0.12367188624452265\n",
            "iteration 3081\n",
            "loss_train 0.12365930742448403\n",
            "iteration 3082\n",
            "loss_train 0.12364673409418406\n",
            "iteration 3083\n",
            "loss_train 0.12363416624972294\n",
            "iteration 3084\n",
            "loss_train 0.12362160388720479\n",
            "iteration 3085\n",
            "loss_train 0.12360904700273763\n",
            "iteration 3086\n",
            "loss_train 0.12359649559243342\n",
            "iteration 3087\n",
            "loss_train 0.12358394965240799\n",
            "iteration 3088\n",
            "loss_train 0.12357140917878116\n",
            "iteration 3089\n",
            "loss_train 0.12355887416767655\n",
            "iteration 3090\n",
            "loss_train 0.12354634461522176\n",
            "iteration 3091\n",
            "loss_train 0.12353382051754824\n",
            "iteration 3092\n",
            "loss_train 0.12352130187079133\n",
            "iteration 3093\n",
            "loss_train 0.1235087886710903\n",
            "iteration 3094\n",
            "loss_train 0.12349628091458821\n",
            "iteration 3095\n",
            "loss_train 0.1234837785974321\n",
            "iteration 3096\n",
            "loss_train 0.12347128171577275\n",
            "iteration 3097\n",
            "loss_train 0.12345879026576485\n",
            "iteration 3098\n",
            "loss_train 0.1234463042435671\n",
            "iteration 3099\n",
            "loss_train 0.12343382364534175\n",
            "iteration 3100\n",
            "loss_train 0.12342134846725519\n",
            "iteration 3101\n",
            "loss_train 0.12340887870547744\n",
            "iteration 3102\n",
            "loss_train 0.12339641435618244\n",
            "iteration 3103\n",
            "loss_train 0.12338395541554802\n",
            "iteration 3104\n",
            "loss_train 0.12337150187975572\n",
            "iteration 3105\n",
            "loss_train 0.12335905374499105\n",
            "iteration 3106\n",
            "loss_train 0.12334661100744314\n",
            "iteration 3107\n",
            "loss_train 0.1233341736633051\n",
            "iteration 3108\n",
            "loss_train 0.12332174170877373\n",
            "iteration 3109\n",
            "loss_train 0.12330931514004975\n",
            "iteration 3110\n",
            "loss_train 0.12329689395333758\n",
            "iteration 3111\n",
            "loss_train 0.12328447814484543\n",
            "iteration 3112\n",
            "loss_train 0.12327206771078543\n",
            "iteration 3113\n",
            "loss_train 0.1232596626473732\n",
            "iteration 3114\n",
            "loss_train 0.12324726295082851\n",
            "iteration 3115\n",
            "loss_train 0.12323486861737469\n",
            "iteration 3116\n",
            "loss_train 0.12322247964323879\n",
            "iteration 3117\n",
            "loss_train 0.12321009602465174\n",
            "iteration 3118\n",
            "loss_train 0.12319771775784821\n",
            "iteration 3119\n",
            "loss_train 0.12318534483906655\n",
            "iteration 3120\n",
            "loss_train 0.1231729772645489\n",
            "iteration 3121\n",
            "loss_train 0.12316061503054122\n",
            "iteration 3122\n",
            "loss_train 0.12314825813329307\n",
            "iteration 3123\n",
            "loss_train 0.1231359065690578\n",
            "iteration 3124\n",
            "loss_train 0.12312356033409254\n",
            "iteration 3125\n",
            "loss_train 0.12311121942465807\n",
            "iteration 3126\n",
            "loss_train 0.12309888383701892\n",
            "iteration 3127\n",
            "loss_train 0.12308655356744332\n",
            "iteration 3128\n",
            "loss_train 0.1230742286122032\n",
            "iteration 3129\n",
            "loss_train 0.12306190896757425\n",
            "iteration 3130\n",
            "loss_train 0.1230495946298358\n",
            "iteration 3131\n",
            "loss_train 0.1230372855952709\n",
            "iteration 3132\n",
            "loss_train 0.1230249818601663\n",
            "iteration 3133\n",
            "loss_train 0.12301268342081235\n",
            "iteration 3134\n",
            "loss_train 0.12300039027350324\n",
            "iteration 3135\n",
            "loss_train 0.12298810241453668\n",
            "iteration 3136\n",
            "loss_train 0.12297581984021409\n",
            "iteration 3137\n",
            "loss_train 0.12296354254684062\n",
            "iteration 3138\n",
            "loss_train 0.12295127053072502\n",
            "iteration 3139\n",
            "loss_train 0.12293900378817973\n",
            "iteration 3140\n",
            "loss_train 0.12292674231552088\n",
            "iteration 3141\n",
            "loss_train 0.12291448610906804\n",
            "iteration 3142\n",
            "loss_train 0.12290223516514467\n",
            "iteration 3143\n",
            "loss_train 0.12288998948007773\n",
            "iteration 3144\n",
            "loss_train 0.12287774905019791\n",
            "iteration 3145\n",
            "loss_train 0.12286551387183944\n",
            "iteration 3146\n",
            "loss_train 0.12285328394134022\n",
            "iteration 3147\n",
            "loss_train 0.12284105925504167\n",
            "iteration 3148\n",
            "loss_train 0.12282883980928896\n",
            "iteration 3149\n",
            "loss_train 0.12281662560043088\n",
            "iteration 3150\n",
            "loss_train 0.12280441662481964\n",
            "iteration 3151\n",
            "loss_train 0.12279221287881126\n",
            "iteration 3152\n",
            "loss_train 0.12278001435876519\n",
            "iteration 3153\n",
            "loss_train 0.12276782106104456\n",
            "iteration 3154\n",
            "loss_train 0.12275563298201608\n",
            "iteration 3155\n",
            "loss_train 0.12274345011805005\n",
            "iteration 3156\n",
            "loss_train 0.12273127246552028\n",
            "iteration 3157\n",
            "loss_train 0.12271910002080423\n",
            "iteration 3158\n",
            "loss_train 0.12270693278028284\n",
            "iteration 3159\n",
            "loss_train 0.12269477074034073\n",
            "iteration 3160\n",
            "loss_train 0.12268261389736601\n",
            "iteration 3161\n",
            "loss_train 0.12267046224775031\n",
            "iteration 3162\n",
            "loss_train 0.12265831578788886\n",
            "iteration 3163\n",
            "loss_train 0.12264617451418047\n",
            "iteration 3164\n",
            "loss_train 0.12263403842302735\n",
            "iteration 3165\n",
            "loss_train 0.12262190751083536\n",
            "iteration 3166\n",
            "loss_train 0.12260978177401394\n",
            "iteration 3167\n",
            "loss_train 0.12259766120897592\n",
            "iteration 3168\n",
            "loss_train 0.1225855458121377\n",
            "iteration 3169\n",
            "loss_train 0.12257343557991929\n",
            "iteration 3170\n",
            "loss_train 0.12256133050874404\n",
            "iteration 3171\n",
            "loss_train 0.12254923059503896\n",
            "iteration 3172\n",
            "loss_train 0.12253713583523451\n",
            "iteration 3173\n",
            "loss_train 0.1225250462257646\n",
            "iteration 3174\n",
            "loss_train 0.12251296176306675\n",
            "iteration 3175\n",
            "loss_train 0.12250088244358182\n",
            "iteration 3176\n",
            "loss_train 0.12248880826375429\n",
            "iteration 3177\n",
            "loss_train 0.12247673922003205\n",
            "iteration 3178\n",
            "loss_train 0.12246467530886647\n",
            "iteration 3179\n",
            "loss_train 0.12245261652671244\n",
            "iteration 3180\n",
            "loss_train 0.12244056287002823\n",
            "iteration 3181\n",
            "loss_train 0.12242851433527568\n",
            "iteration 3182\n",
            "loss_train 0.12241647091892\n",
            "iteration 3183\n",
            "loss_train 0.12240443261742996\n",
            "iteration 3184\n",
            "loss_train 0.12239239942727767\n",
            "iteration 3185\n",
            "loss_train 0.12238037134493863\n",
            "iteration 3186\n",
            "loss_train 0.12236834836689203\n",
            "iteration 3187\n",
            "loss_train 0.1223563304896203\n",
            "iteration 3188\n",
            "loss_train 0.1223443177096093\n",
            "iteration 3189\n",
            "loss_train 0.12233231002334838\n",
            "iteration 3190\n",
            "loss_train 0.12232030742733034\n",
            "iteration 3191\n",
            "loss_train 0.12230830991805136\n",
            "iteration 3192\n",
            "loss_train 0.12229631749201096\n",
            "iteration 3193\n",
            "loss_train 0.12228433014571223\n",
            "iteration 3194\n",
            "loss_train 0.12227234787566159\n",
            "iteration 3195\n",
            "loss_train 0.12226037067836876\n",
            "iteration 3196\n",
            "loss_train 0.12224839855034701\n",
            "iteration 3197\n",
            "loss_train 0.12223643148811297\n",
            "iteration 3198\n",
            "loss_train 0.12222446948818659\n",
            "iteration 3199\n",
            "loss_train 0.1222125125470913\n",
            "iteration 3200\n",
            "loss_train 0.12220056066135379\n",
            "iteration 3201\n",
            "loss_train 0.12218861382750422\n",
            "iteration 3202\n",
            "loss_train 0.12217667204207619\n",
            "iteration 3203\n",
            "loss_train 0.12216473530160646\n",
            "iteration 3204\n",
            "loss_train 0.12215280360263531\n",
            "iteration 3205\n",
            "loss_train 0.12214087694170632\n",
            "iteration 3206\n",
            "loss_train 0.12212895531536652\n",
            "iteration 3207\n",
            "loss_train 0.12211703872016609\n",
            "iteration 3208\n",
            "loss_train 0.12210512715265874\n",
            "iteration 3209\n",
            "loss_train 0.12209322060940155\n",
            "iteration 3210\n",
            "loss_train 0.12208131908695473\n",
            "iteration 3211\n",
            "loss_train 0.12206942258188196\n",
            "iteration 3212\n",
            "loss_train 0.1220575310907503\n",
            "iteration 3213\n",
            "loss_train 0.12204564461013004\n",
            "iteration 3214\n",
            "loss_train 0.12203376313659478\n",
            "iteration 3215\n",
            "loss_train 0.12202188666672152\n",
            "iteration 3216\n",
            "loss_train 0.12201001519709051\n",
            "iteration 3217\n",
            "loss_train 0.12199814872428535\n",
            "iteration 3218\n",
            "loss_train 0.12198628724489288\n",
            "iteration 3219\n",
            "loss_train 0.12197443075550335\n",
            "iteration 3220\n",
            "loss_train 0.12196257925271016\n",
            "iteration 3221\n",
            "loss_train 0.12195073273311016\n",
            "iteration 3222\n",
            "loss_train 0.12193889119330335\n",
            "iteration 3223\n",
            "loss_train 0.1219270546298931\n",
            "iteration 3224\n",
            "loss_train 0.12191522303948601\n",
            "iteration 3225\n",
            "loss_train 0.12190339641869197\n",
            "iteration 3226\n",
            "loss_train 0.1218915747641242\n",
            "iteration 3227\n",
            "loss_train 0.12187975807239905\n",
            "iteration 3228\n",
            "loss_train 0.12186794634013629\n",
            "iteration 3229\n",
            "loss_train 0.12185613956395883\n",
            "iteration 3230\n",
            "loss_train 0.1218443377404929\n",
            "iteration 3231\n",
            "loss_train 0.121832540866368\n",
            "iteration 3232\n",
            "loss_train 0.12182074893821673\n",
            "iteration 3233\n",
            "loss_train 0.12180896195267514\n",
            "iteration 3234\n",
            "loss_train 0.12179717990638238\n",
            "iteration 3235\n",
            "loss_train 0.12178540279598087\n",
            "iteration 3236\n",
            "loss_train 0.12177363061811625\n",
            "iteration 3237\n",
            "loss_train 0.12176186336943744\n",
            "iteration 3238\n",
            "loss_train 0.12175010104659649\n",
            "iteration 3239\n",
            "loss_train 0.12173834364624875\n",
            "iteration 3240\n",
            "loss_train 0.12172659116505273\n",
            "iteration 3241\n",
            "loss_train 0.12171484359967019\n",
            "iteration 3242\n",
            "loss_train 0.12170310094676608\n",
            "iteration 3243\n",
            "loss_train 0.12169136320300851\n",
            "iteration 3244\n",
            "loss_train 0.12167963036506886\n",
            "iteration 3245\n",
            "loss_train 0.12166790242962167\n",
            "iteration 3246\n",
            "loss_train 0.12165617939334467\n",
            "iteration 3247\n",
            "loss_train 0.12164446125291878\n",
            "iteration 3248\n",
            "loss_train 0.12163274800502807\n",
            "iteration 3249\n",
            "loss_train 0.12162103964635983\n",
            "iteration 3250\n",
            "loss_train 0.12160933617360456\n",
            "iteration 3251\n",
            "loss_train 0.12159763758345576\n",
            "iteration 3252\n",
            "loss_train 0.12158594387261033\n",
            "iteration 3253\n",
            "loss_train 0.12157425503776816\n",
            "iteration 3254\n",
            "loss_train 0.12156257107563244\n",
            "iteration 3255\n",
            "loss_train 0.1215508919829093\n",
            "iteration 3256\n",
            "loss_train 0.1215392177563082\n",
            "iteration 3257\n",
            "loss_train 0.12152754839254172\n",
            "iteration 3258\n",
            "loss_train 0.12151588388832554\n",
            "iteration 3259\n",
            "loss_train 0.12150422424037852\n",
            "iteration 3260\n",
            "loss_train 0.1214925694454226\n",
            "iteration 3261\n",
            "loss_train 0.12148091950018289\n",
            "iteration 3262\n",
            "loss_train 0.1214692744013876\n",
            "iteration 3263\n",
            "loss_train 0.12145763414576811\n",
            "iteration 3264\n",
            "loss_train 0.12144599873005885\n",
            "iteration 3265\n",
            "loss_train 0.12143436815099745\n",
            "iteration 3266\n",
            "loss_train 0.12142274240532457\n",
            "iteration 3267\n",
            "loss_train 0.12141112148978404\n",
            "iteration 3268\n",
            "loss_train 0.12139950540112275\n",
            "iteration 3269\n",
            "loss_train 0.12138789413609068\n",
            "iteration 3270\n",
            "loss_train 0.121376287691441\n",
            "iteration 3271\n",
            "loss_train 0.12136468606392982\n",
            "iteration 3272\n",
            "loss_train 0.12135308925031647\n",
            "iteration 3273\n",
            "loss_train 0.12134149724736328\n",
            "iteration 3274\n",
            "loss_train 0.12132991005183572\n",
            "iteration 3275\n",
            "loss_train 0.1213183276605023\n",
            "iteration 3276\n",
            "loss_train 0.12130675007013464\n",
            "iteration 3277\n",
            "loss_train 0.12129517727750733\n",
            "iteration 3278\n",
            "loss_train 0.1212836092793982\n",
            "iteration 3279\n",
            "loss_train 0.12127204607258793\n",
            "iteration 3280\n",
            "loss_train 0.12126048765386045\n",
            "iteration 3281\n",
            "loss_train 0.12124893402000261\n",
            "iteration 3282\n",
            "loss_train 0.12123738516780432\n",
            "iteration 3283\n",
            "loss_train 0.12122584109405869\n",
            "iteration 3284\n",
            "loss_train 0.12121430179556164\n",
            "iteration 3285\n",
            "loss_train 0.12120276726911232\n",
            "iteration 3286\n",
            "loss_train 0.12119123751151276\n",
            "iteration 3287\n",
            "loss_train 0.12117971251956822\n",
            "iteration 3288\n",
            "loss_train 0.12116819229008673\n",
            "iteration 3289\n",
            "loss_train 0.12115667681987957\n",
            "iteration 3290\n",
            "loss_train 0.12114516610576095\n",
            "iteration 3291\n",
            "loss_train 0.12113366014454806\n",
            "iteration 3292\n",
            "loss_train 0.12112215893306112\n",
            "iteration 3293\n",
            "loss_train 0.1211106624681234\n",
            "iteration 3294\n",
            "loss_train 0.12109917074656115\n",
            "iteration 3295\n",
            "loss_train 0.12108768376520367\n",
            "iteration 3296\n",
            "loss_train 0.12107620152088314\n",
            "iteration 3297\n",
            "loss_train 0.12106472401043483\n",
            "iteration 3298\n",
            "loss_train 0.12105325123069696\n",
            "iteration 3299\n",
            "loss_train 0.12104178317851075\n",
            "iteration 3300\n",
            "loss_train 0.12103031985072041\n",
            "iteration 3301\n",
            "loss_train 0.12101886124417316\n",
            "iteration 3302\n",
            "loss_train 0.12100740735571906\n",
            "iteration 3303\n",
            "loss_train 0.12099595818221133\n",
            "iteration 3304\n",
            "loss_train 0.12098451372050599\n",
            "iteration 3305\n",
            "loss_train 0.12097307396746217\n",
            "iteration 3306\n",
            "loss_train 0.12096163891994181\n",
            "iteration 3307\n",
            "loss_train 0.12095020857480995\n",
            "iteration 3308\n",
            "loss_train 0.1209387829289345\n",
            "iteration 3309\n",
            "loss_train 0.12092736197918628\n",
            "iteration 3310\n",
            "loss_train 0.12091594572243926\n",
            "iteration 3311\n",
            "loss_train 0.12090453415557006\n",
            "iteration 3312\n",
            "loss_train 0.12089312727545848\n",
            "iteration 3313\n",
            "loss_train 0.1208817250789871\n",
            "iteration 3314\n",
            "loss_train 0.12087032756304154\n",
            "iteration 3315\n",
            "loss_train 0.12085893472451023\n",
            "iteration 3316\n",
            "loss_train 0.12084754656028467\n",
            "iteration 3317\n",
            "loss_train 0.12083616306725917\n",
            "iteration 3318\n",
            "loss_train 0.12082478424233102\n",
            "iteration 3319\n",
            "loss_train 0.12081341008240035\n",
            "iteration 3320\n",
            "loss_train 0.1208020405843703\n",
            "iteration 3321\n",
            "loss_train 0.1207906757451468\n",
            "iteration 3322\n",
            "loss_train 0.12077931556163882\n",
            "iteration 3323\n",
            "loss_train 0.12076796003075808\n",
            "iteration 3324\n",
            "loss_train 0.12075660914941937\n",
            "iteration 3325\n",
            "loss_train 0.12074526291454016\n",
            "iteration 3326\n",
            "loss_train 0.12073392132304095\n",
            "iteration 3327\n",
            "loss_train 0.12072258437184516\n",
            "iteration 3328\n",
            "loss_train 0.12071125205787903\n",
            "iteration 3329\n",
            "loss_train 0.12069992437807157\n",
            "iteration 3330\n",
            "loss_train 0.12068860132935483\n",
            "iteration 3331\n",
            "loss_train 0.12067728290866368\n",
            "iteration 3332\n",
            "loss_train 0.12066596911293585\n",
            "iteration 3333\n",
            "loss_train 0.12065465993911194\n",
            "iteration 3334\n",
            "loss_train 0.12064335538413537\n",
            "iteration 3335\n",
            "loss_train 0.12063205544495247\n",
            "iteration 3336\n",
            "loss_train 0.12062076011851244\n",
            "iteration 3337\n",
            "loss_train 0.12060946940176726\n",
            "iteration 3338\n",
            "loss_train 0.12059818329167177\n",
            "iteration 3339\n",
            "loss_train 0.12058690178518369\n",
            "iteration 3340\n",
            "loss_train 0.12057562487926361\n",
            "iteration 3341\n",
            "loss_train 0.12056435257087487\n",
            "iteration 3342\n",
            "loss_train 0.12055308485698364\n",
            "iteration 3343\n",
            "loss_train 0.12054182173455906\n",
            "iteration 3344\n",
            "loss_train 0.12053056320057293\n",
            "iteration 3345\n",
            "loss_train 0.12051930925199995\n",
            "iteration 3346\n",
            "loss_train 0.12050805988581767\n",
            "iteration 3347\n",
            "loss_train 0.12049681509900635\n",
            "iteration 3348\n",
            "loss_train 0.12048557488854918\n",
            "iteration 3349\n",
            "loss_train 0.12047433925143206\n",
            "iteration 3350\n",
            "loss_train 0.12046310818464374\n",
            "iteration 3351\n",
            "loss_train 0.12045188168517584\n",
            "iteration 3352\n",
            "loss_train 0.1204406597500227\n",
            "iteration 3353\n",
            "loss_train 0.12042944237618138\n",
            "iteration 3354\n",
            "loss_train 0.12041822956065189\n",
            "iteration 3355\n",
            "loss_train 0.12040702130043696\n",
            "iteration 3356\n",
            "loss_train 0.1203958175925421\n",
            "iteration 3357\n",
            "loss_train 0.12038461843397551\n",
            "iteration 3358\n",
            "loss_train 0.12037342382174841\n",
            "iteration 3359\n",
            "loss_train 0.12036223375287453\n",
            "iteration 3360\n",
            "loss_train 0.12035104822437055\n",
            "iteration 3361\n",
            "loss_train 0.12033986723325582\n",
            "iteration 3362\n",
            "loss_train 0.12032869077655252\n",
            "iteration 3363\n",
            "loss_train 0.12031751885128554\n",
            "iteration 3364\n",
            "loss_train 0.12030635145448258\n",
            "iteration 3365\n",
            "loss_train 0.12029518858317399\n",
            "iteration 3366\n",
            "loss_train 0.12028403023439302\n",
            "iteration 3367\n",
            "loss_train 0.12027287640517556\n",
            "iteration 3368\n",
            "loss_train 0.12026172709256032\n",
            "iteration 3369\n",
            "loss_train 0.12025058229358862\n",
            "iteration 3370\n",
            "loss_train 0.12023944200530469\n",
            "iteration 3371\n",
            "loss_train 0.12022830622475539\n",
            "iteration 3372\n",
            "loss_train 0.12021717494899035\n",
            "iteration 3373\n",
            "loss_train 0.12020604817506182\n",
            "iteration 3374\n",
            "loss_train 0.12019492590002502\n",
            "iteration 3375\n",
            "loss_train 0.12018380812093761\n",
            "iteration 3376\n",
            "loss_train 0.12017269483486012\n",
            "iteration 3377\n",
            "loss_train 0.12016158603885582\n",
            "iteration 3378\n",
            "loss_train 0.12015048172999061\n",
            "iteration 3379\n",
            "loss_train 0.12013938190533316\n",
            "iteration 3380\n",
            "loss_train 0.12012828656195476\n",
            "iteration 3381\n",
            "loss_train 0.1201171956969295\n",
            "iteration 3382\n",
            "loss_train 0.12010610930733413\n",
            "iteration 3383\n",
            "loss_train 0.12009502739024805\n",
            "iteration 3384\n",
            "loss_train 0.12008394994275345\n",
            "iteration 3385\n",
            "loss_train 0.12007287696193515\n",
            "iteration 3386\n",
            "loss_train 0.12006180844488057\n",
            "iteration 3387\n",
            "loss_train 0.12005074438868006\n",
            "iteration 3388\n",
            "loss_train 0.12003968479042634\n",
            "iteration 3389\n",
            "loss_train 0.120028629647215\n",
            "iteration 3390\n",
            "loss_train 0.12001757895614433\n",
            "iteration 3391\n",
            "loss_train 0.12000653271431515\n",
            "iteration 3392\n",
            "loss_train 0.11999549091883102\n",
            "iteration 3393\n",
            "loss_train 0.1199844535667982\n",
            "iteration 3394\n",
            "loss_train 0.11997342065532551\n",
            "iteration 3395\n",
            "loss_train 0.11996239218152452\n",
            "iteration 3396\n",
            "loss_train 0.11995136814250944\n",
            "iteration 3397\n",
            "loss_train 0.11994034853539708\n",
            "iteration 3398\n",
            "loss_train 0.11992933335730692\n",
            "iteration 3399\n",
            "loss_train 0.11991832260536112\n",
            "iteration 3400\n",
            "loss_train 0.11990731627668437\n",
            "iteration 3401\n",
            "loss_train 0.11989631436840423\n",
            "iteration 3402\n",
            "loss_train 0.1198853168776506\n",
            "iteration 3403\n",
            "loss_train 0.11987432380155624\n",
            "iteration 3404\n",
            "loss_train 0.11986333513725644\n",
            "iteration 3405\n",
            "loss_train 0.1198523508818891\n",
            "iteration 3406\n",
            "loss_train 0.11984137103259479\n",
            "iteration 3407\n",
            "loss_train 0.11983039558651667\n",
            "iteration 3408\n",
            "loss_train 0.11981942454080054\n",
            "iteration 3409\n",
            "loss_train 0.1198084578925948\n",
            "iteration 3410\n",
            "loss_train 0.11979749563905041\n",
            "iteration 3411\n",
            "loss_train 0.11978653777732107\n",
            "iteration 3412\n",
            "loss_train 0.11977558430456296\n",
            "iteration 3413\n",
            "loss_train 0.11976463521793486\n",
            "iteration 3414\n",
            "loss_train 0.11975369051459822\n",
            "iteration 3415\n",
            "loss_train 0.11974275019171704\n",
            "iteration 3416\n",
            "loss_train 0.11973181424645789\n",
            "iteration 3417\n",
            "loss_train 0.11972088267599004\n",
            "iteration 3418\n",
            "loss_train 0.11970995547748514\n",
            "iteration 3419\n",
            "loss_train 0.11969903264811764\n",
            "iteration 3420\n",
            "loss_train 0.11968811418506446\n",
            "iteration 3421\n",
            "loss_train 0.11967720008550503\n",
            "iteration 3422\n",
            "loss_train 0.11966629034662155\n",
            "iteration 3423\n",
            "loss_train 0.11965538496559858\n",
            "iteration 3424\n",
            "loss_train 0.11964448393962333\n",
            "iteration 3425\n",
            "loss_train 0.11963358726588565\n",
            "iteration 3426\n",
            "loss_train 0.11962269494157786\n",
            "iteration 3427\n",
            "loss_train 0.11961180696389476\n",
            "iteration 3428\n",
            "loss_train 0.11960092333003391\n",
            "iteration 3429\n",
            "loss_train 0.11959004403719523\n",
            "iteration 3430\n",
            "loss_train 0.11957916908258134\n",
            "iteration 3431\n",
            "loss_train 0.11956829846339731\n",
            "iteration 3432\n",
            "loss_train 0.11955743217685076\n",
            "iteration 3433\n",
            "loss_train 0.11954657022015187\n",
            "iteration 3434\n",
            "loss_train 0.1195357125905133\n",
            "iteration 3435\n",
            "loss_train 0.11952485928515039\n",
            "iteration 3436\n",
            "loss_train 0.11951401030128088\n",
            "iteration 3437\n",
            "loss_train 0.11950316563612501\n",
            "iteration 3438\n",
            "loss_train 0.11949232528690566\n",
            "iteration 3439\n",
            "loss_train 0.11948148925084819\n",
            "iteration 3440\n",
            "loss_train 0.11947065752518042\n",
            "iteration 3441\n",
            "loss_train 0.1194598301071327\n",
            "iteration 3442\n",
            "loss_train 0.119449006993938\n",
            "iteration 3443\n",
            "loss_train 0.11943818818283167\n",
            "iteration 3444\n",
            "loss_train 0.11942737367105159\n",
            "iteration 3445\n",
            "loss_train 0.11941656345583819\n",
            "iteration 3446\n",
            "loss_train 0.11940575753443444\n",
            "iteration 3447\n",
            "loss_train 0.11939495590408565\n",
            "iteration 3448\n",
            "loss_train 0.11938415856203975\n",
            "iteration 3449\n",
            "loss_train 0.11937336550554715\n",
            "iteration 3450\n",
            "loss_train 0.11936257673186072\n",
            "iteration 3451\n",
            "loss_train 0.1193517922382358\n",
            "iteration 3452\n",
            "loss_train 0.11934101202193026\n",
            "iteration 3453\n",
            "loss_train 0.11933023608020439\n",
            "iteration 3454\n",
            "loss_train 0.11931946441032105\n",
            "iteration 3455\n",
            "loss_train 0.11930869700954551\n",
            "iteration 3456\n",
            "loss_train 0.11929793387514538\n",
            "iteration 3457\n",
            "loss_train 0.11928717500439107\n",
            "iteration 3458\n",
            "loss_train 0.11927642039455513\n",
            "iteration 3459\n",
            "loss_train 0.11926567004291276\n",
            "iteration 3460\n",
            "loss_train 0.11925492394674155\n",
            "iteration 3461\n",
            "loss_train 0.11924418210332154\n",
            "iteration 3462\n",
            "loss_train 0.11923344450993523\n",
            "iteration 3463\n",
            "loss_train 0.11922271116386758\n",
            "iteration 3464\n",
            "loss_train 0.119211982062406\n",
            "iteration 3465\n",
            "loss_train 0.11920125720284043\n",
            "iteration 3466\n",
            "loss_train 0.11919053658246298\n",
            "iteration 3467\n",
            "loss_train 0.11917982019856857\n",
            "iteration 3468\n",
            "loss_train 0.11916910804845424\n",
            "iteration 3469\n",
            "loss_train 0.11915840012941964\n",
            "iteration 3470\n",
            "loss_train 0.11914769643876684\n",
            "iteration 3471\n",
            "loss_train 0.11913699697380022\n",
            "iteration 3472\n",
            "loss_train 0.11912630173182667\n",
            "iteration 3473\n",
            "loss_train 0.11911561071015556\n",
            "iteration 3474\n",
            "loss_train 0.11910492390609857\n",
            "iteration 3475\n",
            "loss_train 0.11909424131696983\n",
            "iteration 3476\n",
            "loss_train 0.11908356294008593\n",
            "iteration 3477\n",
            "loss_train 0.11907288877276583\n",
            "iteration 3478\n",
            "loss_train 0.11906221881233085\n",
            "iteration 3479\n",
            "loss_train 0.1190515530561048\n",
            "iteration 3480\n",
            "loss_train 0.1190408915014139\n",
            "iteration 3481\n",
            "loss_train 0.11903023414558667\n",
            "iteration 3482\n",
            "loss_train 0.11901958098595412\n",
            "iteration 3483\n",
            "loss_train 0.11900893201984956\n",
            "iteration 3484\n",
            "loss_train 0.1189982872446088\n",
            "iteration 3485\n",
            "loss_train 0.11898764665756999\n",
            "iteration 3486\n",
            "loss_train 0.11897701025607363\n",
            "iteration 3487\n",
            "loss_train 0.11896637803746261\n",
            "iteration 3488\n",
            "loss_train 0.1189557499990823\n",
            "iteration 3489\n",
            "loss_train 0.11894512613828029\n",
            "iteration 3490\n",
            "loss_train 0.11893450645240669\n",
            "iteration 3491\n",
            "loss_train 0.11892389093881389\n",
            "iteration 3492\n",
            "loss_train 0.11891327959485665\n",
            "iteration 3493\n",
            "loss_train 0.11890267241789211\n",
            "iteration 3494\n",
            "loss_train 0.11889206940527984\n",
            "iteration 3495\n",
            "loss_train 0.11888147055438167\n",
            "iteration 3496\n",
            "loss_train 0.11887087586256183\n",
            "iteration 3497\n",
            "loss_train 0.1188602853271869\n",
            "iteration 3498\n",
            "loss_train 0.11884969894562585\n",
            "iteration 3499\n",
            "loss_train 0.11883911671524992\n",
            "iteration 3500\n",
            "loss_train 0.11882853863343278\n",
            "iteration 3501\n",
            "loss_train 0.11881796469755034\n",
            "iteration 3502\n",
            "loss_train 0.118807394904981\n",
            "iteration 3503\n",
            "loss_train 0.11879682925310539\n",
            "iteration 3504\n",
            "loss_train 0.11878626773930646\n",
            "iteration 3505\n",
            "loss_train 0.11877571036096952\n",
            "iteration 3506\n",
            "loss_train 0.11876515711548229\n",
            "iteration 3507\n",
            "loss_train 0.11875460800023477\n",
            "iteration 3508\n",
            "loss_train 0.11874406301261917\n",
            "iteration 3509\n",
            "loss_train 0.11873352215003014\n",
            "iteration 3510\n",
            "loss_train 0.11872298540986465\n",
            "iteration 3511\n",
            "loss_train 0.11871245278952197\n",
            "iteration 3512\n",
            "loss_train 0.11870192428640362\n",
            "iteration 3513\n",
            "loss_train 0.11869139989791362\n",
            "iteration 3514\n",
            "loss_train 0.11868087962145801\n",
            "iteration 3515\n",
            "loss_train 0.11867036345444536\n",
            "iteration 3516\n",
            "loss_train 0.1186598513942865\n",
            "iteration 3517\n",
            "loss_train 0.11864934343839448\n",
            "iteration 3518\n",
            "loss_train 0.11863883958418471\n",
            "iteration 3519\n",
            "loss_train 0.11862833982907496\n",
            "iteration 3520\n",
            "loss_train 0.11861784417048511\n",
            "iteration 3521\n",
            "loss_train 0.11860735260583753\n",
            "iteration 3522\n",
            "loss_train 0.11859686513255677\n",
            "iteration 3523\n",
            "loss_train 0.11858638174806965\n",
            "iteration 3524\n",
            "loss_train 0.11857590244980533\n",
            "iteration 3525\n",
            "loss_train 0.1185654272351952\n",
            "iteration 3526\n",
            "loss_train 0.11855495610167296\n",
            "iteration 3527\n",
            "loss_train 0.1185444890466746\n",
            "iteration 3528\n",
            "loss_train 0.11853402606763828\n",
            "iteration 3529\n",
            "loss_train 0.11852356716200463\n",
            "iteration 3530\n",
            "loss_train 0.11851311232721633\n",
            "iteration 3531\n",
            "loss_train 0.11850266156071841\n",
            "iteration 3532\n",
            "loss_train 0.11849221485995823\n",
            "iteration 3533\n",
            "loss_train 0.11848177222238526\n",
            "iteration 3534\n",
            "loss_train 0.11847133364545134\n",
            "iteration 3535\n",
            "loss_train 0.11846089912661055\n",
            "iteration 3536\n",
            "loss_train 0.11845046866331922\n",
            "iteration 3537\n",
            "loss_train 0.11844004225303587\n",
            "iteration 3538\n",
            "loss_train 0.1184296198932213\n",
            "iteration 3539\n",
            "loss_train 0.11841920158133858\n",
            "iteration 3540\n",
            "loss_train 0.11840878731485296\n",
            "iteration 3541\n",
            "loss_train 0.11839837709123202\n",
            "iteration 3542\n",
            "loss_train 0.11838797090794552\n",
            "iteration 3543\n",
            "loss_train 0.11837756876246538\n",
            "iteration 3544\n",
            "loss_train 0.11836717065226589\n",
            "iteration 3545\n",
            "loss_train 0.11835677657482349\n",
            "iteration 3546\n",
            "loss_train 0.11834638652761684\n",
            "iteration 3547\n",
            "loss_train 0.11833600050812686\n",
            "iteration 3548\n",
            "loss_train 0.11832561851383658\n",
            "iteration 3549\n",
            "loss_train 0.11831524054223148\n",
            "iteration 3550\n",
            "loss_train 0.11830486659079902\n",
            "iteration 3551\n",
            "loss_train 0.11829449665702894\n",
            "iteration 3552\n",
            "loss_train 0.11828413073841322\n",
            "iteration 3553\n",
            "loss_train 0.11827376883244607\n",
            "iteration 3554\n",
            "loss_train 0.11826341093662385\n",
            "iteration 3555\n",
            "loss_train 0.11825305704844515\n",
            "iteration 3556\n",
            "loss_train 0.11824270716541072\n",
            "iteration 3557\n",
            "loss_train 0.1182323612850236\n",
            "iteration 3558\n",
            "loss_train 0.11822201940478888\n",
            "iteration 3559\n",
            "loss_train 0.11821168152221401\n",
            "iteration 3560\n",
            "loss_train 0.11820134763480845\n",
            "iteration 3561\n",
            "loss_train 0.118191017740084\n",
            "iteration 3562\n",
            "loss_train 0.11818069183555456\n",
            "iteration 3563\n",
            "loss_train 0.11817036991873628\n",
            "iteration 3564\n",
            "loss_train 0.1181600519871474\n",
            "iteration 3565\n",
            "loss_train 0.11814973803830837\n",
            "iteration 3566\n",
            "loss_train 0.11813942806974184\n",
            "iteration 3567\n",
            "loss_train 0.11812912207897265\n",
            "iteration 3568\n",
            "loss_train 0.11811882006352774\n",
            "iteration 3569\n",
            "loss_train 0.11810852202093626\n",
            "iteration 3570\n",
            "loss_train 0.11809822794872954\n",
            "iteration 3571\n",
            "loss_train 0.118087937844441\n",
            "iteration 3572\n",
            "loss_train 0.11807765170560627\n",
            "iteration 3573\n",
            "loss_train 0.11806736952976318\n",
            "iteration 3574\n",
            "loss_train 0.11805709131445166\n",
            "iteration 3575\n",
            "loss_train 0.11804681705721379\n",
            "iteration 3576\n",
            "loss_train 0.11803654675559377\n",
            "iteration 3577\n",
            "loss_train 0.11802628040713806\n",
            "iteration 3578\n",
            "loss_train 0.11801601800939517\n",
            "iteration 3579\n",
            "loss_train 0.11800575955991574\n",
            "iteration 3580\n",
            "loss_train 0.11799550505625264\n",
            "iteration 3581\n",
            "loss_train 0.11798525449596076\n",
            "iteration 3582\n",
            "loss_train 0.11797500787659719\n",
            "iteration 3583\n",
            "loss_train 0.11796476519572119\n",
            "iteration 3584\n",
            "loss_train 0.11795452645089412\n",
            "iteration 3585\n",
            "loss_train 0.11794429163967944\n",
            "iteration 3586\n",
            "loss_train 0.11793406075964269\n",
            "iteration 3587\n",
            "loss_train 0.11792383380835167\n",
            "iteration 3588\n",
            "loss_train 0.11791361078337617\n",
            "iteration 3589\n",
            "loss_train 0.11790339168228817\n",
            "iteration 3590\n",
            "loss_train 0.11789317650266179\n",
            "iteration 3591\n",
            "loss_train 0.11788296524207316\n",
            "iteration 3592\n",
            "loss_train 0.11787275789810063\n",
            "iteration 3593\n",
            "loss_train 0.11786255446832454\n",
            "iteration 3594\n",
            "loss_train 0.11785235495032745\n",
            "iteration 3595\n",
            "loss_train 0.11784215934169402\n",
            "iteration 3596\n",
            "loss_train 0.1178319676400109\n",
            "iteration 3597\n",
            "loss_train 0.11782177984286692\n",
            "iteration 3598\n",
            "loss_train 0.11781159594785305\n",
            "iteration 3599\n",
            "loss_train 0.11780141595256222\n",
            "iteration 3600\n",
            "loss_train 0.11779123985458957\n",
            "iteration 3601\n",
            "loss_train 0.11778106765153228\n",
            "iteration 3602\n",
            "loss_train 0.11777089934098962\n",
            "iteration 3603\n",
            "loss_train 0.117760734920563\n",
            "iteration 3604\n",
            "loss_train 0.11775057438785581\n",
            "iteration 3605\n",
            "loss_train 0.11774041774047356\n",
            "iteration 3606\n",
            "loss_train 0.1177302649760239\n",
            "iteration 3607\n",
            "loss_train 0.11772011609211645\n",
            "iteration 3608\n",
            "loss_train 0.11770997108636301\n",
            "iteration 3609\n",
            "loss_train 0.11769982995637739\n",
            "iteration 3610\n",
            "loss_train 0.11768969269977543\n",
            "iteration 3611\n",
            "loss_train 0.11767955931417512\n",
            "iteration 3612\n",
            "loss_train 0.11766942979719645\n",
            "iteration 3613\n",
            "loss_train 0.11765930414646154\n",
            "iteration 3614\n",
            "loss_train 0.11764918235959447\n",
            "iteration 3615\n",
            "loss_train 0.11763906443422148\n",
            "iteration 3616\n",
            "loss_train 0.11762895036797077\n",
            "iteration 3617\n",
            "loss_train 0.11761884015847265\n",
            "iteration 3618\n",
            "loss_train 0.1176087338033595\n",
            "iteration 3619\n",
            "loss_train 0.11759863130026567\n",
            "iteration 3620\n",
            "loss_train 0.1175885326468276\n",
            "iteration 3621\n",
            "loss_train 0.11757843784068382\n",
            "iteration 3622\n",
            "loss_train 0.11756834687947476\n",
            "iteration 3623\n",
            "loss_train 0.11755825976084305\n",
            "iteration 3624\n",
            "loss_train 0.11754817648243322\n",
            "iteration 3625\n",
            "loss_train 0.11753809704189198\n",
            "iteration 3626\n",
            "loss_train 0.11752802143686795\n",
            "iteration 3627\n",
            "loss_train 0.1175179496650118\n",
            "iteration 3628\n",
            "loss_train 0.11750788172397629\n",
            "iteration 3629\n",
            "loss_train 0.11749781761141607\n",
            "iteration 3630\n",
            "loss_train 0.117487757324988\n",
            "iteration 3631\n",
            "loss_train 0.11747770086235078\n",
            "iteration 3632\n",
            "loss_train 0.11746764822116523\n",
            "iteration 3633\n",
            "loss_train 0.1174575993990942\n",
            "iteration 3634\n",
            "loss_train 0.11744755439380247\n",
            "iteration 3635\n",
            "loss_train 0.11743751320295688\n",
            "iteration 3636\n",
            "loss_train 0.11742747582422627\n",
            "iteration 3637\n",
            "loss_train 0.11741744225528153\n",
            "iteration 3638\n",
            "loss_train 0.11740741249379544\n",
            "iteration 3639\n",
            "loss_train 0.11739738653744289\n",
            "iteration 3640\n",
            "loss_train 0.11738736438390074\n",
            "iteration 3641\n",
            "loss_train 0.11737734603084785\n",
            "iteration 3642\n",
            "loss_train 0.11736733147596505\n",
            "iteration 3643\n",
            "loss_train 0.11735732071693514\n",
            "iteration 3644\n",
            "loss_train 0.11734731375144299\n",
            "iteration 3645\n",
            "loss_train 0.11733731057717547\n",
            "iteration 3646\n",
            "loss_train 0.11732731119182128\n",
            "iteration 3647\n",
            "loss_train 0.11731731559307122\n",
            "iteration 3648\n",
            "loss_train 0.1173073237786181\n",
            "iteration 3649\n",
            "loss_train 0.11729733574615667\n",
            "iteration 3650\n",
            "loss_train 0.11728735149338364\n",
            "iteration 3651\n",
            "loss_train 0.11727737101799764\n",
            "iteration 3652\n",
            "loss_train 0.11726739431769946\n",
            "iteration 3653\n",
            "loss_train 0.11725742139019164\n",
            "iteration 3654\n",
            "loss_train 0.11724745223317887\n",
            "iteration 3655\n",
            "loss_train 0.11723748684436766\n",
            "iteration 3656\n",
            "loss_train 0.11722752522146655\n",
            "iteration 3657\n",
            "loss_train 0.11721756736218608\n",
            "iteration 3658\n",
            "loss_train 0.11720761326423869\n",
            "iteration 3659\n",
            "loss_train 0.11719766292533881\n",
            "iteration 3660\n",
            "loss_train 0.11718771634320277\n",
            "iteration 3661\n",
            "loss_train 0.11717777351554896\n",
            "iteration 3662\n",
            "loss_train 0.11716783444009761\n",
            "iteration 3663\n",
            "loss_train 0.11715789911457092\n",
            "iteration 3664\n",
            "loss_train 0.11714796753669314\n",
            "iteration 3665\n",
            "loss_train 0.1171380397041903\n",
            "iteration 3666\n",
            "loss_train 0.11712811561479053\n",
            "iteration 3667\n",
            "loss_train 0.11711819526622373\n",
            "iteration 3668\n",
            "loss_train 0.11710827865622195\n",
            "iteration 3669\n",
            "loss_train 0.11709836578251899\n",
            "iteration 3670\n",
            "loss_train 0.11708845664285066\n",
            "iteration 3671\n",
            "loss_train 0.11707855123495468\n",
            "iteration 3672\n",
            "loss_train 0.11706864955657076\n",
            "iteration 3673\n",
            "loss_train 0.11705875160544044\n",
            "iteration 3674\n",
            "loss_train 0.11704885737930727\n",
            "iteration 3675\n",
            "loss_train 0.11703896687591667\n",
            "iteration 3676\n",
            "loss_train 0.11702908009301599\n",
            "iteration 3677\n",
            "loss_train 0.1170191970283545\n",
            "iteration 3678\n",
            "loss_train 0.11700931767968342\n",
            "iteration 3679\n",
            "loss_train 0.11699944204475583\n",
            "iteration 3680\n",
            "loss_train 0.1169895701213268\n",
            "iteration 3681\n",
            "loss_train 0.11697970190715318\n",
            "iteration 3682\n",
            "loss_train 0.11696983739999384\n",
            "iteration 3683\n",
            "loss_train 0.11695997659760955\n",
            "iteration 3684\n",
            "loss_train 0.11695011949776293\n",
            "iteration 3685\n",
            "loss_train 0.11694026609821849\n",
            "iteration 3686\n",
            "loss_train 0.1169304163967428\n",
            "iteration 3687\n",
            "loss_train 0.11692057039110404\n",
            "iteration 3688\n",
            "loss_train 0.11691072807907253\n",
            "iteration 3689\n",
            "loss_train 0.11690088945842043\n",
            "iteration 3690\n",
            "loss_train 0.11689105452692168\n",
            "iteration 3691\n",
            "loss_train 0.11688122328235226\n",
            "iteration 3692\n",
            "loss_train 0.11687139572248996\n",
            "iteration 3693\n",
            "loss_train 0.11686157184511442\n",
            "iteration 3694\n",
            "loss_train 0.11685175164800726\n",
            "iteration 3695\n",
            "loss_train 0.11684193512895187\n",
            "iteration 3696\n",
            "loss_train 0.1168321222857336\n",
            "iteration 3697\n",
            "loss_train 0.11682231311613965\n",
            "iteration 3698\n",
            "loss_train 0.11681250761795911\n",
            "iteration 3699\n",
            "loss_train 0.11680270578898289\n",
            "iteration 3700\n",
            "loss_train 0.1167929076270038\n",
            "iteration 3701\n",
            "loss_train 0.11678311312981661\n",
            "iteration 3702\n",
            "loss_train 0.11677332229521778\n",
            "iteration 3703\n",
            "loss_train 0.11676353512100571\n",
            "iteration 3704\n",
            "loss_train 0.11675375160498078\n",
            "iteration 3705\n",
            "loss_train 0.11674397174494501\n",
            "iteration 3706\n",
            "loss_train 0.11673419553870247\n",
            "iteration 3707\n",
            "loss_train 0.116724422984059\n",
            "iteration 3708\n",
            "loss_train 0.11671465407882224\n",
            "iteration 3709\n",
            "loss_train 0.11670488882080185\n",
            "iteration 3710\n",
            "loss_train 0.11669512720780915\n",
            "iteration 3711\n",
            "loss_train 0.11668536923765743\n",
            "iteration 3712\n",
            "loss_train 0.11667561490816178\n",
            "iteration 3713\n",
            "loss_train 0.11666586421713913\n",
            "iteration 3714\n",
            "loss_train 0.11665611716240827\n",
            "iteration 3715\n",
            "loss_train 0.1166463737417898\n",
            "iteration 3716\n",
            "loss_train 0.11663663395310622\n",
            "iteration 3717\n",
            "loss_train 0.1166268977941818\n",
            "iteration 3718\n",
            "loss_train 0.11661716526284266\n",
            "iteration 3719\n",
            "loss_train 0.11660743635691678\n",
            "iteration 3720\n",
            "loss_train 0.11659771107423397\n",
            "iteration 3721\n",
            "loss_train 0.11658798941262577\n",
            "iteration 3722\n",
            "loss_train 0.11657827136992567\n",
            "iteration 3723\n",
            "loss_train 0.11656855694396895\n",
            "iteration 3724\n",
            "loss_train 0.11655884613259267\n",
            "iteration 3725\n",
            "loss_train 0.11654913893363573\n",
            "iteration 3726\n",
            "loss_train 0.11653943534493891\n",
            "iteration 3727\n",
            "loss_train 0.11652973536434467\n",
            "iteration 3728\n",
            "loss_train 0.11652003898969737\n",
            "iteration 3729\n",
            "loss_train 0.11651034621884324\n",
            "iteration 3730\n",
            "loss_train 0.11650065704963015\n",
            "iteration 3731\n",
            "loss_train 0.11649097147990801\n",
            "iteration 3732\n",
            "loss_train 0.1164812895075283\n",
            "iteration 3733\n",
            "loss_train 0.1164716111303444\n",
            "iteration 3734\n",
            "loss_train 0.11646193634621159\n",
            "iteration 3735\n",
            "loss_train 0.1164522651529868\n",
            "iteration 3736\n",
            "loss_train 0.11644259754852883\n",
            "iteration 3737\n",
            "loss_train 0.11643293353069827\n",
            "iteration 3738\n",
            "loss_train 0.11642327309735742\n",
            "iteration 3739\n",
            "loss_train 0.11641361624637057\n",
            "iteration 3740\n",
            "loss_train 0.1164039629756036\n",
            "iteration 3741\n",
            "loss_train 0.1163943132829243\n",
            "iteration 3742\n",
            "loss_train 0.11638466716620216\n",
            "iteration 3743\n",
            "loss_train 0.11637502462330854\n",
            "iteration 3744\n",
            "loss_train 0.11636538565211646\n",
            "iteration 3745\n",
            "loss_train 0.11635575025050086\n",
            "iteration 3746\n",
            "loss_train 0.11634611841633842\n",
            "iteration 3747\n",
            "loss_train 0.1163364901475075\n",
            "iteration 3748\n",
            "loss_train 0.11632686544188833\n",
            "iteration 3749\n",
            "loss_train 0.11631724429736293\n",
            "iteration 3750\n",
            "loss_train 0.11630762671181497\n",
            "iteration 3751\n",
            "loss_train 0.11629801268313002\n",
            "iteration 3752\n",
            "loss_train 0.11628840220919531\n",
            "iteration 3753\n",
            "loss_train 0.11627879528789999\n",
            "iteration 3754\n",
            "loss_train 0.11626919191713478\n",
            "iteration 3755\n",
            "loss_train 0.11625959209479224\n",
            "iteration 3756\n",
            "loss_train 0.11624999581876676\n",
            "iteration 3757\n",
            "loss_train 0.11624040308695442\n",
            "iteration 3758\n",
            "loss_train 0.11623081389725302\n",
            "iteration 3759\n",
            "loss_train 0.11622122824756219\n",
            "iteration 3760\n",
            "loss_train 0.11621164613578323\n",
            "iteration 3761\n",
            "loss_train 0.11620206755981928\n",
            "iteration 3762\n",
            "loss_train 0.11619249251757514\n",
            "iteration 3763\n",
            "loss_train 0.11618292100695744\n",
            "iteration 3764\n",
            "loss_train 0.11617335302587452\n",
            "iteration 3765\n",
            "loss_train 0.11616378857223636\n",
            "iteration 3766\n",
            "loss_train 0.11615422764395489\n",
            "iteration 3767\n",
            "loss_train 0.11614467023894355\n",
            "iteration 3768\n",
            "loss_train 0.11613511635511772\n",
            "iteration 3769\n",
            "loss_train 0.11612556599039434\n",
            "iteration 3770\n",
            "loss_train 0.11611601914269222\n",
            "iteration 3771\n",
            "loss_train 0.11610647580993182\n",
            "iteration 3772\n",
            "loss_train 0.11609693599003536\n",
            "iteration 3773\n",
            "loss_train 0.11608739968092675\n",
            "iteration 3774\n",
            "loss_train 0.11607786688053164\n",
            "iteration 3775\n",
            "loss_train 0.11606833758677751\n",
            "iteration 3776\n",
            "loss_train 0.11605881179759334\n",
            "iteration 3777\n",
            "loss_train 0.11604928951091005\n",
            "iteration 3778\n",
            "loss_train 0.11603977072466014\n",
            "iteration 3779\n",
            "loss_train 0.11603025543677786\n",
            "iteration 3780\n",
            "loss_train 0.11602074364519921\n",
            "iteration 3781\n",
            "loss_train 0.11601123534786188\n",
            "iteration 3782\n",
            "loss_train 0.1160017305427052\n",
            "iteration 3783\n",
            "loss_train 0.1159922292276703\n",
            "iteration 3784\n",
            "loss_train 0.11598273140070002\n",
            "iteration 3785\n",
            "loss_train 0.1159732370597388\n",
            "iteration 3786\n",
            "loss_train 0.11596374620273299\n",
            "iteration 3787\n",
            "loss_train 0.11595425882763036\n",
            "iteration 3788\n",
            "loss_train 0.11594477493238056\n",
            "iteration 3789\n",
            "loss_train 0.11593529451493492\n",
            "iteration 3790\n",
            "loss_train 0.11592581757324649\n",
            "iteration 3791\n",
            "loss_train 0.11591634410526987\n",
            "iteration 3792\n",
            "loss_train 0.11590687410896151\n",
            "iteration 3793\n",
            "loss_train 0.11589740758227947\n",
            "iteration 3794\n",
            "loss_train 0.11588794452318356\n",
            "iteration 3795\n",
            "loss_train 0.11587848492963516\n",
            "iteration 3796\n",
            "loss_train 0.11586902879959743\n",
            "iteration 3797\n",
            "loss_train 0.11585957613103529\n",
            "iteration 3798\n",
            "loss_train 0.11585012692191504\n",
            "iteration 3799\n",
            "loss_train 0.11584068117020509\n",
            "iteration 3800\n",
            "loss_train 0.11583123887387511\n",
            "iteration 3801\n",
            "loss_train 0.11582180003089669\n",
            "iteration 3802\n",
            "loss_train 0.11581236463924308\n",
            "iteration 3803\n",
            "loss_train 0.11580293269688911\n",
            "iteration 3804\n",
            "loss_train 0.11579350420181135\n",
            "iteration 3805\n",
            "loss_train 0.11578407915198799\n",
            "iteration 3806\n",
            "loss_train 0.11577465754539892\n",
            "iteration 3807\n",
            "loss_train 0.11576523938002571\n",
            "iteration 3808\n",
            "loss_train 0.11575582465385155\n",
            "iteration 3809\n",
            "loss_train 0.11574641336486127\n",
            "iteration 3810\n",
            "loss_train 0.11573700551104148\n",
            "iteration 3811\n",
            "loss_train 0.11572760109038024\n",
            "iteration 3812\n",
            "loss_train 0.11571820010086754\n",
            "iteration 3813\n",
            "loss_train 0.11570880254049475\n",
            "iteration 3814\n",
            "loss_train 0.11569940840725507\n",
            "iteration 3815\n",
            "loss_train 0.1156900176991433\n",
            "iteration 3816\n",
            "loss_train 0.11568063041415587\n",
            "iteration 3817\n",
            "loss_train 0.11567124655029087\n",
            "iteration 3818\n",
            "loss_train 0.11566186610554804\n",
            "iteration 3819\n",
            "loss_train 0.11565248907792877\n",
            "iteration 3820\n",
            "loss_train 0.11564311546543606\n",
            "iteration 3821\n",
            "loss_train 0.11563374526607459\n",
            "iteration 3822\n",
            "loss_train 0.11562437847785065\n",
            "iteration 3823\n",
            "loss_train 0.11561501509877212\n",
            "iteration 3824\n",
            "loss_train 0.11560565512684867\n",
            "iteration 3825\n",
            "loss_train 0.11559629856009142\n",
            "iteration 3826\n",
            "loss_train 0.11558694539651324\n",
            "iteration 3827\n",
            "loss_train 0.11557759563412859\n",
            "iteration 3828\n",
            "loss_train 0.11556824927095352\n",
            "iteration 3829\n",
            "loss_train 0.11555890630500579\n",
            "iteration 3830\n",
            "loss_train 0.11554956673430473\n",
            "iteration 3831\n",
            "loss_train 0.11554023055687128\n",
            "iteration 3832\n",
            "loss_train 0.11553089777072803\n",
            "iteration 3833\n",
            "loss_train 0.1155215683738992\n",
            "iteration 3834\n",
            "loss_train 0.1155122423644105\n",
            "iteration 3835\n",
            "loss_train 0.11550291974028952\n",
            "iteration 3836\n",
            "loss_train 0.11549360049956521\n",
            "iteration 3837\n",
            "loss_train 0.11548428464026823\n",
            "iteration 3838\n",
            "loss_train 0.11547497216043093\n",
            "iteration 3839\n",
            "loss_train 0.11546566305808705\n",
            "iteration 3840\n",
            "loss_train 0.11545635733127214\n",
            "iteration 3841\n",
            "loss_train 0.1154470549780233\n",
            "iteration 3842\n",
            "loss_train 0.11543775599637922\n",
            "iteration 3843\n",
            "loss_train 0.1154284603843802\n",
            "iteration 3844\n",
            "loss_train 0.11541916814006807\n",
            "iteration 3845\n",
            "loss_train 0.11540987926148638\n",
            "iteration 3846\n",
            "loss_train 0.11540059374668016\n",
            "iteration 3847\n",
            "loss_train 0.11539131159369613\n",
            "iteration 3848\n",
            "loss_train 0.11538203280058258\n",
            "iteration 3849\n",
            "loss_train 0.11537275736538936\n",
            "iteration 3850\n",
            "loss_train 0.11536348528616788\n",
            "iteration 3851\n",
            "loss_train 0.1153542165609713\n",
            "iteration 3852\n",
            "loss_train 0.11534495118785412\n",
            "iteration 3853\n",
            "loss_train 0.11533568916487257\n",
            "iteration 3854\n",
            "loss_train 0.11532643049008454\n",
            "iteration 3855\n",
            "loss_train 0.11531717516154931\n",
            "iteration 3856\n",
            "loss_train 0.11530792317732794\n",
            "iteration 3857\n",
            "loss_train 0.11529867453548284\n",
            "iteration 3858\n",
            "loss_train 0.11528942923407819\n",
            "iteration 3859\n",
            "loss_train 0.11528018727117967\n",
            "iteration 3860\n",
            "loss_train 0.1152709486448546\n",
            "iteration 3861\n",
            "loss_train 0.11526171335317167\n",
            "iteration 3862\n",
            "loss_train 0.11525248139420137\n",
            "iteration 3863\n",
            "loss_train 0.1152432527660157\n",
            "iteration 3864\n",
            "loss_train 0.11523402746668816\n",
            "iteration 3865\n",
            "loss_train 0.1152248054942938\n",
            "iteration 3866\n",
            "loss_train 0.11521558684690934\n",
            "iteration 3867\n",
            "loss_train 0.11520637152261298\n",
            "iteration 3868\n",
            "loss_train 0.11519715951948453\n",
            "iteration 3869\n",
            "loss_train 0.11518795083560528\n",
            "iteration 3870\n",
            "loss_train 0.1151787454690582\n",
            "iteration 3871\n",
            "loss_train 0.11516954341792765\n",
            "iteration 3872\n",
            "loss_train 0.1151603446802997\n",
            "iteration 3873\n",
            "loss_train 0.1151511492542619\n",
            "iteration 3874\n",
            "loss_train 0.11514195713790332\n",
            "iteration 3875\n",
            "loss_train 0.11513276832931467\n",
            "iteration 3876\n",
            "loss_train 0.1151235828265881\n",
            "iteration 3877\n",
            "loss_train 0.11511440062781739\n",
            "iteration 3878\n",
            "loss_train 0.11510522173109781\n",
            "iteration 3879\n",
            "loss_train 0.11509604613452622\n",
            "iteration 3880\n",
            "loss_train 0.11508687383620093\n",
            "iteration 3881\n",
            "loss_train 0.11507770483422192\n",
            "iteration 3882\n",
            "loss_train 0.11506853912669059\n",
            "iteration 3883\n",
            "loss_train 0.11505937671170989\n",
            "iteration 3884\n",
            "loss_train 0.11505021758738443\n",
            "iteration 3885\n",
            "loss_train 0.11504106175182013\n",
            "iteration 3886\n",
            "loss_train 0.11503190920312471\n",
            "iteration 3887\n",
            "loss_train 0.11502275993940715\n",
            "iteration 3888\n",
            "loss_train 0.11501361395877822\n",
            "iteration 3889\n",
            "loss_train 0.11500447125934991\n",
            "iteration 3890\n",
            "loss_train 0.11499533183923602\n",
            "iteration 3891\n",
            "loss_train 0.1149861956965517\n",
            "iteration 3892\n",
            "loss_train 0.11497706282941374\n",
            "iteration 3893\n",
            "loss_train 0.11496793323594029\n",
            "iteration 3894\n",
            "loss_train 0.11495880691425117\n",
            "iteration 3895\n",
            "loss_train 0.11494968386246764\n",
            "iteration 3896\n",
            "loss_train 0.11494056407871248\n",
            "iteration 3897\n",
            "loss_train 0.11493144756111007\n",
            "iteration 3898\n",
            "loss_train 0.11492233430778612\n",
            "iteration 3899\n",
            "loss_train 0.11491322431686803\n",
            "iteration 3900\n",
            "loss_train 0.11490411758648462\n",
            "iteration 3901\n",
            "loss_train 0.11489501411476617\n",
            "iteration 3902\n",
            "loss_train 0.11488591389984457\n",
            "iteration 3903\n",
            "loss_train 0.11487681693985315\n",
            "iteration 3904\n",
            "loss_train 0.11486772323292681\n",
            "iteration 3905\n",
            "loss_train 0.11485863277720185\n",
            "iteration 3906\n",
            "loss_train 0.11484954557081609\n",
            "iteration 3907\n",
            "loss_train 0.11484046161190896\n",
            "iteration 3908\n",
            "loss_train 0.11483138089862123\n",
            "iteration 3909\n",
            "loss_train 0.11482230342909523\n",
            "iteration 3910\n",
            "loss_train 0.11481322920147487\n",
            "iteration 3911\n",
            "loss_train 0.11480415821390536\n",
            "iteration 3912\n",
            "loss_train 0.11479509046453357\n",
            "iteration 3913\n",
            "loss_train 0.11478602595150773\n",
            "iteration 3914\n",
            "loss_train 0.11477696467297772\n",
            "iteration 3915\n",
            "loss_train 0.11476790662709477\n",
            "iteration 3916\n",
            "loss_train 0.11475885181201155\n",
            "iteration 3917\n",
            "loss_train 0.11474980022588238\n",
            "iteration 3918\n",
            "loss_train 0.11474075186686288\n",
            "iteration 3919\n",
            "loss_train 0.1147317067331103\n",
            "iteration 3920\n",
            "loss_train 0.1147226648227833\n",
            "iteration 3921\n",
            "loss_train 0.11471362613404201\n",
            "iteration 3922\n",
            "loss_train 0.11470459066504805\n",
            "iteration 3923\n",
            "loss_train 0.11469555841396445\n",
            "iteration 3924\n",
            "loss_train 0.11468652937895583\n",
            "iteration 3925\n",
            "loss_train 0.11467750355818815\n",
            "iteration 3926\n",
            "loss_train 0.114668480949829\n",
            "iteration 3927\n",
            "loss_train 0.11465946155204722\n",
            "iteration 3928\n",
            "loss_train 0.11465044536301329\n",
            "iteration 3929\n",
            "loss_train 0.11464143238089909\n",
            "iteration 3930\n",
            "loss_train 0.11463242260387797\n",
            "iteration 3931\n",
            "loss_train 0.11462341603012471\n",
            "iteration 3932\n",
            "loss_train 0.11461441265781562\n",
            "iteration 3933\n",
            "loss_train 0.11460541248512837\n",
            "iteration 3934\n",
            "loss_train 0.11459641551024216\n",
            "iteration 3935\n",
            "loss_train 0.11458742173133762\n",
            "iteration 3936\n",
            "loss_train 0.11457843114659685\n",
            "iteration 3937\n",
            "loss_train 0.11456944375420335\n",
            "iteration 3938\n",
            "loss_train 0.11456045955234208\n",
            "iteration 3939\n",
            "loss_train 0.11455147853919952\n",
            "iteration 3940\n",
            "loss_train 0.11454250071296354\n",
            "iteration 3941\n",
            "loss_train 0.11453352607182345\n",
            "iteration 3942\n",
            "loss_train 0.11452455461397003\n",
            "iteration 3943\n",
            "loss_train 0.11451558633759544\n",
            "iteration 3944\n",
            "loss_train 0.11450662124089336\n",
            "iteration 3945\n",
            "loss_train 0.11449765932205885\n",
            "iteration 3946\n",
            "loss_train 0.11448870057928848\n",
            "iteration 3947\n",
            "loss_train 0.11447974501078019\n",
            "iteration 3948\n",
            "loss_train 0.11447079261473332\n",
            "iteration 3949\n",
            "loss_train 0.11446184338934881\n",
            "iteration 3950\n",
            "loss_train 0.1144528973328288\n",
            "iteration 3951\n",
            "loss_train 0.11444395444337703\n",
            "iteration 3952\n",
            "loss_train 0.1144350147191986\n",
            "iteration 3953\n",
            "loss_train 0.11442607815850006\n",
            "iteration 3954\n",
            "loss_train 0.11441714475948936\n",
            "iteration 3955\n",
            "loss_train 0.11440821452037596\n",
            "iteration 3956\n",
            "loss_train 0.11439928743937058\n",
            "iteration 3957\n",
            "loss_train 0.11439036351468548\n",
            "iteration 3958\n",
            "loss_train 0.11438144274453436\n",
            "iteration 3959\n",
            "loss_train 0.11437252512713224\n",
            "iteration 3960\n",
            "loss_train 0.11436361066069566\n",
            "iteration 3961\n",
            "loss_train 0.11435469934344247\n",
            "iteration 3962\n",
            "loss_train 0.11434579117359205\n",
            "iteration 3963\n",
            "loss_train 0.11433688614936502\n",
            "iteration 3964\n",
            "loss_train 0.11432798426898365\n",
            "iteration 3965\n",
            "loss_train 0.11431908553067141\n",
            "iteration 3966\n",
            "loss_train 0.1143101899326533\n",
            "iteration 3967\n",
            "loss_train 0.11430129747315568\n",
            "iteration 3968\n",
            "loss_train 0.11429240815040628\n",
            "iteration 3969\n",
            "loss_train 0.11428352196263433\n",
            "iteration 3970\n",
            "loss_train 0.11427463890807034\n",
            "iteration 3971\n",
            "loss_train 0.11426575898494634\n",
            "iteration 3972\n",
            "loss_train 0.1142568821914957\n",
            "iteration 3973\n",
            "loss_train 0.11424800852595315\n",
            "iteration 3974\n",
            "loss_train 0.1142391379865549\n",
            "iteration 3975\n",
            "loss_train 0.11423027057153849\n",
            "iteration 3976\n",
            "loss_train 0.11422140627914291\n",
            "iteration 3977\n",
            "loss_train 0.11421254510760846\n",
            "iteration 3978\n",
            "loss_train 0.11420368705517694\n",
            "iteration 3979\n",
            "loss_train 0.11419483212009143\n",
            "iteration 3980\n",
            "loss_train 0.11418598030059647\n",
            "iteration 3981\n",
            "loss_train 0.11417713159493798\n",
            "iteration 3982\n",
            "loss_train 0.11416828600136318\n",
            "iteration 3983\n",
            "loss_train 0.11415944351812081\n",
            "iteration 3984\n",
            "loss_train 0.11415060414346086\n",
            "iteration 3985\n",
            "loss_train 0.11414176787563486\n",
            "iteration 3986\n",
            "loss_train 0.11413293471289553\n",
            "iteration 3987\n",
            "loss_train 0.11412410465349712\n",
            "iteration 3988\n",
            "loss_train 0.1141152776956952\n",
            "iteration 3989\n",
            "loss_train 0.1141064538377466\n",
            "iteration 3990\n",
            "loss_train 0.11409763307790982\n",
            "iteration 3991\n",
            "loss_train 0.11408881541444438\n",
            "iteration 3992\n",
            "loss_train 0.11408000084561147\n",
            "iteration 3993\n",
            "loss_train 0.11407118936967345\n",
            "iteration 3994\n",
            "loss_train 0.11406238098489405\n",
            "iteration 3995\n",
            "loss_train 0.11405357568953858\n",
            "iteration 3996\n",
            "loss_train 0.11404477348187346\n",
            "iteration 3997\n",
            "loss_train 0.11403597436016662\n",
            "iteration 3998\n",
            "loss_train 0.11402717832268731\n",
            "iteration 3999\n",
            "loss_train 0.11401838536770613\n",
            "iteration 4000\n",
            "loss_train 0.11400959549349508\n",
            "iteration 4001\n",
            "loss_train 0.11400080869832746\n",
            "iteration 4002\n",
            "loss_train 0.11399202498047802\n",
            "iteration 4003\n",
            "loss_train 0.11398324433822271\n",
            "iteration 4004\n",
            "loss_train 0.11397446676983904\n",
            "iteration 4005\n",
            "loss_train 0.11396569227360566\n",
            "iteration 4006\n",
            "loss_train 0.1139569208478027\n",
            "iteration 4007\n",
            "loss_train 0.11394815249071168\n",
            "iteration 4008\n",
            "loss_train 0.1139393872006153\n",
            "iteration 4009\n",
            "loss_train 0.11393062497579777\n",
            "iteration 4010\n",
            "loss_train 0.11392186581454454\n",
            "iteration 4011\n",
            "loss_train 0.11391310971514246\n",
            "iteration 4012\n",
            "loss_train 0.11390435667587977\n",
            "iteration 4013\n",
            "loss_train 0.11389560669504593\n",
            "iteration 4014\n",
            "loss_train 0.1138868597709318\n",
            "iteration 4015\n",
            "loss_train 0.11387811590182963\n",
            "iteration 4016\n",
            "loss_train 0.1138693750860329\n",
            "iteration 4017\n",
            "loss_train 0.11386063732183652\n",
            "iteration 4018\n",
            "loss_train 0.11385190260753669\n",
            "iteration 4019\n",
            "loss_train 0.11384317094143095\n",
            "iteration 4020\n",
            "loss_train 0.1138344423218182\n",
            "iteration 4021\n",
            "loss_train 0.11382571674699858\n",
            "iteration 4022\n",
            "loss_train 0.11381699421527372\n",
            "iteration 4023\n",
            "loss_train 0.11380827472494641\n",
            "iteration 4024\n",
            "loss_train 0.11379955827432084\n",
            "iteration 4025\n",
            "loss_train 0.11379084486170257\n",
            "iteration 4026\n",
            "loss_train 0.11378213448539842\n",
            "iteration 4027\n",
            "loss_train 0.11377342714371654\n",
            "iteration 4028\n",
            "loss_train 0.11376472283496639\n",
            "iteration 4029\n",
            "loss_train 0.11375602155745886\n",
            "iteration 4030\n",
            "loss_train 0.113747323309506\n",
            "iteration 4031\n",
            "loss_train 0.1137386280894212\n",
            "iteration 4032\n",
            "loss_train 0.11372993589551934\n",
            "iteration 4033\n",
            "loss_train 0.1137212467261164\n",
            "iteration 4034\n",
            "loss_train 0.11371256057952982\n",
            "iteration 4035\n",
            "loss_train 0.11370387745407823\n",
            "iteration 4036\n",
            "loss_train 0.1136951973480817\n",
            "iteration 4037\n",
            "loss_train 0.11368652025986144\n",
            "iteration 4038\n",
            "loss_train 0.11367784618774021\n",
            "iteration 4039\n",
            "loss_train 0.11366917513004182\n",
            "iteration 4040\n",
            "loss_train 0.11366050708509158\n",
            "iteration 4041\n",
            "loss_train 0.113651842051216\n",
            "iteration 4042\n",
            "loss_train 0.11364318002674292\n",
            "iteration 4043\n",
            "loss_train 0.11363452101000145\n",
            "iteration 4044\n",
            "loss_train 0.11362586499932209\n",
            "iteration 4045\n",
            "loss_train 0.11361721199303658\n",
            "iteration 4046\n",
            "loss_train 0.11360856198947791\n",
            "iteration 4047\n",
            "loss_train 0.11359991498698041\n",
            "iteration 4048\n",
            "loss_train 0.11359127098387978\n",
            "iteration 4049\n",
            "loss_train 0.11358262997851283\n",
            "iteration 4050\n",
            "loss_train 0.11357399196921787\n",
            "iteration 4051\n",
            "loss_train 0.1135653569543344\n",
            "iteration 4052\n",
            "loss_train 0.11355672493220313\n",
            "iteration 4053\n",
            "loss_train 0.11354809590116623\n",
            "iteration 4054\n",
            "loss_train 0.11353946985956703\n",
            "iteration 4055\n",
            "loss_train 0.11353084680575018\n",
            "iteration 4056\n",
            "loss_train 0.11352222673806169\n",
            "iteration 4057\n",
            "loss_train 0.11351360965484865\n",
            "iteration 4058\n",
            "loss_train 0.11350499555445963\n",
            "iteration 4059\n",
            "loss_train 0.11349638443524447\n",
            "iteration 4060\n",
            "loss_train 0.11348777629555419\n",
            "iteration 4061\n",
            "loss_train 0.11347917113374108\n",
            "iteration 4062\n",
            "loss_train 0.11347056894815882\n",
            "iteration 4063\n",
            "loss_train 0.1134619697371623\n",
            "iteration 4064\n",
            "loss_train 0.11345337349910765\n",
            "iteration 4065\n",
            "loss_train 0.11344478023235234\n",
            "iteration 4066\n",
            "loss_train 0.1134361899352551\n",
            "iteration 4067\n",
            "loss_train 0.11342760260617582\n",
            "iteration 4068\n",
            "loss_train 0.11341901824347585\n",
            "iteration 4069\n",
            "loss_train 0.11341043684551769\n",
            "iteration 4070\n",
            "loss_train 0.1134018584106651\n",
            "iteration 4071\n",
            "loss_train 0.11339328293728311\n",
            "iteration 4072\n",
            "loss_train 0.11338471042373803\n",
            "iteration 4073\n",
            "loss_train 0.11337614086839753\n",
            "iteration 4074\n",
            "loss_train 0.11336757426963036\n",
            "iteration 4075\n",
            "loss_train 0.11335901062580662\n",
            "iteration 4076\n",
            "loss_train 0.11335044993529769\n",
            "iteration 4077\n",
            "loss_train 0.11334189219647618\n",
            "iteration 4078\n",
            "loss_train 0.11333333740771594\n",
            "iteration 4079\n",
            "loss_train 0.11332478556739206\n",
            "iteration 4080\n",
            "loss_train 0.11331623667388101\n",
            "iteration 4081\n",
            "loss_train 0.11330769072556036\n",
            "iteration 4082\n",
            "loss_train 0.11329914772080901\n",
            "iteration 4083\n",
            "loss_train 0.11329060765800711\n",
            "iteration 4084\n",
            "loss_train 0.11328207053553598\n",
            "iteration 4085\n",
            "loss_train 0.11327353635177827\n",
            "iteration 4086\n",
            "loss_train 0.11326500510511785\n",
            "iteration 4087\n",
            "loss_train 0.11325647679393987\n",
            "iteration 4088\n",
            "loss_train 0.11324795141663066\n",
            "iteration 4089\n",
            "loss_train 0.11323942897157782\n",
            "iteration 4090\n",
            "loss_train 0.11323090945717024\n",
            "iteration 4091\n",
            "loss_train 0.11322239287179796\n",
            "iteration 4092\n",
            "loss_train 0.11321387921385233\n",
            "iteration 4093\n",
            "loss_train 0.11320536848172585\n",
            "iteration 4094\n",
            "loss_train 0.11319686067381243\n",
            "iteration 4095\n",
            "loss_train 0.113188355788507\n",
            "iteration 4096\n",
            "loss_train 0.11317985382420587\n",
            "iteration 4097\n",
            "loss_train 0.11317135477930652\n",
            "iteration 4098\n",
            "loss_train 0.11316285865220771\n",
            "iteration 4099\n",
            "loss_train 0.11315436544130933\n",
            "iteration 4100\n",
            "loss_train 0.11314587514501263\n",
            "iteration 4101\n",
            "loss_train 0.11313738776172004\n",
            "iteration 4102\n",
            "loss_train 0.11312890328983513\n",
            "iteration 4103\n",
            "loss_train 0.11312042172776282\n",
            "iteration 4104\n",
            "loss_train 0.11311194307390923\n",
            "iteration 4105\n",
            "loss_train 0.1131034673266816\n",
            "iteration 4106\n",
            "loss_train 0.1130949944844885\n",
            "iteration 4107\n",
            "loss_train 0.11308652454573967\n",
            "iteration 4108\n",
            "loss_train 0.11307805750884611\n",
            "iteration 4109\n",
            "loss_train 0.11306959337222001\n",
            "iteration 4110\n",
            "loss_train 0.11306113213427474\n",
            "iteration 4111\n",
            "loss_train 0.11305267379342498\n",
            "iteration 4112\n",
            "loss_train 0.11304421834808655\n",
            "iteration 4113\n",
            "loss_train 0.11303576579667646\n",
            "iteration 4114\n",
            "loss_train 0.11302731613761303\n",
            "iteration 4115\n",
            "loss_train 0.11301886936931571\n",
            "iteration 4116\n",
            "loss_train 0.1130104254902052\n",
            "iteration 4117\n",
            "loss_train 0.11300198449870331\n",
            "iteration 4118\n",
            "loss_train 0.11299354639323322\n",
            "iteration 4119\n",
            "loss_train 0.11298511117221925\n",
            "iteration 4120\n",
            "loss_train 0.11297667883408685\n",
            "iteration 4121\n",
            "loss_train 0.11296824937726276\n",
            "iteration 4122\n",
            "loss_train 0.11295982280017486\n",
            "iteration 4123\n",
            "loss_train 0.11295139910125232\n",
            "iteration 4124\n",
            "loss_train 0.11294297827892538\n",
            "iteration 4125\n",
            "loss_train 0.11293456033162565\n",
            "iteration 4126\n",
            "loss_train 0.11292614525778578\n",
            "iteration 4127\n",
            "loss_train 0.11291773305583969\n",
            "iteration 4128\n",
            "loss_train 0.11290932372422244\n",
            "iteration 4129\n",
            "loss_train 0.11290091726137037\n",
            "iteration 4130\n",
            "loss_train 0.11289251366572098\n",
            "iteration 4131\n",
            "loss_train 0.1128841129357129\n",
            "iteration 4132\n",
            "loss_train 0.11287571506978607\n",
            "iteration 4133\n",
            "loss_train 0.11286732006638148\n",
            "iteration 4134\n",
            "loss_train 0.11285892792394138\n",
            "iteration 4135\n",
            "loss_train 0.11285053864090924\n",
            "iteration 4136\n",
            "loss_train 0.11284215221572967\n",
            "iteration 4137\n",
            "loss_train 0.11283376864684845\n",
            "iteration 4138\n",
            "loss_train 0.11282538793271263\n",
            "iteration 4139\n",
            "loss_train 0.11281701007177028\n",
            "iteration 4140\n",
            "loss_train 0.11280863506247082\n",
            "iteration 4141\n",
            "loss_train 0.11280026290326478\n",
            "iteration 4142\n",
            "loss_train 0.11279189359260382\n",
            "iteration 4143\n",
            "loss_train 0.11278352712894084\n",
            "iteration 4144\n",
            "loss_train 0.11277516351072994\n",
            "iteration 4145\n",
            "loss_train 0.11276680273642632\n",
            "iteration 4146\n",
            "loss_train 0.11275844480448641\n",
            "iteration 4147\n",
            "loss_train 0.11275008971336778\n",
            "iteration 4148\n",
            "loss_train 0.11274173746152917\n",
            "iteration 4149\n",
            "loss_train 0.11273338804743056\n",
            "iteration 4150\n",
            "loss_train 0.11272504146953301\n",
            "iteration 4151\n",
            "loss_train 0.11271669772629875\n",
            "iteration 4152\n",
            "loss_train 0.11270835681619122\n",
            "iteration 4153\n",
            "loss_train 0.11270001873767503\n",
            "iteration 4154\n",
            "loss_train 0.11269168348921596\n",
            "iteration 4155\n",
            "loss_train 0.11268335106928093\n",
            "iteration 4156\n",
            "loss_train 0.11267502147633797\n",
            "iteration 4157\n",
            "loss_train 0.1126666947088564\n",
            "iteration 4158\n",
            "loss_train 0.11265837076530656\n",
            "iteration 4159\n",
            "loss_train 0.1126500496441601\n",
            "iteration 4160\n",
            "loss_train 0.11264173134388965\n",
            "iteration 4161\n",
            "loss_train 0.11263341586296914\n",
            "iteration 4162\n",
            "loss_train 0.1126251031998736\n",
            "iteration 4163\n",
            "loss_train 0.11261679335307921\n",
            "iteration 4164\n",
            "loss_train 0.11260848632106331\n",
            "iteration 4165\n",
            "loss_train 0.11260018210230442\n",
            "iteration 4166\n",
            "loss_train 0.11259188069528216\n",
            "iteration 4167\n",
            "loss_train 0.11258358209847734\n",
            "iteration 4168\n",
            "loss_train 0.11257528631037192\n",
            "iteration 4169\n",
            "loss_train 0.11256699332944894\n",
            "iteration 4170\n",
            "loss_train 0.11255870315419271\n",
            "iteration 4171\n",
            "loss_train 0.11255041578308855\n",
            "iteration 4172\n",
            "loss_train 0.11254213121462303\n",
            "iteration 4173\n",
            "loss_train 0.11253384944728378\n",
            "iteration 4174\n",
            "loss_train 0.11252557047955966\n",
            "iteration 4175\n",
            "loss_train 0.11251729430994059\n",
            "iteration 4176\n",
            "loss_train 0.11250902093691768\n",
            "iteration 4177\n",
            "loss_train 0.11250075035898316\n",
            "iteration 4178\n",
            "loss_train 0.11249248257463039\n",
            "iteration 4179\n",
            "loss_train 0.1124842175823539\n",
            "iteration 4180\n",
            "loss_train 0.11247595538064933\n",
            "iteration 4181\n",
            "loss_train 0.11246769596801338\n",
            "iteration 4182\n",
            "loss_train 0.1124594393429441\n",
            "iteration 4183\n",
            "loss_train 0.11245118550394038\n",
            "iteration 4184\n",
            "loss_train 0.11244293444950247\n",
            "iteration 4185\n",
            "loss_train 0.11243468617813171\n",
            "iteration 4186\n",
            "loss_train 0.11242644068833046\n",
            "iteration 4187\n",
            "loss_train 0.11241819797860231\n",
            "iteration 4188\n",
            "loss_train 0.11240995804745188\n",
            "iteration 4189\n",
            "loss_train 0.11240172089338511\n",
            "iteration 4190\n",
            "loss_train 0.11239348651490878\n",
            "iteration 4191\n",
            "loss_train 0.11238525491053106\n",
            "iteration 4192\n",
            "loss_train 0.11237702607876109\n",
            "iteration 4193\n",
            "loss_train 0.11236880001810916\n",
            "iteration 4194\n",
            "loss_train 0.1123605767270867\n",
            "iteration 4195\n",
            "loss_train 0.11235235620420621\n",
            "iteration 4196\n",
            "loss_train 0.11234413844798145\n",
            "iteration 4197\n",
            "loss_train 0.11233592345692707\n",
            "iteration 4198\n",
            "loss_train 0.11232771122955901\n",
            "iteration 4199\n",
            "loss_train 0.11231950176439426\n",
            "iteration 4200\n",
            "loss_train 0.11231129505995094\n",
            "iteration 4201\n",
            "loss_train 0.1123030911147483\n",
            "iteration 4202\n",
            "loss_train 0.11229488992730663\n",
            "iteration 4203\n",
            "loss_train 0.11228669149614742\n",
            "iteration 4204\n",
            "loss_train 0.11227849581979324\n",
            "iteration 4205\n",
            "loss_train 0.1122703028967677\n",
            "iteration 4206\n",
            "loss_train 0.1122621127255956\n",
            "iteration 4207\n",
            "loss_train 0.11225392530480277\n",
            "iteration 4208\n",
            "loss_train 0.11224574063291635\n",
            "iteration 4209\n",
            "loss_train 0.1122375587084642\n",
            "iteration 4210\n",
            "loss_train 0.11222937952997566\n",
            "iteration 4211\n",
            "loss_train 0.11222120309598099\n",
            "iteration 4212\n",
            "loss_train 0.11221302940501153\n",
            "iteration 4213\n",
            "loss_train 0.11220485845559984\n",
            "iteration 4214\n",
            "loss_train 0.11219669024627946\n",
            "iteration 4215\n",
            "loss_train 0.11218852477558512\n",
            "iteration 4216\n",
            "loss_train 0.11218036204205253\n",
            "iteration 4217\n",
            "loss_train 0.1121722020442186\n",
            "iteration 4218\n",
            "loss_train 0.11216404478062131\n",
            "iteration 4219\n",
            "loss_train 0.1121558902497997\n",
            "iteration 4220\n",
            "loss_train 0.11214773845029395\n",
            "iteration 4221\n",
            "loss_train 0.11213958938064524\n",
            "iteration 4222\n",
            "loss_train 0.11213144303939603\n",
            "iteration 4223\n",
            "loss_train 0.11212329942508958\n",
            "iteration 4224\n",
            "loss_train 0.11211515853627056\n",
            "iteration 4225\n",
            "loss_train 0.11210702037148446\n",
            "iteration 4226\n",
            "loss_train 0.11209888492927797\n",
            "iteration 4227\n",
            "loss_train 0.11209075220819889\n",
            "iteration 4228\n",
            "loss_train 0.11208262220679607\n",
            "iteration 4229\n",
            "loss_train 0.11207449492361943\n",
            "iteration 4230\n",
            "loss_train 0.11206637035722\n",
            "iteration 4231\n",
            "loss_train 0.11205824850614986\n",
            "iteration 4232\n",
            "loss_train 0.11205012936896215\n",
            "iteration 4233\n",
            "loss_train 0.11204201294421115\n",
            "iteration 4234\n",
            "loss_train 0.11203389923045219\n",
            "iteration 4235\n",
            "loss_train 0.11202578822624167\n",
            "iteration 4236\n",
            "loss_train 0.11201767993013707\n",
            "iteration 4237\n",
            "loss_train 0.11200957434069697\n",
            "iteration 4238\n",
            "loss_train 0.11200147145648096\n",
            "iteration 4239\n",
            "loss_train 0.11199337127604972\n",
            "iteration 4240\n",
            "loss_train 0.11198527379796505\n",
            "iteration 4241\n",
            "loss_train 0.11197717902078977\n",
            "iteration 4242\n",
            "loss_train 0.11196908694308785\n",
            "iteration 4243\n",
            "loss_train 0.11196099756342416\n",
            "iteration 4244\n",
            "loss_train 0.11195291088036481\n",
            "iteration 4245\n",
            "loss_train 0.11194482689247692\n",
            "iteration 4246\n",
            "loss_train 0.11193674559832861\n",
            "iteration 4247\n",
            "loss_train 0.11192866699648914\n",
            "iteration 4248\n",
            "loss_train 0.11192059108552878\n",
            "iteration 4249\n",
            "loss_train 0.11191251786401897\n",
            "iteration 4250\n",
            "loss_train 0.11190444733053198\n",
            "iteration 4251\n",
            "loss_train 0.11189637948364148\n",
            "iteration 4252\n",
            "loss_train 0.11188831432192187\n",
            "iteration 4253\n",
            "loss_train 0.1118802518439488\n",
            "iteration 4254\n",
            "loss_train 0.11187219204829892\n",
            "iteration 4255\n",
            "loss_train 0.1118641349335499\n",
            "iteration 4256\n",
            "loss_train 0.11185608049828052\n",
            "iteration 4257\n",
            "loss_train 0.1118480287410706\n",
            "iteration 4258\n",
            "loss_train 0.11183997966050105\n",
            "iteration 4259\n",
            "loss_train 0.11183193325515371\n",
            "iteration 4260\n",
            "loss_train 0.11182388952361161\n",
            "iteration 4261\n",
            "loss_train 0.11181584846445874\n",
            "iteration 4262\n",
            "loss_train 0.11180781007628017\n",
            "iteration 4263\n",
            "loss_train 0.11179977435766199\n",
            "iteration 4264\n",
            "loss_train 0.11179174130719143\n",
            "iteration 4265\n",
            "loss_train 0.11178371092345665\n",
            "iteration 4266\n",
            "loss_train 0.1117756832050469\n",
            "iteration 4267\n",
            "loss_train 0.11176765815055247\n",
            "iteration 4268\n",
            "loss_train 0.11175963575856468\n",
            "iteration 4269\n",
            "loss_train 0.11175161602767596\n",
            "iteration 4270\n",
            "loss_train 0.1117435989564797\n",
            "iteration 4271\n",
            "loss_train 0.11173558454357035\n",
            "iteration 4272\n",
            "loss_train 0.11172757278754337\n",
            "iteration 4273\n",
            "loss_train 0.11171956368699539\n",
            "iteration 4274\n",
            "loss_train 0.11171155724052387\n",
            "iteration 4275\n",
            "loss_train 0.11170355344672747\n",
            "iteration 4276\n",
            "loss_train 0.11169555230420587\n",
            "iteration 4277\n",
            "loss_train 0.11168755381155968\n",
            "iteration 4278\n",
            "loss_train 0.11167955796739061\n",
            "iteration 4279\n",
            "loss_train 0.11167156477030143\n",
            "iteration 4280\n",
            "loss_train 0.11166357421889582\n",
            "iteration 4281\n",
            "loss_train 0.11165558631177866\n",
            "iteration 4282\n",
            "loss_train 0.11164760104755576\n",
            "iteration 4283\n",
            "loss_train 0.11163961842483397\n",
            "iteration 4284\n",
            "loss_train 0.11163163844222114\n",
            "iteration 4285\n",
            "loss_train 0.1116236610983262\n",
            "iteration 4286\n",
            "loss_train 0.11161568639175905\n",
            "iteration 4287\n",
            "loss_train 0.11160771432113067\n",
            "iteration 4288\n",
            "loss_train 0.11159974488505303\n",
            "iteration 4289\n",
            "loss_train 0.11159177808213912\n",
            "iteration 4290\n",
            "loss_train 0.11158381391100297\n",
            "iteration 4291\n",
            "loss_train 0.11157585237025956\n",
            "iteration 4292\n",
            "loss_train 0.11156789345852498\n",
            "iteration 4293\n",
            "loss_train 0.11155993717441635\n",
            "iteration 4294\n",
            "loss_train 0.1115519835165517\n",
            "iteration 4295\n",
            "loss_train 0.11154403248355015\n",
            "iteration 4296\n",
            "loss_train 0.11153608407403177\n",
            "iteration 4297\n",
            "loss_train 0.11152813828661777\n",
            "iteration 4298\n",
            "loss_train 0.11152019511993026\n",
            "iteration 4299\n",
            "loss_train 0.11151225457259241\n",
            "iteration 4300\n",
            "loss_train 0.1115043166432284\n",
            "iteration 4301\n",
            "loss_train 0.11149638133046333\n",
            "iteration 4302\n",
            "loss_train 0.11148844863292351\n",
            "iteration 4303\n",
            "loss_train 0.11148051854923602\n",
            "iteration 4304\n",
            "loss_train 0.11147259107802913\n",
            "iteration 4305\n",
            "loss_train 0.11146466621793198\n",
            "iteration 4306\n",
            "loss_train 0.11145674396757488\n",
            "iteration 4307\n",
            "loss_train 0.11144882432558902\n",
            "iteration 4308\n",
            "loss_train 0.11144090729060657\n",
            "iteration 4309\n",
            "loss_train 0.11143299286126077\n",
            "iteration 4310\n",
            "loss_train 0.11142508103618587\n",
            "iteration 4311\n",
            "loss_train 0.11141717181401707\n",
            "iteration 4312\n",
            "loss_train 0.11140926519339063\n",
            "iteration 4313\n",
            "loss_train 0.11140136117294369\n",
            "iteration 4314\n",
            "loss_train 0.11139345975131458\n",
            "iteration 4315\n",
            "loss_train 0.11138556092714243\n",
            "iteration 4316\n",
            "loss_train 0.1113776646990675\n",
            "iteration 4317\n",
            "loss_train 0.11136977106573097\n",
            "iteration 4318\n",
            "loss_train 0.11136188002577505\n",
            "iteration 4319\n",
            "loss_train 0.11135399157784294\n",
            "iteration 4320\n",
            "loss_train 0.1113461057205788\n",
            "iteration 4321\n",
            "loss_train 0.11133822245262782\n",
            "iteration 4322\n",
            "loss_train 0.11133034177263618\n",
            "iteration 4323\n",
            "loss_train 0.11132246367925104\n",
            "iteration 4324\n",
            "loss_train 0.11131458817112048\n",
            "iteration 4325\n",
            "loss_train 0.11130671524689369\n",
            "iteration 4326\n",
            "loss_train 0.1112988449052208\n",
            "iteration 4327\n",
            "loss_train 0.11129097714475286\n",
            "iteration 4328\n",
            "loss_train 0.11128311196414196\n",
            "iteration 4329\n",
            "loss_train 0.11127524936204122\n",
            "iteration 4330\n",
            "loss_train 0.11126738933710463\n",
            "iteration 4331\n",
            "loss_train 0.11125953188798725\n",
            "iteration 4332\n",
            "loss_train 0.11125167701334512\n",
            "iteration 4333\n",
            "loss_train 0.11124382471183522\n",
            "iteration 4334\n",
            "loss_train 0.11123597498211546\n",
            "iteration 4335\n",
            "loss_train 0.11122812782284486\n",
            "iteration 4336\n",
            "loss_train 0.11122028323268332\n",
            "iteration 4337\n",
            "loss_train 0.11121244121029172\n",
            "iteration 4338\n",
            "loss_train 0.11120460175433201\n",
            "iteration 4339\n",
            "loss_train 0.11119676486346695\n",
            "iteration 4340\n",
            "loss_train 0.11118893053636049\n",
            "iteration 4341\n",
            "loss_train 0.11118109877167726\n",
            "iteration 4342\n",
            "loss_train 0.11117326956808316\n",
            "iteration 4343\n",
            "loss_train 0.11116544292424486\n",
            "iteration 4344\n",
            "loss_train 0.1111576188388301\n",
            "iteration 4345\n",
            "loss_train 0.11114979731050756\n",
            "iteration 4346\n",
            "loss_train 0.11114197833794687\n",
            "iteration 4347\n",
            "loss_train 0.11113416191981865\n",
            "iteration 4348\n",
            "loss_train 0.11112634805479446\n",
            "iteration 4349\n",
            "loss_train 0.1111185367415469\n",
            "iteration 4350\n",
            "loss_train 0.1111107279787494\n",
            "iteration 4351\n",
            "loss_train 0.11110292176507645\n",
            "iteration 4352\n",
            "loss_train 0.1110951180992035\n",
            "iteration 4353\n",
            "loss_train 0.11108731697980698\n",
            "iteration 4354\n",
            "loss_train 0.11107951840556414\n",
            "iteration 4355\n",
            "loss_train 0.11107172237515338\n",
            "iteration 4356\n",
            "loss_train 0.11106392888725396\n",
            "iteration 4357\n",
            "loss_train 0.11105613794054608\n",
            "iteration 4358\n",
            "loss_train 0.11104834953371095\n",
            "iteration 4359\n",
            "loss_train 0.11104056366543068\n",
            "iteration 4360\n",
            "loss_train 0.11103278033438845\n",
            "iteration 4361\n",
            "loss_train 0.1110249995392682\n",
            "iteration 4362\n",
            "loss_train 0.11101722127875503\n",
            "iteration 4363\n",
            "loss_train 0.11100944555153484\n",
            "iteration 4364\n",
            "loss_train 0.11100167235629455\n",
            "iteration 4365\n",
            "loss_train 0.11099390169172203\n",
            "iteration 4366\n",
            "loss_train 0.11098613355650606\n",
            "iteration 4367\n",
            "loss_train 0.11097836794933644\n",
            "iteration 4368\n",
            "loss_train 0.11097060486890382\n",
            "iteration 4369\n",
            "loss_train 0.11096284431389992\n",
            "iteration 4370\n",
            "loss_train 0.11095508628301731\n",
            "iteration 4371\n",
            "loss_train 0.1109473307749495\n",
            "iteration 4372\n",
            "loss_train 0.11093957778839102\n",
            "iteration 4373\n",
            "loss_train 0.11093182732203728\n",
            "iteration 4374\n",
            "loss_train 0.11092407937458469\n",
            "iteration 4375\n",
            "loss_train 0.1109163339447305\n",
            "iteration 4376\n",
            "loss_train 0.110908591031173\n",
            "iteration 4377\n",
            "loss_train 0.11090085063261142\n",
            "iteration 4378\n",
            "loss_train 0.1108931127477458\n",
            "iteration 4379\n",
            "loss_train 0.11088537737527733\n",
            "iteration 4380\n",
            "loss_train 0.11087764451390796\n",
            "iteration 4381\n",
            "loss_train 0.11086991416234063\n",
            "iteration 4382\n",
            "loss_train 0.11086218631927926\n",
            "iteration 4383\n",
            "loss_train 0.11085446098342858\n",
            "iteration 4384\n",
            "loss_train 0.11084673815349445\n",
            "iteration 4385\n",
            "loss_train 0.11083901782818352\n",
            "iteration 4386\n",
            "loss_train 0.11083130000620339\n",
            "iteration 4387\n",
            "loss_train 0.1108235846862626\n",
            "iteration 4388\n",
            "loss_train 0.11081587186707063\n",
            "iteration 4389\n",
            "loss_train 0.11080816154733793\n",
            "iteration 4390\n",
            "loss_train 0.11080045372577575\n",
            "iteration 4391\n",
            "loss_train 0.11079274840109642\n",
            "iteration 4392\n",
            "loss_train 0.11078504557201309\n",
            "iteration 4393\n",
            "loss_train 0.11077734523723991\n",
            "iteration 4394\n",
            "loss_train 0.11076964739549186\n",
            "iteration 4395\n",
            "loss_train 0.11076195204548502\n",
            "iteration 4396\n",
            "loss_train 0.1107542591859362\n",
            "iteration 4397\n",
            "loss_train 0.11074656881556319\n",
            "iteration 4398\n",
            "loss_train 0.11073888093308476\n",
            "iteration 4399\n",
            "loss_train 0.11073119553722058\n",
            "iteration 4400\n",
            "loss_train 0.11072351262669117\n",
            "iteration 4401\n",
            "loss_train 0.11071583220021802\n",
            "iteration 4402\n",
            "loss_train 0.1107081542565236\n",
            "iteration 4403\n",
            "loss_train 0.11070047879433119\n",
            "iteration 4404\n",
            "loss_train 0.11069280581236506\n",
            "iteration 4405\n",
            "loss_train 0.11068513530935034\n",
            "iteration 4406\n",
            "loss_train 0.11067746728401316\n",
            "iteration 4407\n",
            "loss_train 0.11066980173508045\n",
            "iteration 4408\n",
            "loss_train 0.11066213866128011\n",
            "iteration 4409\n",
            "loss_train 0.11065447806134097\n",
            "iteration 4410\n",
            "loss_train 0.11064681993399281\n",
            "iteration 4411\n",
            "loss_train 0.1106391642779662\n",
            "iteration 4412\n",
            "loss_train 0.11063151109199268\n",
            "iteration 4413\n",
            "loss_train 0.1106238603748047\n",
            "iteration 4414\n",
            "loss_train 0.11061621212513566\n",
            "iteration 4415\n",
            "loss_train 0.11060856634171982\n",
            "iteration 4416\n",
            "loss_train 0.11060092302329233\n",
            "iteration 4417\n",
            "loss_train 0.11059328216858931\n",
            "iteration 4418\n",
            "loss_train 0.11058564377634772\n",
            "iteration 4419\n",
            "loss_train 0.11057800784530541\n",
            "iteration 4420\n",
            "loss_train 0.11057037437420124\n",
            "iteration 4421\n",
            "loss_train 0.11056274336177488\n",
            "iteration 4422\n",
            "loss_train 0.11055511480676694\n",
            "iteration 4423\n",
            "loss_train 0.11054748870791889\n",
            "iteration 4424\n",
            "loss_train 0.11053986506397309\n",
            "iteration 4425\n",
            "loss_train 0.11053224387367294\n",
            "iteration 4426\n",
            "loss_train 0.11052462513576256\n",
            "iteration 4427\n",
            "loss_train 0.110517008848987\n",
            "iteration 4428\n",
            "loss_train 0.11050939501209231\n",
            "iteration 4429\n",
            "loss_train 0.11050178362382539\n",
            "iteration 4430\n",
            "loss_train 0.11049417468293395\n",
            "iteration 4431\n",
            "loss_train 0.11048656818816667\n",
            "iteration 4432\n",
            "loss_train 0.11047896413827314\n",
            "iteration 4433\n",
            "loss_train 0.11047136253200382\n",
            "iteration 4434\n",
            "loss_train 0.11046376336811005\n",
            "iteration 4435\n",
            "loss_train 0.11045616664534402\n",
            "iteration 4436\n",
            "loss_train 0.11044857236245892\n",
            "iteration 4437\n",
            "loss_train 0.11044098051820872\n",
            "iteration 4438\n",
            "loss_train 0.11043339111134833\n",
            "iteration 4439\n",
            "loss_train 0.11042580414063358\n",
            "iteration 4440\n",
            "loss_train 0.11041821960482108\n",
            "iteration 4441\n",
            "loss_train 0.11041063750266843\n",
            "iteration 4442\n",
            "loss_train 0.11040305783293407\n",
            "iteration 4443\n",
            "loss_train 0.11039548059437733\n",
            "iteration 4444\n",
            "loss_train 0.11038790578575841\n",
            "iteration 4445\n",
            "loss_train 0.11038033340583842\n",
            "iteration 4446\n",
            "loss_train 0.11037276345337935\n",
            "iteration 4447\n",
            "loss_train 0.11036519592714404\n",
            "iteration 4448\n",
            "loss_train 0.11035763082589621\n",
            "iteration 4449\n",
            "loss_train 0.11035006814840052\n",
            "iteration 4450\n",
            "loss_train 0.11034250789342241\n",
            "iteration 4451\n",
            "loss_train 0.11033495005972832\n",
            "iteration 4452\n",
            "loss_train 0.11032739464608543\n",
            "iteration 4453\n",
            "loss_train 0.1103198416512619\n",
            "iteration 4454\n",
            "loss_train 0.11031229107402675\n",
            "iteration 4455\n",
            "loss_train 0.11030474291314979\n",
            "iteration 4456\n",
            "loss_train 0.11029719716740183\n",
            "iteration 4457\n",
            "loss_train 0.11028965383555449\n",
            "iteration 4458\n",
            "loss_train 0.11028211291638017\n",
            "iteration 4459\n",
            "loss_train 0.11027457440865235\n",
            "iteration 4460\n",
            "loss_train 0.11026703831114516\n",
            "iteration 4461\n",
            "loss_train 0.1102595046226338\n",
            "iteration 4462\n",
            "loss_train 0.11025197334189413\n",
            "iteration 4463\n",
            "loss_train 0.1102444444677031\n",
            "iteration 4464\n",
            "loss_train 0.11023691799883834\n",
            "iteration 4465\n",
            "loss_train 0.11022939393407846\n",
            "iteration 4466\n",
            "loss_train 0.11022187227220286\n",
            "iteration 4467\n",
            "loss_train 0.11021435301199184\n",
            "iteration 4468\n",
            "loss_train 0.11020683615222664\n",
            "iteration 4469\n",
            "loss_train 0.11019932169168922\n",
            "iteration 4470\n",
            "loss_train 0.1101918096291625\n",
            "iteration 4471\n",
            "loss_train 0.11018429996343015\n",
            "iteration 4472\n",
            "loss_train 0.11017679269327692\n",
            "iteration 4473\n",
            "loss_train 0.11016928781748819\n",
            "iteration 4474\n",
            "loss_train 0.11016178533485027\n",
            "iteration 4475\n",
            "loss_train 0.11015428524415041\n",
            "iteration 4476\n",
            "loss_train 0.11014678754417663\n",
            "iteration 4477\n",
            "loss_train 0.11013929223371786\n",
            "iteration 4478\n",
            "loss_train 0.1101317993115638\n",
            "iteration 4479\n",
            "loss_train 0.11012430877650513\n",
            "iteration 4480\n",
            "loss_train 0.11011682062733329\n",
            "iteration 4481\n",
            "loss_train 0.11010933486284057\n",
            "iteration 4482\n",
            "loss_train 0.11010185148182022\n",
            "iteration 4483\n",
            "loss_train 0.1100943704830662\n",
            "iteration 4484\n",
            "loss_train 0.11008689186537338\n",
            "iteration 4485\n",
            "loss_train 0.11007941562753755\n",
            "iteration 4486\n",
            "loss_train 0.11007194176835523\n",
            "iteration 4487\n",
            "loss_train 0.11006447028662389\n",
            "iteration 4488\n",
            "loss_train 0.11005700118114178\n",
            "iteration 4489\n",
            "loss_train 0.11004953445070807\n",
            "iteration 4490\n",
            "loss_train 0.11004207009412266\n",
            "iteration 4491\n",
            "loss_train 0.11003460811018638\n",
            "iteration 4492\n",
            "loss_train 0.11002714849770093\n",
            "iteration 4493\n",
            "loss_train 0.11001969125546876\n",
            "iteration 4494\n",
            "loss_train 0.11001223638229324\n",
            "iteration 4495\n",
            "loss_train 0.11000478387697861\n",
            "iteration 4496\n",
            "loss_train 0.1099973337383298\n",
            "iteration 4497\n",
            "loss_train 0.10998988596515274\n",
            "iteration 4498\n",
            "loss_train 0.10998244055625418\n",
            "iteration 4499\n",
            "loss_train 0.10997499751044162\n",
            "iteration 4500\n",
            "loss_train 0.10996755682652348\n",
            "iteration 4501\n",
            "loss_train 0.10996011850330892\n",
            "iteration 4502\n",
            "loss_train 0.10995268253960812\n",
            "iteration 4503\n",
            "loss_train 0.10994524893423185\n",
            "iteration 4504\n",
            "loss_train 0.109937817685992\n",
            "iteration 4505\n",
            "loss_train 0.10993038879370107\n",
            "iteration 4506\n",
            "loss_train 0.10992296225617242\n",
            "iteration 4507\n",
            "loss_train 0.10991553807222038\n",
            "iteration 4508\n",
            "loss_train 0.10990811624065995\n",
            "iteration 4509\n",
            "loss_train 0.10990069676030709\n",
            "iteration 4510\n",
            "loss_train 0.10989327962997852\n",
            "iteration 4511\n",
            "loss_train 0.10988586484849179\n",
            "iteration 4512\n",
            "loss_train 0.10987845241466536\n",
            "iteration 4513\n",
            "loss_train 0.10987104232731837\n",
            "iteration 4514\n",
            "loss_train 0.10986363458527096\n",
            "iteration 4515\n",
            "loss_train 0.1098562291873439\n",
            "iteration 4516\n",
            "loss_train 0.10984882613235898\n",
            "iteration 4517\n",
            "loss_train 0.10984142541913879\n",
            "iteration 4518\n",
            "loss_train 0.10983402704650658\n",
            "iteration 4519\n",
            "loss_train 0.10982663101328659\n",
            "iteration 4520\n",
            "loss_train 0.10981923731830383\n",
            "iteration 4521\n",
            "loss_train 0.10981184596038411\n",
            "iteration 4522\n",
            "loss_train 0.10980445693835411\n",
            "iteration 4523\n",
            "loss_train 0.1097970702510413\n",
            "iteration 4524\n",
            "loss_train 0.10978968589727395\n",
            "iteration 4525\n",
            "loss_train 0.10978230387588117\n",
            "iteration 4526\n",
            "loss_train 0.10977492418569294\n",
            "iteration 4527\n",
            "loss_train 0.10976754682554\n",
            "iteration 4528\n",
            "loss_train 0.10976017179425389\n",
            "iteration 4529\n",
            "loss_train 0.10975279909066707\n",
            "iteration 4530\n",
            "loss_train 0.10974542871361265\n",
            "iteration 4531\n",
            "loss_train 0.1097380606619247\n",
            "iteration 4532\n",
            "loss_train 0.10973069493443809\n",
            "iteration 4533\n",
            "loss_train 0.10972333152998841\n",
            "iteration 4534\n",
            "loss_train 0.10971597044741216\n",
            "iteration 4535\n",
            "loss_train 0.10970861168554655\n",
            "iteration 4536\n",
            "loss_train 0.10970125524322977\n",
            "iteration 4537\n",
            "loss_train 0.10969390111930064\n",
            "iteration 4538\n",
            "loss_train 0.1096865493125989\n",
            "iteration 4539\n",
            "loss_train 0.10967919982196506\n",
            "iteration 4540\n",
            "loss_train 0.10967185264624042\n",
            "iteration 4541\n",
            "loss_train 0.10966450778426719\n",
            "iteration 4542\n",
            "loss_train 0.10965716523488823\n",
            "iteration 4543\n",
            "loss_train 0.10964982499694732\n",
            "iteration 4544\n",
            "loss_train 0.10964248706928904\n",
            "iteration 4545\n",
            "loss_train 0.10963515145075872\n",
            "iteration 4546\n",
            "loss_train 0.10962781814020255\n",
            "iteration 4547\n",
            "loss_train 0.10962048713646745\n",
            "iteration 4548\n",
            "loss_train 0.10961315843840122\n",
            "iteration 4549\n",
            "loss_train 0.10960583204485244\n",
            "iteration 4550\n",
            "loss_train 0.10959850795467053\n",
            "iteration 4551\n",
            "loss_train 0.10959118616670556\n",
            "iteration 4552\n",
            "loss_train 0.10958386667980859\n",
            "iteration 4553\n",
            "loss_train 0.1095765494928314\n",
            "iteration 4554\n",
            "loss_train 0.10956923460462648\n",
            "iteration 4555\n",
            "loss_train 0.10956192201404728\n",
            "iteration 4556\n",
            "loss_train 0.10955461171994799\n",
            "iteration 4557\n",
            "loss_train 0.10954730372118353\n",
            "iteration 4558\n",
            "loss_train 0.10953999801660964\n",
            "iteration 4559\n",
            "loss_train 0.1095326946050829\n",
            "iteration 4560\n",
            "loss_train 0.1095253934854607\n",
            "iteration 4561\n",
            "loss_train 0.10951809465660114\n",
            "iteration 4562\n",
            "loss_train 0.10951079811736318\n",
            "iteration 4563\n",
            "loss_train 0.10950350386660652\n",
            "iteration 4564\n",
            "loss_train 0.10949621190319175\n",
            "iteration 4565\n",
            "loss_train 0.10948892222598013\n",
            "iteration 4566\n",
            "loss_train 0.10948163483383379\n",
            "iteration 4567\n",
            "loss_train 0.10947434972561557\n",
            "iteration 4568\n",
            "loss_train 0.10946706690018922\n",
            "iteration 4569\n",
            "loss_train 0.10945978635641916\n",
            "iteration 4570\n",
            "loss_train 0.10945250809317071\n",
            "iteration 4571\n",
            "loss_train 0.10944523210930984\n",
            "iteration 4572\n",
            "loss_train 0.10943795840370339\n",
            "iteration 4573\n",
            "loss_train 0.10943068697521906\n",
            "iteration 4574\n",
            "loss_train 0.10942341782272513\n",
            "iteration 4575\n",
            "loss_train 0.10941615094509088\n",
            "iteration 4576\n",
            "loss_train 0.10940888634118622\n",
            "iteration 4577\n",
            "loss_train 0.10940162400988192\n",
            "iteration 4578\n",
            "loss_train 0.10939436395004948\n",
            "iteration 4579\n",
            "loss_train 0.10938710616056126\n",
            "iteration 4580\n",
            "loss_train 0.10937985064029031\n",
            "iteration 4581\n",
            "loss_train 0.10937259738811053\n",
            "iteration 4582\n",
            "loss_train 0.10936534640289658\n",
            "iteration 4583\n",
            "loss_train 0.10935809768352382\n",
            "iteration 4584\n",
            "loss_train 0.10935085122886852\n",
            "iteration 4585\n",
            "loss_train 0.10934360703780768\n",
            "iteration 4586\n",
            "loss_train 0.109336365109219\n",
            "iteration 4587\n",
            "loss_train 0.10932912544198099\n",
            "iteration 4588\n",
            "loss_train 0.10932188803497309\n",
            "iteration 4589\n",
            "loss_train 0.10931465288707524\n",
            "iteration 4590\n",
            "loss_train 0.10930741999716836\n",
            "iteration 4591\n",
            "loss_train 0.1093001893641341\n",
            "iteration 4592\n",
            "loss_train 0.10929296098685484\n",
            "iteration 4593\n",
            "loss_train 0.10928573486421371\n",
            "iteration 4594\n",
            "loss_train 0.1092785109950947\n",
            "iteration 4595\n",
            "loss_train 0.10927128937838251\n",
            "iteration 4596\n",
            "loss_train 0.10926407001296261\n",
            "iteration 4597\n",
            "loss_train 0.10925685289772129\n",
            "iteration 4598\n",
            "loss_train 0.1092496380315455\n",
            "iteration 4599\n",
            "loss_train 0.10924242541332307\n",
            "iteration 4600\n",
            "loss_train 0.10923521504194257\n",
            "iteration 4601\n",
            "loss_train 0.10922800691629325\n",
            "iteration 4602\n",
            "loss_train 0.10922080103526524\n",
            "iteration 4603\n",
            "loss_train 0.10921359739774941\n",
            "iteration 4604\n",
            "loss_train 0.10920639600263733\n",
            "iteration 4605\n",
            "loss_train 0.10919919684882136\n",
            "iteration 4606\n",
            "loss_train 0.10919199993519468\n",
            "iteration 4607\n",
            "loss_train 0.10918480526065115\n",
            "iteration 4608\n",
            "loss_train 0.10917761282408546\n",
            "iteration 4609\n",
            "loss_train 0.10917042262439296\n",
            "iteration 4610\n",
            "loss_train 0.10916323466046993\n",
            "iteration 4611\n",
            "loss_train 0.10915604893121318\n",
            "iteration 4612\n",
            "loss_train 0.10914886543552058\n",
            "iteration 4613\n",
            "loss_train 0.1091416841722904\n",
            "iteration 4614\n",
            "loss_train 0.10913450514042199\n",
            "iteration 4615\n",
            "loss_train 0.10912732833881522\n",
            "iteration 4616\n",
            "loss_train 0.10912015376637085\n",
            "iteration 4617\n",
            "loss_train 0.10911298142199037\n",
            "iteration 4618\n",
            "loss_train 0.10910581130457599\n",
            "iteration 4619\n",
            "loss_train 0.10909864341303069\n",
            "iteration 4620\n",
            "loss_train 0.10909147774625823\n",
            "iteration 4621\n",
            "loss_train 0.10908431430316308\n",
            "iteration 4622\n",
            "loss_train 0.10907715308265047\n",
            "iteration 4623\n",
            "loss_train 0.10906999408362643\n",
            "iteration 4624\n",
            "loss_train 0.10906283730499765\n",
            "iteration 4625\n",
            "loss_train 0.10905568274567169\n",
            "iteration 4626\n",
            "loss_train 0.10904853040455671\n",
            "iteration 4627\n",
            "loss_train 0.10904138028056178\n",
            "iteration 4628\n",
            "loss_train 0.10903423237259656\n",
            "iteration 4629\n",
            "loss_train 0.10902708667957156\n",
            "iteration 4630\n",
            "loss_train 0.10901994320039804\n",
            "iteration 4631\n",
            "loss_train 0.10901280193398794\n",
            "iteration 4632\n",
            "loss_train 0.10900566287925398\n",
            "iteration 4633\n",
            "loss_train 0.10899852603510966\n",
            "iteration 4634\n",
            "loss_train 0.10899139140046914\n",
            "iteration 4635\n",
            "loss_train 0.1089842589742474\n",
            "iteration 4636\n",
            "loss_train 0.10897712875536013\n",
            "iteration 4637\n",
            "loss_train 0.10897000074272373\n",
            "iteration 4638\n",
            "loss_train 0.10896287493525542\n",
            "iteration 4639\n",
            "loss_train 0.10895575133187314\n",
            "iteration 4640\n",
            "loss_train 0.10894862993149547\n",
            "iteration 4641\n",
            "loss_train 0.10894151073304181\n",
            "iteration 4642\n",
            "loss_train 0.10893439373543237\n",
            "iteration 4643\n",
            "loss_train 0.10892727893758791\n",
            "iteration 4644\n",
            "loss_train 0.10892016633843012\n",
            "iteration 4645\n",
            "loss_train 0.10891305593688135\n",
            "iteration 4646\n",
            "loss_train 0.10890594773186461\n",
            "iteration 4647\n",
            "loss_train 0.1088988417223038\n",
            "iteration 4648\n",
            "loss_train 0.1088917379071234\n",
            "iteration 4649\n",
            "loss_train 0.10888463628524869\n",
            "iteration 4650\n",
            "loss_train 0.1088775368556057\n",
            "iteration 4651\n",
            "loss_train 0.10887043961712124\n",
            "iteration 4652\n",
            "loss_train 0.10886334456872274\n",
            "iteration 4653\n",
            "loss_train 0.10885625170933835\n",
            "iteration 4654\n",
            "loss_train 0.10884916103789709\n",
            "iteration 4655\n",
            "loss_train 0.10884207255332864\n",
            "iteration 4656\n",
            "loss_train 0.10883498625456335\n",
            "iteration 4657\n",
            "loss_train 0.10882790214053238\n",
            "iteration 4658\n",
            "loss_train 0.10882082021016758\n",
            "iteration 4659\n",
            "loss_train 0.10881374046240151\n",
            "iteration 4660\n",
            "loss_train 0.10880666289616753\n",
            "iteration 4661\n",
            "loss_train 0.10879958751039961\n",
            "iteration 4662\n",
            "loss_train 0.1087925143040326\n",
            "iteration 4663\n",
            "loss_train 0.10878544327600195\n",
            "iteration 4664\n",
            "loss_train 0.10877837442524381\n",
            "iteration 4665\n",
            "loss_train 0.10877130775069518\n",
            "iteration 4666\n",
            "loss_train 0.10876424325129373\n",
            "iteration 4667\n",
            "loss_train 0.10875718092597782\n",
            "iteration 4668\n",
            "loss_train 0.10875012077368654\n",
            "iteration 4669\n",
            "loss_train 0.10874306279335968\n",
            "iteration 4670\n",
            "loss_train 0.10873600698393786\n",
            "iteration 4671\n",
            "loss_train 0.10872895334436228\n",
            "iteration 4672\n",
            "loss_train 0.10872190187357499\n",
            "iteration 4673\n",
            "loss_train 0.1087148525705186\n",
            "iteration 4674\n",
            "loss_train 0.1087078054341366\n",
            "iteration 4675\n",
            "loss_train 0.10870076046337308\n",
            "iteration 4676\n",
            "loss_train 0.10869371765717291\n",
            "iteration 4677\n",
            "loss_train 0.1086866770144817\n",
            "iteration 4678\n",
            "loss_train 0.10867963853424564\n",
            "iteration 4679\n",
            "loss_train 0.1086726022154118\n",
            "iteration 4680\n",
            "loss_train 0.10866556805692788\n",
            "iteration 4681\n",
            "loss_train 0.10865853605774227\n",
            "iteration 4682\n",
            "loss_train 0.10865150621680413\n",
            "iteration 4683\n",
            "loss_train 0.1086444785330633\n",
            "iteration 4684\n",
            "loss_train 0.10863745300547037\n",
            "iteration 4685\n",
            "loss_train 0.1086304296329766\n",
            "iteration 4686\n",
            "loss_train 0.10862340841453394\n",
            "iteration 4687\n",
            "loss_train 0.10861638934909507\n",
            "iteration 4688\n",
            "loss_train 0.1086093724356135\n",
            "iteration 4689\n",
            "loss_train 0.10860235767304326\n",
            "iteration 4690\n",
            "loss_train 0.10859534506033912\n",
            "iteration 4691\n",
            "loss_train 0.10858833459645668\n",
            "iteration 4692\n",
            "loss_train 0.10858132628035214\n",
            "iteration 4693\n",
            "loss_train 0.10857432011098243\n",
            "iteration 4694\n",
            "loss_train 0.10856731608730523\n",
            "iteration 4695\n",
            "loss_train 0.10856031420827884\n",
            "iteration 4696\n",
            "loss_train 0.10855331447286236\n",
            "iteration 4697\n",
            "loss_train 0.10854631688001547\n",
            "iteration 4698\n",
            "loss_train 0.1085393214286987\n",
            "iteration 4699\n",
            "loss_train 0.10853232811787317\n",
            "iteration 4700\n",
            "loss_train 0.10852533694650073\n",
            "iteration 4701\n",
            "loss_train 0.10851834791354399\n",
            "iteration 4702\n",
            "loss_train 0.10851136101796616\n",
            "iteration 4703\n",
            "loss_train 0.10850437625873122\n",
            "iteration 4704\n",
            "loss_train 0.10849739363480387\n",
            "iteration 4705\n",
            "loss_train 0.1084904131451494\n",
            "iteration 4706\n",
            "loss_train 0.10848343478873389\n",
            "iteration 4707\n",
            "loss_train 0.10847645856452413\n",
            "iteration 4708\n",
            "loss_train 0.10846948447148753\n",
            "iteration 4709\n",
            "loss_train 0.10846251250859226\n",
            "iteration 4710\n",
            "loss_train 0.10845554267480714\n",
            "iteration 4711\n",
            "loss_train 0.10844857496910174\n",
            "iteration 4712\n",
            "loss_train 0.10844160939044628\n",
            "iteration 4713\n",
            "loss_train 0.10843464593781166\n",
            "iteration 4714\n",
            "loss_train 0.10842768461016951\n",
            "iteration 4715\n",
            "loss_train 0.10842072540649217\n",
            "iteration 4716\n",
            "loss_train 0.10841376832575263\n",
            "iteration 4717\n",
            "loss_train 0.10840681336692455\n",
            "iteration 4718\n",
            "loss_train 0.10839986052898234\n",
            "iteration 4719\n",
            "loss_train 0.10839290981090108\n",
            "iteration 4720\n",
            "loss_train 0.10838596121165653\n",
            "iteration 4721\n",
            "loss_train 0.10837901473022513\n",
            "iteration 4722\n",
            "loss_train 0.10837207036558404\n",
            "iteration 4723\n",
            "loss_train 0.10836512811671109\n",
            "iteration 4724\n",
            "loss_train 0.1083581879825848\n",
            "iteration 4725\n",
            "loss_train 0.10835124996218436\n",
            "iteration 4726\n",
            "loss_train 0.1083443140544896\n",
            "iteration 4727\n",
            "loss_train 0.10833738025848123\n",
            "iteration 4728\n",
            "loss_train 0.10833044857314039\n",
            "iteration 4729\n",
            "loss_train 0.10832351899744906\n",
            "iteration 4730\n",
            "loss_train 0.10831659153038986\n",
            "iteration 4731\n",
            "loss_train 0.10830966617094613\n",
            "iteration 4732\n",
            "loss_train 0.10830274291810181\n",
            "iteration 4733\n",
            "loss_train 0.10829582177084161\n",
            "iteration 4734\n",
            "loss_train 0.10828890272815088\n",
            "iteration 4735\n",
            "loss_train 0.10828198578901567\n",
            "iteration 4736\n",
            "loss_train 0.10827507095242264\n",
            "iteration 4737\n",
            "loss_train 0.10826815821735922\n",
            "iteration 4738\n",
            "loss_train 0.10826124758281347\n",
            "iteration 4739\n",
            "loss_train 0.10825433904777418\n",
            "iteration 4740\n",
            "loss_train 0.10824743261123072\n",
            "iteration 4741\n",
            "loss_train 0.10824052827217322\n",
            "iteration 4742\n",
            "loss_train 0.10823362602959244\n",
            "iteration 4743\n",
            "loss_train 0.10822672588247988\n",
            "iteration 4744\n",
            "loss_train 0.10821982782982761\n",
            "iteration 4745\n",
            "loss_train 0.10821293187062851\n",
            "iteration 4746\n",
            "loss_train 0.10820603800387607\n",
            "iteration 4747\n",
            "loss_train 0.10819914622856434\n",
            "iteration 4748\n",
            "loss_train 0.10819225654368828\n",
            "iteration 4749\n",
            "loss_train 0.10818536894824325\n",
            "iteration 4750\n",
            "loss_train 0.10817848344122548\n",
            "iteration 4751\n",
            "loss_train 0.10817160002163186\n",
            "iteration 4752\n",
            "loss_train 0.10816471868845987\n",
            "iteration 4753\n",
            "loss_train 0.10815783944070766\n",
            "iteration 4754\n",
            "loss_train 0.10815096227737413\n",
            "iteration 4755\n",
            "loss_train 0.10814408719745877\n",
            "iteration 4756\n",
            "loss_train 0.10813721419996178\n",
            "iteration 4757\n",
            "loss_train 0.10813034328388406\n",
            "iteration 4758\n",
            "loss_train 0.10812347444822705\n",
            "iteration 4759\n",
            "loss_train 0.10811660769199298\n",
            "iteration 4760\n",
            "loss_train 0.10810974301418469\n",
            "iteration 4761\n",
            "loss_train 0.10810288041380575\n",
            "iteration 4762\n",
            "loss_train 0.10809601988986033\n",
            "iteration 4763\n",
            "loss_train 0.10808916144135328\n",
            "iteration 4764\n",
            "loss_train 0.10808230506729007\n",
            "iteration 4765\n",
            "loss_train 0.10807545076667689\n",
            "iteration 4766\n",
            "loss_train 0.10806859853852063\n",
            "iteration 4767\n",
            "loss_train 0.10806174838182878\n",
            "iteration 4768\n",
            "loss_train 0.10805490029560945\n",
            "iteration 4769\n",
            "loss_train 0.10804805427887151\n",
            "iteration 4770\n",
            "loss_train 0.10804121033062444\n",
            "iteration 4771\n",
            "loss_train 0.10803436844987838\n",
            "iteration 4772\n",
            "loss_train 0.10802752863564412\n",
            "iteration 4773\n",
            "loss_train 0.10802069088693314\n",
            "iteration 4774\n",
            "loss_train 0.10801385520275754\n",
            "iteration 4775\n",
            "loss_train 0.10800702158213013\n",
            "iteration 4776\n",
            "loss_train 0.10800019002406432\n",
            "iteration 4777\n",
            "loss_train 0.10799336052757415\n",
            "iteration 4778\n",
            "loss_train 0.10798653309167448\n",
            "iteration 4779\n",
            "loss_train 0.10797970771538065\n",
            "iteration 4780\n",
            "loss_train 0.10797288439770865\n",
            "iteration 4781\n",
            "loss_train 0.10796606313767529\n",
            "iteration 4782\n",
            "loss_train 0.10795924393429791\n",
            "iteration 4783\n",
            "loss_train 0.10795242678659447\n",
            "iteration 4784\n",
            "loss_train 0.1079456116935837\n",
            "iteration 4785\n",
            "loss_train 0.10793879865428488\n",
            "iteration 4786\n",
            "loss_train 0.10793198766771804\n",
            "iteration 4787\n",
            "loss_train 0.10792517873290378\n",
            "iteration 4788\n",
            "loss_train 0.10791837184886326\n",
            "iteration 4789\n",
            "loss_train 0.10791156701461854\n",
            "iteration 4790\n",
            "loss_train 0.10790476422919215\n",
            "iteration 4791\n",
            "loss_train 0.10789796349160727\n",
            "iteration 4792\n",
            "loss_train 0.1078911648008878\n",
            "iteration 4793\n",
            "loss_train 0.10788436815605827\n",
            "iteration 4794\n",
            "loss_train 0.10787757355614379\n",
            "iteration 4795\n",
            "loss_train 0.10787078100017024\n",
            "iteration 4796\n",
            "loss_train 0.10786399048716397\n",
            "iteration 4797\n",
            "loss_train 0.10785720201615215\n",
            "iteration 4798\n",
            "loss_train 0.1078504155861625\n",
            "iteration 4799\n",
            "loss_train 0.10784363119622344\n",
            "iteration 4800\n",
            "loss_train 0.1078368488453639\n",
            "iteration 4801\n",
            "loss_train 0.10783006853261368\n",
            "iteration 4802\n",
            "loss_train 0.10782329025700298\n",
            "iteration 4803\n",
            "loss_train 0.10781651401756286\n",
            "iteration 4804\n",
            "loss_train 0.10780973981332483\n",
            "iteration 4805\n",
            "loss_train 0.10780296764332119\n",
            "iteration 4806\n",
            "loss_train 0.10779619750658474\n",
            "iteration 4807\n",
            "loss_train 0.1077894294021491\n",
            "iteration 4808\n",
            "loss_train 0.1077826633290483\n",
            "iteration 4809\n",
            "loss_train 0.10777589928631727\n",
            "iteration 4810\n",
            "loss_train 0.10776913727299134\n",
            "iteration 4811\n",
            "loss_train 0.10776237728810666\n",
            "iteration 4812\n",
            "loss_train 0.10775561933069985\n",
            "iteration 4813\n",
            "loss_train 0.10774886339980833\n",
            "iteration 4814\n",
            "loss_train 0.10774210949447002\n",
            "iteration 4815\n",
            "loss_train 0.10773535761372358\n",
            "iteration 4816\n",
            "loss_train 0.1077286077566082\n",
            "iteration 4817\n",
            "loss_train 0.10772185992216385\n",
            "iteration 4818\n",
            "loss_train 0.10771511410943092\n",
            "iteration 4819\n",
            "loss_train 0.1077083703174507\n",
            "iteration 4820\n",
            "loss_train 0.1077016285452649\n",
            "iteration 4821\n",
            "loss_train 0.1076948887919159\n",
            "iteration 4822\n",
            "loss_train 0.10768815105644683\n",
            "iteration 4823\n",
            "loss_train 0.10768141533790129\n",
            "iteration 4824\n",
            "loss_train 0.10767468163532364\n",
            "iteration 4825\n",
            "loss_train 0.10766794994775877\n",
            "iteration 4826\n",
            "loss_train 0.10766122027425232\n",
            "iteration 4827\n",
            "loss_train 0.10765449261385036\n",
            "iteration 4828\n",
            "loss_train 0.1076477669655998\n",
            "iteration 4829\n",
            "loss_train 0.1076410433285481\n",
            "iteration 4830\n",
            "loss_train 0.10763432170174324\n",
            "iteration 4831\n",
            "loss_train 0.10762760208423404\n",
            "iteration 4832\n",
            "loss_train 0.1076208844750698\n",
            "iteration 4833\n",
            "loss_train 0.10761416887330043\n",
            "iteration 4834\n",
            "loss_train 0.10760745527797652\n",
            "iteration 4835\n",
            "loss_train 0.10760074368814934\n",
            "iteration 4836\n",
            "loss_train 0.10759403410287066\n",
            "iteration 4837\n",
            "loss_train 0.10758732652119289\n",
            "iteration 4838\n",
            "loss_train 0.10758062094216922\n",
            "iteration 4839\n",
            "loss_train 0.10757391736485325\n",
            "iteration 4840\n",
            "loss_train 0.1075672157882993\n",
            "iteration 4841\n",
            "loss_train 0.1075605162115624\n",
            "iteration 4842\n",
            "loss_train 0.107553818633698\n",
            "iteration 4843\n",
            "loss_train 0.10754712305376234\n",
            "iteration 4844\n",
            "loss_train 0.10754042947081224\n",
            "iteration 4845\n",
            "loss_train 0.10753373788390506\n",
            "iteration 4846\n",
            "loss_train 0.10752704829209889\n",
            "iteration 4847\n",
            "loss_train 0.10752036069445235\n",
            "iteration 4848\n",
            "loss_train 0.10751367509002473\n",
            "iteration 4849\n",
            "loss_train 0.10750699147787593\n",
            "iteration 4850\n",
            "loss_train 0.10750030985706643\n",
            "iteration 4851\n",
            "loss_train 0.1074936302266574\n",
            "iteration 4852\n",
            "loss_train 0.10748695258571053\n",
            "iteration 4853\n",
            "loss_train 0.10748027693328824\n",
            "iteration 4854\n",
            "loss_train 0.10747360326845341\n",
            "iteration 4855\n",
            "loss_train 0.10746693159026967\n",
            "iteration 4856\n",
            "loss_train 0.1074602618978012\n",
            "iteration 4857\n",
            "loss_train 0.10745359419011286\n",
            "iteration 4858\n",
            "loss_train 0.10744692846627\n",
            "iteration 4859\n",
            "loss_train 0.1074402647253387\n",
            "iteration 4860\n",
            "loss_train 0.10743360296638559\n",
            "iteration 4861\n",
            "loss_train 0.10742694318847792\n",
            "iteration 4862\n",
            "loss_train 0.10742028539068356\n",
            "iteration 4863\n",
            "loss_train 0.10741362957207097\n",
            "iteration 4864\n",
            "loss_train 0.10740697573170928\n",
            "iteration 4865\n",
            "loss_train 0.1074003238686681\n",
            "iteration 4866\n",
            "loss_train 0.10739367398201784\n",
            "iteration 4867\n",
            "loss_train 0.10738702607082933\n",
            "iteration 4868\n",
            "loss_train 0.1073803801341741\n",
            "iteration 4869\n",
            "loss_train 0.10737373617112429\n",
            "iteration 4870\n",
            "loss_train 0.1073670941807526\n",
            "iteration 4871\n",
            "loss_train 0.10736045416213241\n",
            "iteration 4872\n",
            "loss_train 0.10735381611433767\n",
            "iteration 4873\n",
            "loss_train 0.10734718003644286\n",
            "iteration 4874\n",
            "loss_train 0.10734054592752314\n",
            "iteration 4875\n",
            "loss_train 0.1073339137866543\n",
            "iteration 4876\n",
            "loss_train 0.10732728361291266\n",
            "iteration 4877\n",
            "loss_train 0.10732065540537522\n",
            "iteration 4878\n",
            "loss_train 0.10731402916311951\n",
            "iteration 4879\n",
            "loss_train 0.10730740488522371\n",
            "iteration 4880\n",
            "loss_train 0.10730078257076653\n",
            "iteration 4881\n",
            "loss_train 0.10729416221882741\n",
            "iteration 4882\n",
            "loss_train 0.10728754382848629\n",
            "iteration 4883\n",
            "loss_train 0.1072809273988237\n",
            "iteration 4884\n",
            "loss_train 0.10727431292892084\n",
            "iteration 4885\n",
            "loss_train 0.10726770041785944\n",
            "iteration 4886\n",
            "loss_train 0.10726108986472188\n",
            "iteration 4887\n",
            "loss_train 0.1072544812685911\n",
            "iteration 4888\n",
            "loss_train 0.10724787462855066\n",
            "iteration 4889\n",
            "loss_train 0.10724126994368476\n",
            "iteration 4890\n",
            "loss_train 0.10723466721307807\n",
            "iteration 4891\n",
            "loss_train 0.10722806643581596\n",
            "iteration 4892\n",
            "loss_train 0.10722146761098438\n",
            "iteration 4893\n",
            "loss_train 0.10721487073766982\n",
            "iteration 4894\n",
            "loss_train 0.10720827581495948\n",
            "iteration 4895\n",
            "loss_train 0.10720168284194095\n",
            "iteration 4896\n",
            "loss_train 0.10719509181770272\n",
            "iteration 4897\n",
            "loss_train 0.10718850274133354\n",
            "iteration 4898\n",
            "loss_train 0.10718191561192299\n",
            "iteration 4899\n",
            "loss_train 0.10717533042856117\n",
            "iteration 4900\n",
            "loss_train 0.1071687471903387\n",
            "iteration 4901\n",
            "loss_train 0.10716216589634682\n",
            "iteration 4902\n",
            "loss_train 0.10715558654567753\n",
            "iteration 4903\n",
            "loss_train 0.10714900913742313\n",
            "iteration 4904\n",
            "loss_train 0.10714243367067675\n",
            "iteration 4905\n",
            "loss_train 0.10713586014453196\n",
            "iteration 4906\n",
            "loss_train 0.107129288558083\n",
            "iteration 4907\n",
            "loss_train 0.1071227189104247\n",
            "iteration 4908\n",
            "loss_train 0.10711615120065242\n",
            "iteration 4909\n",
            "loss_train 0.10710958542786214\n",
            "iteration 4910\n",
            "loss_train 0.10710302159115039\n",
            "iteration 4911\n",
            "loss_train 0.10709645968961438\n",
            "iteration 4912\n",
            "loss_train 0.1070898997223518\n",
            "iteration 4913\n",
            "loss_train 0.107083341688461\n",
            "iteration 4914\n",
            "loss_train 0.10707678558704087\n",
            "iteration 4915\n",
            "loss_train 0.10707023141719087\n",
            "iteration 4916\n",
            "loss_train 0.10706367917801109\n",
            "iteration 4917\n",
            "loss_train 0.10705712886860215\n",
            "iteration 4918\n",
            "loss_train 0.10705058048806533\n",
            "iteration 4919\n",
            "loss_train 0.10704403403550247\n",
            "iteration 4920\n",
            "loss_train 0.10703748951001586\n",
            "iteration 4921\n",
            "loss_train 0.10703094691070862\n",
            "iteration 4922\n",
            "loss_train 0.10702440623668415\n",
            "iteration 4923\n",
            "loss_train 0.10701786748704674\n",
            "iteration 4924\n",
            "loss_train 0.10701133066090097\n",
            "iteration 4925\n",
            "loss_train 0.1070047957573522\n",
            "iteration 4926\n",
            "loss_train 0.10699826277550635\n",
            "iteration 4927\n",
            "loss_train 0.10699173171446977\n",
            "iteration 4928\n",
            "loss_train 0.10698520257334955\n",
            "iteration 4929\n",
            "loss_train 0.10697867535125331\n",
            "iteration 4930\n",
            "loss_train 0.10697215004728919\n",
            "iteration 4931\n",
            "loss_train 0.106965626660566\n",
            "iteration 4932\n",
            "loss_train 0.106959105190193\n",
            "iteration 4933\n",
            "loss_train 0.10695258563528016\n",
            "iteration 4934\n",
            "loss_train 0.10694606799493794\n",
            "iteration 4935\n",
            "loss_train 0.1069395522682774\n",
            "iteration 4936\n",
            "loss_train 0.10693303845441017\n",
            "iteration 4937\n",
            "loss_train 0.10692652655244843\n",
            "iteration 4938\n",
            "loss_train 0.10692001656150502\n",
            "iteration 4939\n",
            "loss_train 0.10691350848069324\n",
            "iteration 4940\n",
            "loss_train 0.10690700230912697\n",
            "iteration 4941\n",
            "loss_train 0.10690049804592076\n",
            "iteration 4942\n",
            "loss_train 0.10689399569018969\n",
            "iteration 4943\n",
            "loss_train 0.10688749524104936\n",
            "iteration 4944\n",
            "loss_train 0.10688099669761593\n",
            "iteration 4945\n",
            "loss_train 0.10687450005900624\n",
            "iteration 4946\n",
            "loss_train 0.1068680053243376\n",
            "iteration 4947\n",
            "loss_train 0.10686151249272788\n",
            "iteration 4948\n",
            "loss_train 0.10685502156329564\n",
            "iteration 4949\n",
            "loss_train 0.10684853253515983\n",
            "iteration 4950\n",
            "loss_train 0.10684204540744013\n",
            "iteration 4951\n",
            "loss_train 0.10683556017925668\n",
            "iteration 4952\n",
            "loss_train 0.10682907684973024\n",
            "iteration 4953\n",
            "loss_train 0.10682259541798209\n",
            "iteration 4954\n",
            "loss_train 0.10681611588313417\n",
            "iteration 4955\n",
            "loss_train 0.10680963824430881\n",
            "iteration 4956\n",
            "loss_train 0.10680316250062907\n",
            "iteration 4957\n",
            "loss_train 0.10679668865121852\n",
            "iteration 4958\n",
            "loss_train 0.10679021669520132\n",
            "iteration 4959\n",
            "loss_train 0.10678374663170206\n",
            "iteration 4960\n",
            "loss_train 0.10677727845984611\n",
            "iteration 4961\n",
            "loss_train 0.10677081217875917\n",
            "iteration 4962\n",
            "loss_train 0.10676434778756769\n",
            "iteration 4963\n",
            "loss_train 0.10675788528539859\n",
            "iteration 4964\n",
            "loss_train 0.10675142467137935\n",
            "iteration 4965\n",
            "loss_train 0.10674496594463805\n",
            "iteration 4966\n",
            "loss_train 0.1067385091043033\n",
            "iteration 4967\n",
            "loss_train 0.10673205414950422\n",
            "iteration 4968\n",
            "loss_train 0.10672560107937062\n",
            "iteration 4969\n",
            "loss_train 0.10671914989303274\n",
            "iteration 4970\n",
            "loss_train 0.10671270058962147\n",
            "iteration 4971\n",
            "loss_train 0.10670625316826816\n",
            "iteration 4972\n",
            "loss_train 0.10669980762810481\n",
            "iteration 4973\n",
            "loss_train 0.10669336396826397\n",
            "iteration 4974\n",
            "loss_train 0.10668692218787863\n",
            "iteration 4975\n",
            "loss_train 0.10668048228608253\n",
            "iteration 4976\n",
            "loss_train 0.10667404426200974\n",
            "iteration 4977\n",
            "loss_train 0.10666760811479503\n",
            "iteration 4978\n",
            "loss_train 0.10666117384357374\n",
            "iteration 4979\n",
            "loss_train 0.10665474144748165\n",
            "iteration 4980\n",
            "loss_train 0.1066483109256552\n",
            "iteration 4981\n",
            "loss_train 0.10664188227723134\n",
            "iteration 4982\n",
            "loss_train 0.10663545550134758\n",
            "iteration 4983\n",
            "loss_train 0.10662903059714192\n",
            "iteration 4984\n",
            "loss_train 0.10662260756375301\n",
            "iteration 4985\n",
            "loss_train 0.10661618640031995\n",
            "iteration 4986\n",
            "loss_train 0.1066097671059826\n",
            "iteration 4987\n",
            "loss_train 0.10660334967988104\n",
            "iteration 4988\n",
            "loss_train 0.10659693412115614\n",
            "iteration 4989\n",
            "loss_train 0.10659052042894926\n",
            "iteration 4990\n",
            "loss_train 0.10658410860240229\n",
            "iteration 4991\n",
            "loss_train 0.10657769864065772\n",
            "iteration 4992\n",
            "loss_train 0.1065712905428585\n",
            "iteration 4993\n",
            "loss_train 0.10656488430814819\n",
            "iteration 4994\n",
            "loss_train 0.10655847993567087\n",
            "iteration 4995\n",
            "loss_train 0.1065520774245712\n",
            "iteration 4996\n",
            "loss_train 0.10654567677399436\n",
            "iteration 4997\n",
            "loss_train 0.10653927798308604\n",
            "iteration 4998\n",
            "loss_train 0.10653288105099254\n",
            "iteration 4999\n",
            "loss_train 0.10652648597686074\n",
            "iteration 5000\n",
            "loss_train 0.10652009275983784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how our model did while learning!"
      ],
      "metadata": {
        "id": "e7WHcpPiEcIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Produce the Loss Function Visualization Graphs\n",
        "fig, ax = plt.subplots(figsize=(18,8))\n",
        "ax.plot(losses[0], color='blue', label='Training', linewidth=3);\n",
        "ax.plot(losses[1], color='orange', label='Validation', linewidth=3);\n",
        "ax.legend();\n",
        "ax.set_ylabel('Log Loss')\n",
        "ax.set_xlabel('Iterations')\n",
        "ax.set_title('Loss Function Graph')\n",
        "ax.autoscale(axis='x', tight=True)\n",
        "fig.tight_layout();\n",
        "\n"
      ],
      "metadata": {
        "id": "4wXFzZPjFjOn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "218c5165-7bad-4312-d931-a4ffca99586a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABQgAAAI4CAYAAAAmvQRNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd7ReVZ0//vdOgUQSQIqiQaQXEUkgFKlhLKAIjIglMCpioaigY/uJIIoyMypfRUdBRdQBFaw4KCAKSkdpEjDSMWhU+hASYiBl//54briXeBNOkluSc1+vtZ61nrPPPud8nhv+YL3XZ59daq0BAAAAAIamYYNdAAAAAAAweASEAAAAADCECQgBAAAAYAgTEAIAAADAECYgBAAAAIAhTEAIAAAAAEOYgBAAgH5XSplVStl4sOvoC6WUT5RSvjPYdQAA9BUBIQDAACmlTCulvHwQnvvtUsqTXSHdws8b+/F5l5ZS3tFzrNY6ptZ6Tz89702llN+VUh4vpTzQ9f2oUkrpj+cBALSNgBAAYGj4bFdIt/Dz/cEuqC+UUj6Q5ItJPpdkvSTPTXJEkl2TrLKYa4YPWIEAACsBASEAwCArpaxaSjmllPK3rs8ppZRVu86tU0r5eSnl0VLKI6WUK0opw7rOfaSU8tdSysxSyu2llJct5XO/XUr5dI/jSaWU6T2Op5VSPlhKubmUMqOU8v1Syqge5w8opdxUSnmslHJ3KWWfUspJSXZP8uWuTsUvd82tpZRNu76vUUo5s5TyYCnl3lLKcT1+06GllCtLKSeXUv6vlPKnUsqrFlP/GklOTHJUrfVHtdaZteP3tdZDaq1P9Pidp5VSLiilPJ5kr1LKvqWU33fV/pdSyid63HfDrnrf1fXv8fdSygcXefwqXb9hZillaill4tL87QEAViQCQgCAwfexJDsnGZ9k2yQ7Jjmu69wHkkxPsm463XHHJqmllC2SvCfJDrXWsUn2TjKtH2p7Q5J9kmyU5CVJDk2SUsqOSc5M8qEkaybZI8m0WuvHklyR5D1dnYrv6eWe/51kjSQbJ9kzyVuSvK3H+Z2S3J5knSSfTXLGYpYLvzTJqkn+t8HvODjJSUnGJrkyyeNdz10zyb5Jjiyl/Osi1+yVZLMkr0zykUWWh++f5Jyu689L8uUGNQAArJAEhAAAg++QJCfWWh+otT6Y5JNJ3tx1bm6S5yV5Ya11bq31ilprTTI/nXDsRaWUkbXWabXWu5fwjA92dSE+Wkp5aClq+1Kt9W+11keS/CydEDNJ3p7km7XWX9VaF9Ra/1prve2Zbta1vPdNST7a1fE3Lcn/6/F7k+TeWuvptdb5Sf6n6/c/t5fbrZPkoVrrvB73v7rrN/6jlLJHj7n/W2u9qqvWObXWS2utt3Qd35zk7HTCyp4+WWt9vNZ6S5JvJZnc49yVtdYLumo8K51gFwBgpSQgBAAYfM9Pcm+P43u7xpLOu/XuSvLLUso9pZT/L0lqrXcleV+STyR5oJRyTinl+Vm8k2uta3Z91lmK2u7r8X12kjFd31+QZEmB5OKsk2Rk/vn3juvtmbXW2V1fx+SfPZxknVLKiB7zd6m1rtl1ruf/6/6l54WllJ1KKb/pWuY8I533Fi76d+l5Tc9/k6fVmM7fZVTPOgAAViYCQgCAwfe3JC/scbxB11i6uuw+UGvdOJ1lrf++8F2Dtdbv1Vp367q2JvnMUj738STP6nG83lJc+5ckmyzmXF3CdQ+l0xW56O/961I8e6FrkjyR5IAGcxet6XvpLA1+Qa11jSRfTbLoMuYXLFLj35ahRgCAFZ6AEABgYI0spYzq8RmRzvLW40op65ZS1kny8STfSZJSymtKKZt2vYNvRjpLixeUUrYopfxL12Ymc5L8I8mCpazlpiSvLqWsVUpZL52OxKbOSPK2UsrLSinDSinjSilbdp27P533C/6TriW5P0hyUillbCnlhUn+feHvXRq11kfTWY59ainloK77DSuljE+y2jNcPjbJI7XWOV3vUzy4lznHl1KeVUrZOp13JLZi52cAgEUJCAEABtYF6YR5Cz+fSPLpJNcnuTnJLUlu7BpLOptkXJxkVjodc6fWWn+TzvsH/yudjrz7kjwnyUeXspazkkxJZ3OTX2YpArBa67XphGZfSCe4vCzdXYFfTHJQ1y7EX+rl8vem0714TzobhnwvyTeXsvaFdXw2nYDxw+kEk/cn+VqSjyS5egmXHpXkxFLKzHQC2R/0MueydJZ3X5LOEu1fLkuNAAArutJ5xzUAAJAkpZQNk/wpycieG6AAALSVDkIAAAAAGMIEhAAAAAAwhFliDAAAAABDmA5CAAAAABjCRgx2AX1pnXXWqRtuuOFglwEAAAAAK5wbbrjhoVrruouOtyog3HDDDXP99dcPdhkAAAAAsMIppdzb27glxgAAAAAwhAkIAQAAAGAIExACAAAAwBDWqncQAgAAALBymTt3bqZPn545c+YMdimtMWrUqKy//voZOXJko/kCQgAAAAAGzfTp0zN27NhsuOGGKaUMdjkrvVprHn744UyfPj0bbbRRo2ssMQYAAABg0MyZMydrr722cLCPlFKy9tprL1VH5oAGhKWUtUop55ZSHi+l3FtKOXgJc7crpVxeSplVSrm/lHLMQNYKAAAAwMAQDvatpf17DvQS468keTLJc5OMT3J+KWVKrXVqz0mllHWS/CLJ+5P8KMkqSdYf4FoBAAAAoPUGrIOwlLJaktclOb7WOqvWemWS85K8uZfp/57kolrrd2utT9RaZ9Zabx2oWgEAAAAYGh5++OGMHz8+48ePz3rrrZdx48Y9dfzkk08u8drrr78+Rx999DM+Y5dddumrcvvFQHYQbp5kXq31jh5jU5Ls2cvcnZPcUkq5OsmmSX6X5N211j8vOrGU8q4k70qSDTbYoM+LBgAAAKC91l577dx0001Jkk984hMZM2ZMPvjBDz51ft68eRkxovcIbeLEiZk4ceIzPuPqq6/um2L7yUC+g3BMkscWGZuRZGwvc9dP8tYkxyTZIMmfkpzd201rrV+vtU6stU5cd911+7BcAAAAAIaiQw89NEcccUR22mmnfPjDH861116bl770pZkwYUJ22WWX3H777UmSSy+9NK95zWuSdMLFww47LJMmTcrGG2+cL33pS0/db8yYMU/NnzRpUg466KBsueWWOeSQQ1JrTZJccMEF2XLLLbP99tvn6KOPfuq+A2EgOwhnJVl9kbHVk8zsZe4/kpxba70uSUopn0zyUClljVrrjP4tEwAAAIDB0J97lXTlcI1Nnz49V199dYYPH57HHnssV1xxRUaMGJGLL744xx57bH784x//0zW33XZbfvOb32TmzJnZYostcuSRR2bkyJFPm/P73/8+U6dOzfOf//zsuuuuueqqqzJx4sQcfvjhufzyy7PRRhtl8uTJy/NTl9pABoR3JBlRStms1npn19i2Sab2MvfmJD3/2ZbynxAAAAAAlt3rX//6DB8+PEkyY8aMvPWtb82dd96ZUkrmzp3b6zX77rtvVl111ay66qp5znOek/vvvz/rr//0fXd33HHHp8bGjx+fadOmZcyYMdl4442z0UYbJUkmT56cr3/96/34655uwJYY11ofT/KTJCeWUlYrpeya5IAkZ/Uy/VtJXltKGV9KGZnk+CRX6h4EAAAAYCCsttpqT30//vjjs9dee+UPf/hDfvazn2XOnDm9XrPqqqs+9X348OGZN2/eMs0ZaAP5DsIkOSrJ6CQPpPNOwSNrrVNLKbuXUmYtnFRr/XWSY5Oc3zV30yQHD3CtAAAAAAygWvvvszxmzJiRcePGJUm+/e1vL/8PXcQWW2yRe+65J9OmTUuSfP/73+/zZyzJgAaEtdZHaq3/Wmtdrda6Qa31e13jV9Raxywy97Ra67ha67NrrfvVWv8ykLUCAAAAQJJ8+MMfzkc/+tFMmDChXzr+Ro8enVNPPTX77LNPtt9++4wdOzZrrLFGnz9ncUpd3gh1BTJx4sR6/fXXD3YZAAAAADR06623ZqutthrsMgbdrFmzMmbMmNRa8+53vzubbbZZ3v/+9y/z/Xr7u5ZSbqi1Tlx07kAvMQYAAAAAFnH66adn/Pjx2XrrrTNjxowcfvjhA/bsgdzFGAAAAADoxfvf//7l6hhcHq3qIHzkkeS66wa7CgAAAABYebQqIPzTn5IzzxzsKgAAAABg5dGqgBAAAAAAWDqtCwhbtCkzAAAAAPS71gWEAAAAANDUXnvtlYsuuuhpY6ecckqOPPLIXudPmjQp119/fZLk1a9+dR599NF/mvOJT3wiJ5988hKf+9Of/jR//OMfnzr++Mc/nosvvnhpy+8TrQsIdRACAAAA0NTkyZNzzjnnPG3snHPOyeTJk5/x2gsuuCBrrrnmMj130YDwxBNPzMtf/vJlutfyal1ACAAAAABNHXTQQTn//PPz5JNPJkmmTZuWv/3tbzn77LMzceLEbL311jnhhBN6vXbDDTfMQw89lCQ56aSTsvnmm2e33XbL7bff/tSc008/PTvssEO23XbbvO51r8vs2bNz9dVX57zzzsuHPvShjB8/PnfffXcOPfTQ/OhHP0qSXHLJJZkwYUK22WabHHbYYXniiSeeet4JJ5yQ7bbbLttss01uu+22PvkbjOiTuwAAAADA8vpe6b97H9z7stO11lorO+64Yy688MIccMABOeecc/KGN7whxx57bNZaa63Mnz8/L3vZy3LzzTfnJS95Sa/3uOGGG3LOOefkpptuyrx587Lddttl++23T5IceOCBeec735kkOe6443LGGWfkve99b/bff/+85jWvyUEHHfS0e82ZMyeHHnpoLrnkkmy++eZ5y1vektNOOy3ve9/7kiTrrLNObrzxxpx66qk5+eST841vfGO5/zSt6yC0xBgAAACApdFzmfHC5cU/+MEPst1222XChAmZOnXq05YDL+qKK67Ia1/72jzrWc/K6quvnv333/+pc3/4wx+y++67Z5tttsl3v/vdTJ06dYm13H777dloo42y+eabJ0ne+ta35vLLL3/q/IEHHpgk2X777TNt2rRl/clP07qAEAAAAACWxgEHHJBLLrkkN954Y2bPnp211lorJ598ci655JLcfPPN2XfffTNnzpxluvehhx6aL3/5y7nllltywgknLPN9Flp11VWTJMOHD8+8efOW614LtW6JsQ5CAAAAgJXUYpYB97cxY8Zkr732ymGHHZbJkyfnsccey2qrrZY11lgj999/fy688MJMmjRpsdfvscceOfTQQ/PRj3408+bNy89+9rMcfvjhSZKZM2fmec97XubOnZvvfve7GTduXJJk7NixmTlz5j/da4sttsi0adNy1113ZdNNN81ZZ52VPffcs19+90I6CAEAAAAY8iZPnpwpU6Zk8uTJ2XbbbTNhwoRsueWWOfjgg7Prrrsu8drtttsub3zjG7PtttvmVa96VXbYYYenzn3qU5/KTjvtlF133TVbbrnlU+NvetOb8rnPfS4TJkzI3Xff/dT4qFGj8q1vfSuvf/3rs80222TYsGE54ogj+v4H91Bqi1ruSplYDz/8+nz1q4NdCQAAAABN3Hrrrdlqq60Gu4zW6e3vWkq5odY6cdG5OggBAAAAYAgTEAIAAADAENa6gLBFK6YBAAAAhoQ2vQJvRbC0f8/WBYQAAAAArDxGjRqVhx9+WEjYR2qtefjhhzNq1KjG14zox3oGhf+WAAAAAFYe66+/fqZPn54HH3xwsEtpjVGjRmX99ddvPL91ASEAAAAAK4+RI0dmo402GuwyhjRLjAEAAABgCGtdQGiJMQAAAAA017qAEAAAAABornUBoQ5CAAAAAGiudQEhAAAAANCcgBAAAAAAhrDWBYSWGAMAAABAc60LCAEAAACA5loXEOogBAAAAIDmWhcQAgAAAADNCQgBAAAAYAhrXUBoiTEAAAAANNe6gBAAAAAAaK51AaEOQgAAAABornUBIQAAAADQnIAQAAAAAIaw1gWElhgDAAAAQHOtCwgBAAAAgOZaFxDqIAQAAACA5loXEAIAAAAAzbUuINRBCAAAAADNtS4gBAAAAACaExACAAAAwBDWuoDQEmMAAAAAaK51ASEAAAAA0FzrAkIdhAAAAADQXOsCQgAAAACgOQEhAAAAAAxhrQsILTEGAAAAgOZaFxACAAAAAM21LiDUQQgAAAAAzbUuIAQAAAAAmhMQAgAAAMAQ1rqA0BJjAAAAAGiudQEhAAAAANBc6wJCHYQAAAAA0FzrAkIAAAAAoDkBIQAAAAAMYa0LCC0xBgAAAIDmWhcQAgAAAADNtS4g1EEIAAAAAM21LiAEAAAAAJoTEAIAAADAENa6gNASYwAAAABornUBIQAAAADQXOsCQh2EAAAAANBc6wJCAAAAAKC51gWEOggBAAAAoLnWBYQAAAAAQHMCQgAAAAAYwloXEFpiDAAAAADNtS4gBAAAAACaa11AqIMQAAAAAJprXUAIAAAAADQnIAQAAACAIax1AaElxgAAAADQXOsCQgAAAACguQENCEspa5VSzi2lPF5KubeUcvBi5n2ilDK3lDKrx2fjZ7r/i8ZNzeu2/M++LxwAAAAAWmrEAD/vK0meTPLcJOOTnF9KmVJrndrL3O/XWv9taW4+epU5efbov/dBmQAAAAAwNAxYB2EpZbUkr0tyfK11Vq31yiTnJXlznz4nXkIIAAAAAE0N5BLjzZPMq7Xe0WNsSpKtFzN/v1LKI6WUqaWUIxd301LKu0op15dSrk8iHgQAAACApTCQAeGYJI8tMjYjydhe5v4gyVZJ1k3yziQfL6VM7u2mtdav11on1lonJjoIAQAAAGBpDGRAOCvJ6ouMrZ5k5qITa61/rLX+rdY6v9Z6dZIvJjloAGoEAAAAgCFlIAPCO5KMKKVs1mNs2yS9bVCyqJqkNHuMDkIAAAAAaGrAAsJa6+NJfpLkxFLKaqWUXZMckOSsReeWUg4opTy7dOyY5Ogk/ztQtQIAAADAUDGQHYRJclSS0UkeSHJ2kiNrrVNLKbuXUmb1mPemJHels/z4zCSfqbX+T5MHeAchAAAAADQ3YiAfVmt9JMm/9jJ+RTqbmCw87nVDkoZPWfZLAQAAAGCIGegOQgAAAABgBdK6gNASYwAAAABornUBIQAAAADQXAsDQh2EAAAAANBUCwNCAAAAAKCpFgaEOggBAAAAoKkWBoQAAAAAQFOtCwjtYgwAAAAAzbUuIAQAAAAAmmthQKiDEAAAAACaamFACAAAAAA01bqA0DsIAQAAAKC51gWE4kEAAAAAaK51AaEOQgAAAABornUBIQAAAADQXAsDQh2EAAAAANBUCwNCAAAAAKCp1gWE3kEIAAAAAM21LiAEAAAAAJprXUBYig5CAAAAAGiqdQFhlQ8CAAAAQGOtCwh1EAIAAABAc60LCAEAAACA5loYEOogBAAAAICmWhgQAgAAAABNtS4gLDoIAQAAAKCx1gWEAAAAAEBzLQwIdRACAAAAQFMtDAgBAAAAgKZaFxB6ByEAAAAANNe6gFA8CAAAAADNtS4g1EEIAAAAAM21LiAEAAAAAJprXUCogxAAAAAAmmtdQAgAAAAANNe+gLDoIAQAAACAptoXEAIAAAAAjbUwINRBCAAAAABNtTAgBAAAAACaal1AaBdjAAAAAGiudQEhAAAAANBc6wJCHYQAAAAA0FzrAkLxIAAAAAA017qAUAchAAAAADTXuoAQAAAAAGiudQGhDkIAAAAAaK51ASEAAAAA0FwLA0IdhAAAAADQVAsDQgAAAACgqdYFhKXoIAQAAACAploXEAIAAAAAzbUwINRBCAAAAABNtS4gLAJCAAAAAGisdQEhAAAAANBcCwNCHYQAAAAA0FQLA0IAAAAAoKnWBYSl6CAEAAAAgKZaFxBaYQwAAAAAzbUuINRBCAAAAADNtS4gFA8CAAAAQHOtCwiLiBAAAAAAGmtdQAgAAAAANNe6gFAHIQAAAAA017qAEAAAAABoroUBoQ5CAAAAAGiqhQEhAAAAANBU6wLCUnQQAgAAAEBTrQsIAQAAAIDmWhcQ2sUYAAAAAJprXUAoHgQAAACA5loXEOogBAAAAIDmWhcQAgAAAADNtTAg1EEIAAAAAE21MCAEAAAAAJpqXUDoHYQAAAAA0FzrAkLxIAAAAAA017qAUAchAAAAADTXuoAQAAAAAGhuQAPCUspapZRzSymPl1LuLaUc/AzzVyml3FpKmd74GToIAQAAAKCxEQP8vK8keTLJc5OMT3J+KWVKrXXqYuZ/KMmDScY2fYB4EAAAAACaG7AOwlLKaklel+T4WuusWuuVSc5L8ubFzN8oyb8l+c+leo6IEAAAAAAaG8glxpsnmVdrvaPH2JQkWy9m/n8nOTbJP/q7MAAAAAAYqgYyIByT5LFFxmakl+XDpZTXJhleaz33mW5aSnlXKeX6Usr1nWMdhAAAAADQ1EAGhLOSrL7I2OpJZvYc6FqK/NkkRze5aa3167XWibXWiZ3jPqgUAAAAAIaIgdyk5I4kI0opm9Va7+wa2zbJohuUbJZkwyRXlFKSZJUka5RS7kuyc6112pIeooMQAAAAAJobsICw1vp4KeUnSU4spbwjnV2MD0iyyyJT/5DkBT2Od0ny5STbpbOj8TM8qE/KBQAAAIAhYSCXGCfJUUlGJ3kgydlJjqy1Ti2l7F5KmZUktdZ5tdb7Fn6SPJJkQdfx/Gd6gA5CAAAAAGhuIJcYp9b6SJJ/7WX8inQ2MentmkuTrN/4GctaHAAAAAAMQQPdQdjviogQAAAAABprXUAIAAAAADTXwoBQByEAAAAANNXCgBAAAAAAaKp1AaF3EAIAAABAc60LCAEAAACA5loYEOogBAAAAICmWhgQAgAAAABNtS4gLEUHIQAAAAA01bqAEAAAAABornUBoV2MAQAAAKC51gWE4kEAAAAAaK51AaEOQgAAAABornUBIQAAAADQXOsCQh2EAAAAANBc+wLCIiAEAAAAgKZaFxCKBwEAAACgudYFhJYYAwAAAEBzrQsIAQAAAIDmWhcQ6iAEAAAAgOZaFxACAAAAAM21MCDUQQgAAAAATbUwIAQAAAAAmmpdQFiKDkIAAAAAaKp1ASEAAAAA0FzrAkK7GAMAAABAc60LCMWDAAAAANBco4CwlDKslDKsx/F6pZR3lFJ27b/Slo13EAIAAABAc007CM9P8t4kKaWMSXJ9ks8lubSU8pZ+qm2ZWGIMAAAAAM01DQgnJvl11/cDkzyW5DlJ3pnkg/1Q1zITEAIAAABAc00DwjFJHu36/sok59Za56YTGm7SH4UtK0uMAQAAAKC5pgHhn5PsWkpZLcneSX7VNb5Wktn9Udiy0kEIAAAAAM2NaDjv80nOSjIryb1JLu8a3yPJLf1Q1zLTQQgAAAAAzTUKCGutXyul3JDkBUl+VWtd0HXq7iTH91dxy0ZACAAAAABNNe0gTK31+nR2L06SlFJG1lrP75eqloMOQgAAAABortE7CEspR5dSXtfj+Iwk/yil3F5K2aLfqlsG3kEIAAAAAM013aTk6CQPJkkpZY8kb0hycJKbkvy//ilt2QwbtuCZJwEAAAAASZovMR6X5E9d3/dL8sNa6w9KKbckuaJfKltGOggBAAAAoLmmHYSPJXlO1/dXJLmk6/vcJKP6uqjl4R2EAAAAANBc0w7CXyY5vZRyY5JNk1zYNb51ujsLVwg6CAEAAACguaYdhO9OclWSdZMcVGt9pGt8uyRn90dhy6qUmiojBAAAAIBGGnUQ1lofS/LeXsZP6POKltPCgLCUwa4EAAAAAFZ8TZcYp5SyapJDkrwoSU0yNcnZtdYn+qm2ZWKJMQAAAAA012iJcSnlRUnuTPL5JDsl2TnJKUnuKKVs1X/lLT1LjAEAAACguabvIPxikt8n2aDWunutdfckGySZkk5QuMIQEAIAAABAc02XGO+aZIeudxEm6byXsJTysSS/7ZfKllGJgBAAAAAAmmraQTgnyZq9jK/RdW6FMawsGOwSAAAAAGCl0TQg/FmS00spu5ZShnd9dkvytSTn9V95S88SYwAAAABormlAeEw6m5RckU7H4JwklyW5I8n7+6e0ZSMgBAAAAIDmGr2DsNb6aJIDSimbJlm4a/GtSe5OMrqfalsm3kEIAAAAAM013aQkSVJrvSvJXQuPSynbJrkxyfA+rmuZ6SAEAAAAgOaaLjFeaZQiHQQAAACAptoXEFpiDAAAAACNtS8gtMQYAAAAABpb4jsISylrPcP1a/ZhLX1CQAgAAAAAzT3TJiUPJVlS3Fae4fyAs8QYAAAAAJp7poBwrwGpog8NG7ZgsEsAAAAAgJXGEgPCWutlA1VIX9FBCAAAAADN2aQEAAAAAIaw9gWEOggBAAAAoLH2BYQ6CAEAAACgsVYGhAvsUwIAAAAAjbQvIEzN/PmDXQUAAAAArByWuIvxQqWUby7mVE0yJ8ldSb5fa/1bXxW2rEoREAIAAABAU40CwiTrJtk9yYIkf+gae3GSkuSGJAcmObGUsnut9aY+r3IpCAgBAAAAoLmmS4yvSnJhkvVrrXvUWvdIsn6SC5L8MskLk5yf5P/1S5VLwRJjAAAAAGiuaUB4TJITa62zFw50fT8pyftrrU8m+UyS8X1f4tIZNmyBgBAAAAAAGmoaEI5J8rxextfrOpckj6X5kuV+o4MQAAAAAJprGhCem+SMUsrrSykbdn1en+SMJD/pmrNjkjv6o8ilMWyYgBAAAAAAmmra8XdEks8n+U6Pa+Yl+WaSD3Yd35rknX1a3TISEAIAAABAM40Cwq73DR5RSvlAkk26hu+utT7eY86g7l7c0/x5NZ0NlgEAAACAJVnadwbOT7IgSe36vkKaP19ACAAAAABNNHoHYSllRCnlc0n+L8mUJLck+b9SymdLKSP7s8Bl0QkIAQAAAIBn0rSD8LNJJqfzLsIru8Z2T/Kf6YSMH1zMdYNCQAgAAAAAzTQNCA9Oclit9YIeY3eXUh5M8o2saAHhPAEhAAAAADTRaIlxkjWS3N3L+N1J1uy7cvqGDkIAAAAAaKZpQDglydG9jB+TZIXZvXihBXq2L90AACAASURBVPMXDHYJAAAAALBSaLrE+MNJLiilvDzJb7vGdk7y/CSv6o/ClocOQgAAAABoplEHYa318iSbJ/lRkjFdnx8m2aLWeuWSrh0MAkIAAAAAaKbpEuPUWv9Wa/1YrfV1XZ/jkowspfyg6T1KKWuVUs4tpTxeSrm3lHLwYua9v5RyTynlsVLK30opXyilNO12zAIBIQAAAAA00jggXIw1k7xuKeZ/JcmTSZ6b5JAkp5VStu5l3nlJtqu1rp7kxUm2Te/vQOyVDkIAAAAAaGZ5A8LGSimrpRMmHl9rndW1NPm8JG9edG6t9e5a66MLL02yIMmmTZ81b66AEAAAAACaGLCAMJ13GM6rtd7RY2xKkt46CFNKObiU8liSh9LpIPzaYua9q5RyfSnl+oVjjzwiIAQAAACAJgYyIByT5LFFxmYkGdvb5Frr97qWGG+e5KtJ7l/MvK/XWifWWicuHLv/fgEhAAAAADSxxI0/SinnPcP1qy/Fs2b1Mn/1JDOXdFGt9c5SytQkpyY5sMmDzv1JzVHHJGN7jR4BAAAAgIWeaWfghxuc/1PDZ92RZEQpZbNa651dY9smmdrg2hFJNmn4nNx157zst19y4YXJ6NFNrwIAAACAoWeJAWGt9W199aBa6+OllJ8kObGU8o4k45MckGSXRed2nT+v1vpAKeVFST6a5KKmz1p15BO57LLkda9LfvrTZJVV+uhHAAAAAEDLDOQ7CJPkqCSjkzyQ5OwkR9Zap5ZSdi+lzOoxb9ckt5RSHk9yQdfn2KYPGTVyTpJOB+EhhyTz5vVV+QAAAADQLs+0xLhP1VofSfKvvYxfkc4mJguPl6tz8eh3P5H3ndD5/qMfJWPGJGeckQwb6DgUAAAAAFZwrYzMjj5yZo4+uvv4299O3vOepNrcGAAAAACeppUBYfnVzvnCSdPzth59iKedlrz73UJCAAAAAOiplQFhkgy7/DU5/dSZmTy5e0xICAAAAABP19qAMI9OyfDfTs6Z354vJAQAAACAxWhvQJgkfzs/I275QM48M0JCAAAAAOhF+wLC0c9PNj28+/j2L2bEPV/pNSQ86qhkwYKBLxEAAAAAVhQjBruAPrXmS5L9r0uGjUieeDj5y4864zccnRGrbZQzz3x1kuTsszvDX/1q8vjjyTe/mYxo118CAAAAABppVwfhsJHJ8FWSMix56ZnJ2jt2xuuC5Ko3ZsTMm3PmmcnBB3dfctZZyRvekDzxxOCUDAAAAACDqV0BYU8jRid7/G/yrA06x/NmJZe9JiPm3pczz0ze8Y7uqeeem+y3X6ebEAAAAACGkvYGhEkyer1k0vnJiLGd49l/SS7bP8Pr7Hz968kHPtA99Ve/SvbeO3n00cEpFQAAAAAGQ7sDwiRZ88XJbj9MyvDO8SPXJde8JSUL8rnPJSee2D31qquSvfZKHnhgcEoFAAAAgIHW/oAwSZ6/dzLxv7uP//LjZMqxKSU5/vjklFO6T910U7LLLslddw18mQAAAAAw0IZGQJgkmx2ZbPG+7uM/fia5+4wkyTHHdHYyHtb117j77uSlL02uvXYQ6gQAAACAATR0AsIkmXByMm6/7uNrj0juuyRJ8ra3dTYrGT26c+qhhzrLjX/+80GoEwAAAAAGyNAKCIcNT3b5XvLsCZ3jOi+5/LXJIzcmSfbfP7nkkmTttTunZ89ODjggOf30QaoXAAAAAPrZ0AoIk2TkmGTPnyWjx3WO581MfrNP8tgdSTpLi6+6Ktlww87pBQuSd70rOeGEpNbBKRkAAAAA+svQCwiT5Fnjkr0uSlZ5duf4iQeTX78imT09SbLFFsk11yQTJnRfcuKJycEHJ//4xyDUCwAAAAD9ZGgGhEmy5tbJpAuS4c/qHM/+c/KbvZMnHk6SrLdectllyStf2X3JOeckkyYlf//7wJcLAAAAAP1h6AaESbLOzsnuP0mGjewcz/hjcum+ydxZSZKxYzublBxxRPcl116b7LhjctNNg1AvAAAAAPSxoR0QJsnz905eelaS0jl++HfJFQcm859IkowcmZx6avLf/50M6/prTZ+e7Lpr8tOfDk7JAAAAANBXBIRJ8sI3Jjt8pfv4vl8lV70xWTA3SVJK8p73JBdckKy+emfK7NnJgQcm//mfNi8BAAAAYOUlIFxosyOTl3yq+3j6/yZXH5IsmPfU0N57dzYv2XjjznGtybHHJq9/fTJz5gDXCwAAAAB9QEDY09YfS7b6UPfxn3+YXPPWZMH8p4Ze9KLkd79L9tyze9qPf5zstFNy++0DWCsAAAAA9AEBYU+lJOM/k2xxTPfYvd9Lrn1HUhc8NbTOOskvf5m8973d0269NdlhB+8lBAAAAGDlIiBcVCnJdl/oLDle6J5vJ9cd+bSXDa6ySvKlLyVnnZWMHt0Zmzkzee1rk499LJk/PwAAAACwwhMQ9qaUZOKXk03e3j1219eTG47+px1J/u3fkquvTjbcsHvsP/4j2Wef5P77B6ZcAAAAAFhWAsLFKcOSHb6WbPjm7rE7vpxcd9TTlhsnyfjxyQ03dDYxWejiizvjv/71ANULAAAAAMtAQLgkw4YnO38z2eCN3WN3fTX53TuetnFJkqy1VnL++clxx3UaEJPkvvuSl788OeEES44BAAAAWDEJCJ/JsBHJLt9JXnhw99g930p++9ZkwbynTR0+PPnUp5Jf/CJ5znM6Y7UmJ56YvOxlyV//OoB1AwAAAEADAsImho1IXnpmsvGh3WPTvptcNTlZMPefpr/ylclNNyV77dU9dtllnSXHF17Y/+UCAAAAQFMCwqaGDU92OiPZ9PDusb/8KLnioGT+E/80/XnPS371q+STn0yGdf2VH3ooefWrk/e8J5k9e4DqBgAAAIAlEBAujTIs2eG0ZPOju8f+el5y+QHJvMf/afrw4cnHP55cckknMFzoK19Jtt++s7EJAAAAAAwmAeHSKiXZ/pRkqw91j/39ouTXr0ieeKTXSyZNSqZMSQ44oHvsttuSnXdOPv3pZN68Xi8DAAAAgH4nIFwWpSTjP5O8+OPdYw9dk1y8RzK7951I1l03Offc5BvfSFZbrTM2b15y/PHJHnskd989AHUDAAAAwCIEhMuqlOQln0y2/2L32Iypya92TR67c7GXvP3tnW7CXXbpHr/mmmTbbZNTT00WLOjnugEAAACgBwHh8tri6OSl30nKiM7x4/d2QsJHfr/YSzbZpLOr8ac/nYxYeNnjybvfnfzLvyR33TUAdQMAAABABIR9Y6NDkj3PS4aP7hw/8WBy8Z7J/Zcu9pIRI5KPfSz57W+TrbbqHr/ssuQlL0m+8IVk/vz+LRsAAAAABIR95fmvSv7l4mTkmp3jeTOT3+ydTPveEi/bfvvkxhuTY4/t7HqcJP/4R/Lv/57stlty6639XDcAAAAAQ5qAsC+tu0vyiiuS0c/vHC94Mrn6kOQPJyW1LvayUaOSk05Krr220z240G9/m4wf3zn35JP9XDsAAAAAQ5KAsK+t+eLklVcna2zdPXbzccnv3pEsmLvES7fbLrnuuuTEE5ORIztjTz6ZHHdcJyi87LJ+rBsAAACAIUlA2B9We2HyiiuT5/5L99g930wu3Td5csYSL11lleT44zvLjnfYoXv81luTSZOSQw9NHnywX6oGAAAAYAgSEPaXVdZMJl2YbPTW7rH7fpVcvHvy+F+e8fIXvzi5+urklFOSMWO6x//nf5Ittki+8Y1kwYJ+qBsAAACAIUVA2J+Gr5Ls/K1km092jz16S3LRjslDv3vGy0eMSI45JrnttuSgg7rH/+//kne+s7OJyZQp/VA3AAAAAEOGgLC/lZJs8/Fk5/9JhnW9WHDOfcnFeyZ/OqvRLcaNS374w+SCC5KNNuoev+aaznsLjzoqeeihfqgdAAAAgNYTEA6Ujd+S7HVRsspaneMFTyTXvCX5/YeTBfMb3eJVr0qmTk0+9rHuTUwWLEhOOy3ZbLPkS19K5i55HxQAAAAAeBoB4UB67l7J3tcma7yoe+zWzyWX7feMm5csNHp08ulPJzffnLzyld3jjz7aWY687bbJRRf1cd0AAAAAtJaAcKCN3SR55TXJuP26x/5+YfLLnZLH7mh8my23TH7xi+S885JNN+0ev/XWZJ99kv33T+68sw/rBgAAAKCVBISDYeTqyR4/TbY+tnvssds7m5f89eeNb1NKst9+yR/+kHz2s8nYsd3nfvaz5EUvSt7znuSBB/qwdgAAAABaRUA4WMqwZNuTkl2+lwwf1RmbO6Oz3HjKcY3fS5gkq66afOhDyR13JIcd1gkOk2TevOQrX0k22SQ58cRk1qx++B0AAAAArNQEhINtw8nJK65MnrVB99jUk5JL90nmPLhUt1pvveSMM5Lrrkv22KN7fNas5IQTOkuRTzvNRiYAAAAAdBMQrgjW2j7Z54ZkvR67jtx3cfKL7ZOHfrfUt9t+++TSSzvLjLfeunv8/vuTo47qjP3oR0mty186AAAAACs3AeGKYtQ6yaQLkhcf3z02+y/Jxbsnd5621GleKclrXpNMmdLpKhw3rvvcnXcmr399MnFi8vOfCwoBAAAAhjIB4Ypk2PDkJScme/48GblmZ2zB3OS6o5KrD0nmPrbUtxw+vPNewjvvTP7rv5I11ug+d+ONnU1Odt45uegiQSEAAADAUCQgXBGN2zd51Q3Js8d3j917dnLhhOTh65bplqNHJx/5SHL33ckHPpCMGtV97tprk332SXbbLbnkEkEhAAAAwFAiIFxRjdk4ecXVySbv6B6bdU/yy12SW09O6oJluu3aaycnn5zcc09yzDGdHZAXuvrq5OUvTyZN6rzDUFAIAAAA0H4CwhXZiNHJTqcnu5ydjBjbGavzkt9/KLl032TOA8t86+c9LznllE5H4bvfnYwc2X3u8suTvfZKdt89ueACQSEAAABAmwkIVwYbvil59U3JWjt0j/39F8kF2yb3XbJctx43Lvnyl5O77kre9a5kxIjuc1ddley7bzJhQvKDHyTz5y/XowAAAABYAQkIVxZjNk5ecWWy1Ye6x+bcl/z65cmNH0jmz1mu22+wQfK1ryV33JG84x1P7yicMiV54xuTrbZKvvnN5Mknl+tRAAAAAKxABIQrk+GrJBM+m0z6RbLqut3jt30++cXE5JHfL/cjNtooOf30zjsK3//+5FnP6j53553J29+ebLJJZ3nyzJnL/TgAAAAABpmAcGX0/L2TV09Jnrd399iMqckvd0qm/keyYN5yP2L99ZPPfz6ZNi057rhkjTW6z02f3gkPX/CC5MMfTv7yl+V+HAAAAACDREC4shr9vGTShckOpybDu9r8FsxNpnwsuXiPZOZdffKYdddNPvWp5M9/Tv7rv5LnPKf73IwZyec+1+k6PPjg5IYb+uSRAAAAAAwgAeHKrJRksyOTV92UrL1T9/hD13Q2MLnzq322BfHqqycf+Uino/C005LNN+8+N39+cvbZycSJyZ57JuedlyxY0CePBQAAAKCfCQjbYPXNOhuYvORTSenahnj+7OS6I5NfvyyZeXefPWr06OSII5Jbb+0EgZMmPf385ZcnBxyQbLll5z2Fjz7aZ48GAAAAoB8ICNti2Ijkxccle/82WX2r7vH7f5NcsE1y2xeSBfP77nHDkv32S37zm87S4kMOSUaM6D5/552d9xSOG5e8612dnZABAAAAWPEICNtmre2TfW5ItvpwUrr+eef/I7nx35Nf7ZbM+GOfP3K77ZLvfCf50586m5b03NBk9uzOrsjjxye77dZZivzkk31eAgDw/7d352FyXYWd97+nel+0y5Jl2ZK8yJZtwDskBoPNZngJAUIYHJZAMgQSJkwyDEmYPMBLCBmGIe+beXleCPAO+xIS9oADIYDBZgs2Nt5tybtWW7vUi3qr8/5xqlTV1VXV1d3VVd1d38/znOfeOvfce0/Jupb6p3PPkSRJkmbJgHApau+BS94Pz/85rHxyof7gz+Hbl8Bd700LmtTZ6afD+98Pu3fDRz8KT37y5OM/+UlazGTTJnjnO139WJIkSZIkaSEwIFzK1lwB194CT343ZDpSXXYU7ngnfOdy2P+zebltX1/hteIbb4RXvnLy68ePPw7vfS9s2QIvehF8/eswVv+8UpIkSZIkSTUIsU6r3C4El19+ebzlllua3Y2F6chd8PPfh0M3T64/541w0fuga/W83n7v3vSq8Uc/Cnv2TD2+fj28/vXwhjfAOefMa1ckSZIkSZJaUgjhlzHGy0vrHUHYKlY+CZ7/U7jkA9DWU6h/4GPwrW3w0GdgHsPiDRvgXe+CRx6BL30JnvOcyccffzy9nrx1K1xzDXzhC3DixLx1R5IkSZIkSTmOIGxFA4/ALW+BPd+aXL/uarjiw7Di/HJn1d1DD8HHPw6f/GQaYVhq1ao0Z+HrXgeXXw4hNKRbkiRJkiRJS1KlEYQGhK0qRtj1DfjlW2BoV6E+0wHb3gYX/iV09DekK+Pj8O1vp1eQr78estmpbc47D373d+E1r0mLnEiSJEmSJGlmDAhV3tgA3PVXcN/fQZwo1PdshIvfD1te1dChe7t3w6c/Df/7f8PDD5dvc8018NrXwstfDsuXN6xrkiRJkiRJi5oBoao7fAfc/IdwoGRl47VXwuUfhNWXNbQ72Sz88Ifw2c/Cl78MAwNT2/T0wMtelsLC5z538krJkiRJkiRJmsyAUNOLWXj4M/Crt8OJx4sOBDj79+Gi/w7d6xrercFB+MY34DOfgX/7t/KvIJ9yCvz2b8MrXwlXXQUZl9+RJEmSJEmaxIBQtRs7Bne9F+7/X5AdK9R3LIcnvQvOfQu0dTala3v2pBWOP/MZuPPO8m1OOw1e8Qq47jp42tNc3ESSJEmSJAkMCDUbx7bDrW+FPddPru8/Cy56H2x6RVPTt9tvT0HhF7+YgsNyNm9Oowqvuw4uvtiwUJIkSZIktS4DQs3enm/DL/8Ujm+fXL/mqXDJB2DdM5vTr5xsFn784xQUfvnLsH9/+XZbt6bXkH/rt+CyywwLJUmSJElSazEg1NxMjMKOD8Fdfw2jhycf2/ibcPH/gBXnN6dvRcbH4YYb4B//Eb7yFThypHy7TZvSAicvexk84xnQ1tbYfkqSJEmSJDWaAaHqY/Qw3P0+uP+DkB0p1IcMnP0GePJfQc+pzetfkdHRtKjJF78IX/96+ZWQIS1w8pKXpJGFz342dHU1tp+SJEmSJEmNsCACwhDCauDjwPOBA8B/izF+oUy7PwNeB2zOtftwjPED013fgLCBBh+F298Bj3xucn1bL5z3Fjj/z6FrdXP6VsbwMHz3u/DVr8I3vwmHD5dvt2wZ/MZvpJGF114Ly5c3tp+SJEmSJEnzZaEEhP8AZID/CFwMXA9cGWO8u6TdnwPfA+4Azga+C/xFjPGL1a5vQNgEh26D2/4MHv/+5PqO5bDtrbDtv6T9BWRsDH70oxQWfu1rsG9f+XYdHfDMZ6bA8MUvhrPPbmw/JUmSJEmS6qnpAWEIoQ84DDwpxrg9V/dZYHeM8e3TnPtBUl/fUq2dAWGTxAh7vwu/+gs4cvvkY52r4YK/gHP/E7T3Nad/VWSz8POfp6Dwq1+Fhx6q3HbbtkJYeOWV0N7euH5KkiRJkiTN1UIICC8BfhJj7C2qexvwrBjji6ucF4BbgY/GGD9S5vgbgTcCbNq06bJHH3207n1XjWIWdn4F7ngXHLtv8rHu9XDhX8I5b4K2hTnJX4xwxx2F15Bvu61y25Ur4YUvTGHhC14Aq1Y1rp+SJEmSJEmzsRACwquAL8UYTy2q+wPg1THGq6uc91fAS4GnxhhHKrUDRxAuGNkJePQLcOe7YaBkSF7PRrjgz9OCJu29ZU9fKHbvhuuvh299C773vTSPYTmZDDztaWnOwmuvhSuucFVkSZIkSZK08CyEgLDcCML/ClxdaQRhCOGPgf8KXBVj3DXdPQwIF5jsGDz0Sbjrr2Go5D9f9zrY9jbY+kfQ0d+c/s3A8DD84AcpLPzWt2BXld+Nq1bB855XCAw3bmxcPyVJkiRJkipZCAFhfg7CC2OMO3J1nwH2lJuDMITw+8B7gGfGGKvMDFdgQLhATZyABz4Gd78PTpSsCNK5Oi1kcu5boHNFc/o3QzHC7benoPCb34Sbb051lVx4YXoN+dpr4aqroLu7cX2VJEmSJEnKa3pAmOvEF4EIvIG0ivG/UH4V41cD/xdwTYzx3lqvb0C4wI0Pw4Mfh3vfP3VEYceKFBJu+1PoWtOc/s3SgQPpFeR//ddU9u6t3LanJ4WEz3kOPPvZcMklvo4sSZIkSZIaY6EEhKuBTwDPAw4Cb48xfiE3P+G3Y4z9uXYPA6cDxXMOfi7G+IfVrm9AuEhMjMLDn04jCgcfnnysvS/NT7jtv0Df5ub0bw5ihDvvLISFN90Eo6OV269cCVdfncLC5zwHzj8fQmhYdyVJkiRJUgtZEAHhfDMgXGSyY/DIP8DdfwPHt08+Ftpg03+A8/8MVl/SnP7VweAg/OhH8J3vpMBw+/bq7U89NYWF+cBwy5aGdFOSJEmSJLUAA0ItXNkJ2PnlFBQeuXPq8fXPSUHhhucv+uF1jzwCN9wA3/9+WvSk2uvIAGeeCddcA896FjzzmbB586L/JZAkSZIkSU1iQKiFL0bY+x249wPw+A1Tj698Cpz/Nth8HWQ6Gt+/OosR7rsvBYU/+EEKDg8frn7O6aenoDBftm0zMJQkSZIkSbUxINTicuiXcM8HYOeXIGYnH+s5Dba+Gc55I3Sf0pz+zYOJibQ6cn504Y03wtBQ9XPWrk2LnuQDw4suctETSZIkSZJUngGhFqeBh+G+v0urH0+UpGWZLtjyKjjvP8Oqi5vTv3k0Ogq/+EUKCm+8EX7yExgYqH7O8uXw9Ken0PDXfx2uuAL6+hrTX0mSJEmStLAZEGpxGzkIO/4etv+/cOLxqcdPuQrO+xM4/SWQaW98/xpgfBx+9asUFt50U9oeOlT9nLY2uPjiFBZeeWUqmzb5WrIkSZIkSa3IgFBLw8QIPPYluP//gUNl/lv3ngHn/ic46z9C99rG96+Bslm4997CCMMbb4Q9e6Y/b8OGFBTmQ8NLL4WurvnvryRJkiRJai4DQi0tMcLBf4f7P5gCwzg++XimEza9As55E5zyjJYYMhcjPPxwCgp/+tNU7r57+vM6O+Gyy1JY+Gu/Bk99KpxxRkv8kkmSJEmS1FIMCLV0De2GHR+BBz4KI/unHl9xQQoKz/xd6FzZ+P410ZEj8O//nsLCn/0Mfv5zOH58+vPWrUtB4RVXFLZr1sx/fyVJkiRJ0vwxINTSN3ECHv0ibP8wHLp56vG2Hth8XQoL1zy1JYfITUykUYU/+1khNNyxo7ZzzzprcmB46aUugCJJkiRJ0mJiQKjWcujWNKLwkc/D+ODU4ysvgq1vgs2/03KjCkvt359GFv70p2nV5FtugWPHpj8vk4ELLywEhpddBk96EnR3z3+fJUmSJEnSzBkQqjWNHYNHvpBeQT5y+9Tjbd1w+svgrN+D9c+GTFvj+7jAZLOwfTvcfHMKDG++GW67DUZHpz+3vT2FhpdckkYYXnopXHQR9PfPf78lSZIkSVJ1BoRqbTHCwV/AAx9JryFPnJjapvcMOPN1cNbrYdnZDe/iQjY6CnfcMTk0vOee9Ms6nRDgvPNSWJgPDi+5BFatmv9+S5IkSZKkAgNCKW/0MDz8OXjok3D4tvJt1j0rjSrc9NvQ7kR75Rw/DrfeWngt+bbbap/PEODMMwth4SWXwFOeAhs3tuTUkJIkSZIkNYQBoVTO4V/Bg5+ERz8PIwenHm/vh02vgC2vhnVX+wryNI4ehdtvT8Fhvtx7b3ptuRarV6egMF8uugguuAB6e+e335IkSZIktQIDQqmaiRHY/a00qnDvtyGWSbR6TkurIG95Nay6xKFuNRoagjvvnBwa3nknjI3Vdn4mA1u3FgLDfHi4aZP/CSRJkiRJmgkDQqlWw3vh4c+msPDYfeXbLN8GW14DW14F/Wc2tn9LwOgo3H13ei35l79M8xvecUdtqyfnrVgxebThhRemsrK1F6WWJEmSJKkiA0JppvILmzzy+bSwycj+8u3WXplGFW76D9C9trF9XEJihMceS0Hh7bcXQsMdO2p/RRngtNMKYeGFF6ZXlC+8MAWKkiRJkiS1MgNCaS6y47Dveyks3PU1GB+c2ia0wfpnpzkLT3+ZYWGdDA2l0Yb5wDAfIB4+PLPrbNw4OTjMh4fLl89PvyVJkiRJWmgMCKV6GR+EXf+cwsK934E4MbWNYeG8ihF27y4EhnfemULE++6DkZGZXeuMMwqjDC+4ALZtS2XNmvnpuyRJkiRJzWJAKM2HE/vhsX+CR74AB35avo1hYcOMj8NDD6WwsLjcf3+a93Am1q6F884rBIb5smULtLfPS/clSZIkSZpXBoTSfBvaBY99GR770vRh4Rkvh9N/E3o2NLaPLWp8HB54AO65Z2pwWOtqynkdHWlV5dLg8LzzfF1ZkiRJkrSwGRBKjTS4E3Z+pXpYCLDm1+CMl8LpL4Xl5zWufwJSOPjAA5MDw/vuS9uhoZlfb8OGQmC4dWuhnHkmdHbWv/+SJEmSJM2EAaHULCfDwn+CAz+r3G75thQUnv5SWHMFhEzj+qhJslnYtSuFhfmSDw/37Jn59TKZ9Gry1q1wzjmTw8MtW9KoREmSJEmS5psBobQQDO6EXV9PKyE/cWP5BU4gvXq88SUpLFx/NbR1NbSbquzYsUJYWBwc7tgx83kOAdra0gjD4tAwHyJu3ux8h5IkSZKk+jEglBaakYOw+/oUGO79DkwMl2/X3g+nPhdOexGc9n9A72mN7adqMj4OjzxSCA137CiUnTtnd82OjkJ4ePbZcNZZhXLmmdDbW9evIEmSJEla4gwIpYVsfAj2fS+Fhbv/OYWHlay6JqAfFQAAIABJREFUJIWFG18Eq6+ATFvj+qlZGR6GBx8sBIYPPFDY37179tfdsGFyaFhcNmyAEOr3HSRJkiRJi58BobRYZMfTwiY7vw67vwEDD1Vu27UWNrwwhYUbroXOlY3rp+picHByeFgcIu7dO/vrdndXDg8dfShJkiRJrcmAUFqMYoTj29OryHuuz81bOF6+bWiDtVemoHDD82HVpY4uXOQGBgqjDR9+GB56KJUHH4RHH4WJClNY1uLUU1NQuHlzWihly5bC/ubN0NNTpy8hSZIkSVowDAilpWDsGOz9txQW7vkXOPF45badq9PchRueD6c+H/rOaFw/Ne/Gx9PchvnQsLg8+CAcPjy3669bNzU0LN7298/9O0iSJEmSGsuAUFpqYhYO3ZrCwt3Xw6Gbq7dffn4hLFz/LGjva0w/1RSHD08edVjP0YcAa9ZMDQ3z+5s3w0rfdpckSZKkBceAUFrqhh9PC53s+y7s/S6c2Fe5baYDTnlGLix8Nqy+FDLtjeurmio/+vDRR9PKy/ltfn/nztRmLpYtgzPOmFo2bSrs+xqzJEmSJDWWAaHUSmKEo3eloHDvd2H/jTBxonL7juVwyjPh1GfD+mtg5VMgZBrXXy0oExOwZ8/k8LA4RHzsMRgdnft91qwpHxzmy8aN0NEx9/tIkiRJkhIDQqmVjQ/D/ptSWLjvu3DkzurtO1fD+qth3TUpNFx+PoTQkK5q4ctmYd++qeFh8XZ4eO73CQE2bCg/EnHjxlQ2bDBElCRJkqRaGRBKKhjemwsLvw9P3ABDu6q3716fRhauvyaFhsvOMTBURTHCoUPpVeWdO9OIw/x+vuzaNffXmCH9Nly3rhAYViorVvhbVpIkSZIMCCWVFyMcfyAFhft+kLYnnqh+TvepsO4qOOWqtF3xZMi0Naa/WhImJuDxxyeHhqVB4r596bdnPfT2Tg4MTzttaojoaERJkiRJS50BoaTaxAhH74HHb4DHfwBP/BBGD1c/p2M5rH16ITRccwW0dTWku1q6Rkdh9+6pow937kz1u3fDE0/UL0QsNxpxwwY49dS0zZf166HdNX0kSZIkLUIGhJJmJ2bh8O2FwHD/j2HsaPVzMl2w5qmFwPCUK1OIKNXZ2Bjs3VsIDCuVesyJmBcCnHLK5NCwXJC4YYMrNUuSJElaWAwIJdVHdiKtkPzETWnhk/03pTkNqwmZ9Bry2l8vFOcxVIPECEeOFMLCPXvKh4j1HI2Yt2LF9CHihg3OkShJkiSpMQwIJc2PGGHgwUJg+MRNMPDA9Od1rYU1vwan5ALD1VdAR//891eqoHQ04p496XO+7NuXtvv31//e3d0pPFy3Lr3CXK2sXGmYKEmSJGl2DAglNc7w3vQqcj40PHw7MM3/a0IGVj5l8ijD/rNNQrTgjI2lBVZKg8PSsm9ffVZqLtXRUVuQuG4drF0LmUz9+yBJkiRpcTIglNQ8Y8fg4C9g/8/gwM/g4M+nX/gEoOsUWPO0tOjJmivSKMPutfPfX6kOslk4dKh8cFhaNzg4P33IZNJ8idVCxHXrUptTTkkjGSVJkiQtXQaEkhaOmIVj21NYmA8Mj9zFtKMMAfq2FMLCNVfA6ktdAEWL3vHjaVRitfLEE2l7/Pj89aO/vxAWFpfiELG49PXNX18kSZIk1Z8BoaSFbbajDAmw/LxCYLjmClh1MbQ5FEpL09BQISycrhw5Mr996emZPkQsLsuWOWuAJEmS1EwGhJIWl/wow0M3w8FcOXwbZEemPze0w8onw+rL0wjDVZekz+29899vaQEZGSkfJhbX7d+fyoED8zNnYrGurqmh4Zo1qaxdO3mb3+/pmd8+SZIkSa3EgFDS4pcdS68iF4eGR++CODH9uSEDy7fBqlxguPqStO1cOf/9lhaBGNOIw3xguH9/ChKLP5eW0dH571dPz9TQsFyQWLzt73ekoiRJklSOAaGkpWl8CA7/KoWF+eDw+Pbaz+87sxAWrrokjTjs2TB//ZWWiBjTfIi1hon798PwcGP61tk5s0BxzRpYscIVnyVJkrT0GRBKah2jR+DQL+HQrem15MO3pteVa1kEBaB7fS4wvBhWPiWV5edCpmNeuy0tdYODk8PEAwfg4MHCttz+2Fhj+hYCrFoFq1cXtsWlXF2+vrOzMX2UJEmS5sqAUFJrGxuAI3ekwDAfHB69K722XItMJyw/P4WFq55SCA671/suozRPYoSBgfIBYrW6Ro1UzOvrqz1MLP7c1+f/PiRJktRYBoSSVGpiFI7dUzTS8Lb0uvL4YO3X6FpbCAvz4eHyC6DdlRWkZhkaqjwisVzdoUNw7Fjj+9nRUXm04qpVsHJlYVu67zyLkiRJmg0DQkmqRXYCBh6AQ7fB0Tvh8B1p5OHQY7VfI2Rg2dYUGK64MFcuSHW+piwtSGNjaZGWQ4fg8OG0LS2V6rPZxve3ra16gDjdfldX4/ssSZKk5jMglKS5GD0CR+5MYWHxdnyg9muE9jSX4YoL0yjDlbntsq3Q5iRm0mKUX6ylXHA4XdjY6Fehi3V3Tz9KsdL+8uXQ3t68vkuSJGn2DAglqd5iFgYfKYwyzAeHx3dQ84IokILDZVsLIw1Pjjg81+BQWsKGh6cGiIcPp1efjxwplMOHp+43M1yENH/iihVzK4aMkiRJjWdAKEmNMj4ER+9OgeHRe9L+0Xtm9poyQGjLBYcXwPJtsOw8WJ4rnSvnp++SFoWRkelDxHJ1+f2JiWZ/A+jtnXvI2OGsDZIkSTNiQChJzTZ2HI7emwLDY/fAkdx28NGZX6t7fS4sLAkO+7ZAxmE5kiqLEQYHpw8RK+03Y0GXSnp6qgeIy5alV6Lzpfhzfn/ZMoNGSZLUOgwIJWmhGjsOx+6bPNrw6N3p9eWZynTCsnNyoeG2QnC4/DzoXFX3rktqPdlsmnfx6NHZl2PHUlC5UPT0lA8PqwWL5T739UEm0+xvI0mSVJkBoSQtNuODacThsfty5f60Pb4DsiMzv173ujSv4bKtuRBxK/Sfk/Y7ltW//5JUQTYLAwNzDxmbsYJ0NSFAf//0wWKlkHHZslT6+1PYGEKzv5EkSVpqDAglaanITqT5DE+Ghvng8H4Y3ju7a3avLwkN8yHiOdCxvL79l6Q6iLF6yHj8eAoRjx2bvF/6+fjxhRc0QgoH+/oKgWHxtlzddNtly1wYRpIkGRBKUmsYO1YUGhYFh8e2z27UIeRGHpYJDpdtNTyUtOjFCENDlcPDWkPGY8fS3I4LWVfXzEPFam16ehzlKEnSYmNAKEmtLD/q8PgOOP5AYTuwAwYeguzY7K7btRb6z4K+M9O2/yzoz+33nuGCKZJaysRECgtnEzLmPw8MpO3wcLO/zfQymfLBYV/f1G25ukpturoMHiVJmi8GhJKk8rITMLQzhYYDD8Cx3PZ4Pjwcnd11Qxv0bpoaHOYDxa41/gQoSRVMTKSwMB8YFoeHlbbTtVmIr1KXkw8eZxs0Vmrb2+siMpIkGRBKkmYuOwHDu6aOPDy+AwYenH14CNC+bHJweHIU4pnQtxnae+v3PSSpxcWYRiXWI2jMtxmdwx8BzdLbO/PAMX9Ob+/k/dKtczxKkhYDA0JJUn3FLAzvSaMMBx7ObR+Cwdz+bBdMyetam4LCvs3Qu7mwny+dqxyBKElNNDo6dZTj4GAqAwOTt+XqKh0bm+WsF83W0VE5PKy1rtox53yUJNWDAaEkqbHGh2HwkUJwOPAwDBbtjw/M7frt/SWh4ZbJn7vXQ/BdMklabEZHC8HhTILF6doPDTX7m81dvcLG4mP50tPj/I+S1AoqBYQOhJckzY/2HlhxfiqlYoSRA2WCw4dg8FEYfAziePXrjw/A0btTKSfTmeZALA4Ne8+A3tPTtu8MaO+b+/eUJNVVZ2cqq1bV97rZbAoJZxMs5s8rt83vN2KOx/z9DhyYn+uHkILCfGA43XYmbUu3hpGStLAYEEqSGi8E6D4llbVPm3o8OwEn9ubCwgplYpqhINnRtNjKwAOV23SsTEFhz+lpWxwg5rfOhShJS0J+8ZP+fli/vr7XjrEw8rFaiDjTY8VtTpyob58rfY/8/ebbTMPIuYSVhpGSND0DQknSwpNpywV0p8MpT596PEYYOQhDVQLE0UPT32fsCBw5AkfurNymc/XU0PDk9gzo3WiIKEktLoQUQnV1werV83OPiYm00MxsA8ZyxwYH0zWHh1NdI+d/bGYYmS/d3dX351rnqtmSFhMDQknS4hMCdK9NZfVl5duMHU+vKg8+WggSh3bB0E4Y3JlWZ87W8JPQ6KFUjtxRuU3XmjQKsXcj9JyWSvF+z8Y0WtI5ESVJs9TWVhgBOV/GxycHhsXbcnW1bsvVNXIV7EaGkcU6O+ceNM70HEdLSpotA0JJ0tLUsQxWXphKOTELJ/anoHBwZyE8HCraH95dW4g4cjCVI7dXbhPaoefUFBaWBojF+x0r/Ju9JKkp2tth2bJU5lt+RORcQsZat40MI4uNjqZy9Ghj7zuTcLG4dHXNvi5f397uX2OkxcqAUJLUmkIGetanUmkUYszCiScmh4ZTgsTd0y+oAqnN0K5UqmnrzYWGpxXCxEkh4gboPhU65nEIiSRJ86wRIyLz8mFkcWh44kShLr9fa12t5zTLiROpHD7c+HtnMvULHWdb19bW+O8tLQUGhJIkVRIyuVF/p8KaK8q3yU7AyBNpFOKJvSkwHN6TSvF+LXMiQlp8ZbrFVQDa+1NQ2HNqyXbD5M/d6yDjH/eSpNbVyDAyL0YYGalP0DiTumaNlszLrxbe6Ne5i7W31ydwzM8rOtOSP7ejw9GUWlz8iUGSpLnItKVQrmdD9Xbjw7kAcU96dblciDi0e/rVmU9eb6C2IJEAXWunBoflAkVfb5YkqS5CKAROK1c27r7Z7NTgsFI4mQ8w86X082zqxmt4qWK+jY+nMjjY7J7MPmQsFzjWo7hwjqoxIJQkqRHae6D/rFQqiRHGjuUCw925MLEoUBzaDSf2wfA+yI7UeOMII/tTocpCKwCZrpIAcX0qXevSSMSTZT10rnLRFUmSFphMJq3W3NvbnPuPjxfCwrkEjnM5P5ttzncvZ2QklYWio6N+YWO58LKzc+q20n5+67yVC4cBoSRJC0UI0LkilRXnV24XI4wdTUFhPjA8sbfkc77sB2Jt98+OpNWeBx+toa9t0HXK5NCwXJDYvS7Vt/fU1gdJkrRotben0tfXnPvHODmknEvgmA/3yn2upYzVsM5do42NpTIw0OyeFIRQCA9nEjDOJoyczfFWelXcgFCSpMUmBOhcmcqKbdXbZsdSSFgcHA7vLfmcCxjHZ/AuTpwohJC1aO8vhIU9JWFicV3XWuha47yJkiRpxkJIgU5HR2PnnCwnm01zQs4mXJxrOFnu/GbPT1lJfr7OhTTSslS5ALORAWXx/cuVeo3C9G/fkiQtZZmOtCJy72nTtx0bmBwYnniiUEaK9k88nkYwzsT4QPrn6oGHamvfuSoXFp6S266F7qL90vr2Za3zz7uSJGnBK17ReSGIMYWE8xlQ5q8/Olrb/shIWmV8ocv3eyGbLkQsLpUYEEqSpKSjHzrOgWXnTN92YiTNa1gcGk4JEnP1I0+kkYwzMXo4leM7amuf6SgfHlYMGNdCW9fM+iRJkrRIhVCYK3AhmZhIrz3XEibOJHisV9uF+Kp4OfUIMQ0IJUnSzLV1Qe/pqUwnP2fipNGIj5cZofg4jByAkUPUPG9iXnYsvTo9vLf2c9qXTQ4OO9dA1+qSbUmdIxUlSZLqpq0tlYUy0rJUfuTlTIPHuQaW+Xks88erlXqtHm5AKEmS5lfxnInLz52+fXYijR4c2Z8LDPPbA2k+xfx+cf1M5k/MGz+eyuDDM/gu7ZPDw87V5belde1NWk5SkiRJs1Y88nLZsmb3prxstvYwcXQUnvvc8tdpaEAYQlgNfBx4PnAA+G8xxi+UaXcN8C7gUuBwjHFLI/spSZKaKNMG3WtTqdX4cPngsFKgOHIgLbQyU3G8MOpxJtq6K4xQLNl2rppc2vscsShJkqSKMpn6vD7e6BGEHwJGgfXAxcD1IYTbY4x3l7QbBD4B/APwl43toiRJWnTae6D9DOg7o7b2MZt77bk4ODwIo4dKtgfTK8/57cTQ7Po3cQKGd6cyE5kO6Fg5NTispbT3Gy5KkiSpJg0LCEMIfcDLgSfFGAeAH4cQ/hl4LfD24rYxxl8AvwghVBj4KEmSNAchUwjS2Fr7eRMnKgSJuW2lkHGmi7TkZcdyAeb+mZ8b2nOvdq+aecjYsdxwUZIkqYU0cgThucB4jHF7Ud3twLMa2AdJkqTZa+uG3o2p1CrGNEfipNGIxaFicd3hySU7Mvu+xvHC69QzFTK5UHEldKxIJb9faVu639Y5+75LkiSpoRoZEPYDx0rqjgJzmuYxhPBG4I0AmzZtmsulJEmS6i8E6OhPpW/zzM4dH54cGI4dmRoiVioTw7Pvc8ymAHP00Oyv0dZTFBiuhM4KQePJYyVt2vtTUClJkqR518iAcABYXlK3HDg+l4vGGD8GfAzg8ssvj3O5liRJ0oLS3pNK72kzP3dipEqAmAsaxyocn82q0FPuPwzDwzC8d5YXCLmwsCQ8PLm/vFDal0/+XFza5jhjtyRJUgtoZEC4HWgPIWyNMe7I1V0ElC5QIkmSpLlq64KeU1OZqYnR3GjFo2kxl5P7ZbZjR8vXxewcv0DMXesI8OjsL5PpnBoaVgsUJ5WiILKtx3kZJUnSktWwgDDGOBhC+CrwnhDCG0irGL8EuLK0bQghA3QCHelj6AayMcbRRvVXkiSpZbV1Qts66F43u/NjhPGByuHhaJVgMX+sHqMYAbKjs5+LsVhom2HQuAzal6VXpTuW5T73p7q2bsNGSZK0oDRyBCHAm4FPAE8AB4E/ijHeHUK4Cvh2jLE/1+6ZwA1F5w0DPwKubmBfJUmSNBshFEKx3tNnd43sGIwdKwkUc9uxY9OX8WMpeIzj9flOcaLwCvZchbYUFHbkAsN8mFj6uVy4WO6zoxslSdIcNTQgjDEeAl5apv4m0iIm+c8/BPxbjiRJUqvKdEDXmlRmK8a0EvSUALHGkDEfNI4dg4kT9ftucaLo9ek6CJnJgeGUsLG/qL5K2Njel85r64NMW336JkmSFoVGjyCUJEmSGiOE9DpvW/fsX5fOmxiF8eM1BotHc20Hctvj6ZXr/H62zrPmxGzh3nNYvHqStu4UGLb350rfzLYdFY5lOurUQUmSVE8GhJIkSdJ02jqhbY4jGvMmRicHhqUBYunn8YHctsLn7Mjc+zSljydSGTlY3+tmOssEhxXCxPxoxuJtpbaZTl+zliRpDgwIJUmSpEZq64S21dC1uj7Xy44VhYbVwsTi0LHcCMfBwpZYn75N6esojI7WZy7HYqEtBYVtvbngsLewX1x38nOZY9Xat/X42rUkaUkzIJQkSZIWs0wHdK5KpR5ihInhqaFhfjs2ABO57ZTjFc7JH4vZ+vRxSp8nCq9Zz5e27tkHkJVCyeJjmS5HQUqSmsaAUJIkSVJBCLnwqre+180vGpMPDMsGjLVui8LK8YH6rVZdTf6169FD83P9kCkJFHvSyMX8CMb23HbSfo3tiuvaelOobBgpSSpiQChJkiRp/hUvGlOPuRyLZcdy4eEQTAwV7ee244O5+uL90mNV2kwM1be/5cRsIfycb/kwcrogsb1n7u0cGSlJi4IBoSRJkqTFLdMBnStTmQ8xm0YPlg0dpwkhS4PGSsFkdmx++l7p+zQqjCQXDBePejwZLuYC45P7s92WqXPFbEmaEQNCSZIkSaomZIpeu147P/fIjhUFh8O58HC4sD8xnAsZh0v2Z9puKM3Z2DCx0JdGCm21hYuZ7jQCMlOngNIVtSUtUgaEkiRJktRsmQ7oXJHKfMuOVQ4Si+uKQ8XZtsuOzv/3KSdO5EZrDjb4xqFKmNiVCyK7i/a7CkHlyf2ukjZl2he3K23vHJOSZsGAUJIkSZJaSaYjlY7l83+v7ESFIDG36MvEcIVtSV32RDq33HbK+cPzt2L2tJo0YnKSUGOgWC6MrBZYVmlf7voZ4wZpMfGJlSRJkiTNj0wbZPqho7+x982OVQ8cZxJOlttmKxxr5FySFcXC92hmd0JbldGQXel17Px+1bouaOusXFdcP11dpq2JvyDSwmZAKEmSJElaWk6OklzW2PtmJ8qMaiwOFkdyn0eKgsaRkmNF+8V1EyMl7SucG8cb+50riRO50aINWAW8VvnQslwQ2Zarn/e6kvs7b6UWCANCSZIkSZLqIdMGmT5o72teH7ITKTCsFi5WCiGLA8zsiZIwc4bnNu017yqKQ8uFMNgzL9NZEhrmRz6WK9WOFZW2ovZl66ud11XmmKMvlzoDQkmSJEmSlopMG2R6gd7m9SHGNJKxYrg4UrQdnVyXHYGJKnXTnV+xbqR5vx7TyY6mMj7Q7J5UFjJVQsVygWKlwLIkgKwp7Cxz/UrnhTZHZM6SAaEkSZIkSaqfECDkX/Nu8PyTlRSHlrMJF7OjM6+r5T4LYt7KGsRsIeRd0EIuLOyovg0dKWQM1drN5pwK21rOCZmm/soZEEqSJEmSpKWtOLRkgYSWkIK3SaHiaFGAWFImytSdPFbhnIrnjkx/zeJrE5v9K1WjWAhnF5vQVj6UrBRYzjbArMCAUJIkSZIkqRlCJre6c3eze1Jddry+oWXZ80ZqCyxPnj8y9X6LJsgsI07AxETTRmkaEEqSJEmSJKmyTHsqzZzbshbZ8fTadhxLIWIcywWI1ba5/RmdMzqL+4xOvldpXZMZEEqSJEmSJGnxOxlk9kBHszszAzGmEYTThYoTFcLFms7J1fN/l+2CAaEkSZIkSZLULCFAyIeb8618QNjcJVIkSZIkSZIkNZUBoSRJkiRJktTCDAglSZIkSZKkFmZAKEmSJEmSJLUwA0JJkiRJkiSphRkQSpIkSZIkSS3MgFCSJEmSJElqYQaEkiRJkiRJUgszIJQkSZIkSZJamAGhJEmSJEmS1MIMCCVJkiRJkqQWZkAoSZIkSZIktTADQkmSJEmSJKmFGRBKkiRJkiRJLcyAUJIkSZIkSWphBoSSJEmSJElSCzMglCRJkiRJklqYAaEkSZIkSZLUwgwIJUmSJEmSpBZmQChJkiRJkiS1sBBjbHYf6iaEcBy4v9n9kFRXa4EDze6EpLryuZaWJp9taenxuZaWns0xxlNKK9ub0ZN5dH+M8fJmd0JS/YQQbvG5lpYWn2tpafLZlpYen2updfiKsSRJkiRJktTCDAglSZIkSZKkFrbUAsKPNbsDkurO51paenyupaXJZ1taenyupRaxpBYpkSRJkiRJkjQzS20EoSRJkiRJkqQZMCCUJEmSJEmSWpgBoSRJkiRJktTClkRAGEJYHUL4WghhMITwaAjhVc3uk6TJQgh/HEK4JYQwEkL4VMmx54QQ7gshDIUQbgghbC461hVC+EQI4VgIYV8I4a21nitpfuWez4/n/uw9HkL4VQjhhUXHfbalRSiE8LkQwt7c87k9hPCGomM+19IiFkLYGkI4EUL4XFHdq3J/lg+GEL4eQlhddKzqz9rVzpW0uCyJgBD4EDAKrAdeDfx9COHC5nZJUok9wHuBTxRXhhDWAl8F3gmsBm4B/rGoybuBrcBm4Brgz0MIL6jxXEnzqx3YCTwLWAG8A/inEMIWn21pUXsfsCXGuBz4TeC9IYTLfK6lJeFDwM35D7mfmz8KvJb08/QQ8OGS9mV/1q7hXEmLyKJfxTiE0AccBp4UY9yeq/sssDvG+Pamdk7SFCGE9wKnxxhfn/v8RuD1McYrc5/7gAPAJTHG+0IIe3LHv5s7/tfA1hjjddOd2+jvJglCCHcAfwWswWdbWvRCCOcBPwT+BFiJz7W0aIUQrgN+C7gHOCfG+JoQwn8n/YPAq3JtzgbuJf05nqXKz9rVzo0xHm/w15M0R0thBOG5wHj+f1g5twOOIJQWhwtJzywAMcZB4EHgwhDCKmBD8XEmP98Vz53nPksqI4SwnvTn8t34bEuLWgjhwyGEIeA+YC/wL/hcS4tWCGE58B7grSWHSp/NB0kjBs9l+p+1q50raZFZCgFhP3CspO4osKwJfZE0c/2kZ7ZY/hnuL/pcemy6cyU1UAihA/g88OncaCCfbWkRizG+mfTMXUV6NXgEn2tpMftr4OMxxl0l9dM919V+1va5lpaQpRAQDgDLS+qWAw5plhaHas/wQNHn0mPTnSupQUIIGeCzpFEDf5yr9tmWFrkY40SM8cfA6cAf4XMtLUohhIuB5wJ/V+bwdM91tefW51paQpZCQLgdaA8hbC2qu4j0epOkhe9u0jMLnJyT6Gzg7hjjYdJrTRcVtS9+viueO899lpQTQgjAx0mTk788xjiWO+SzLS0d7RSeQZ9rafG5GtgCPBZC2Ae8DXh5COFWpj6bZwFdpJ+zp/tZu9q5khaZRR8Q5uYv+SrwnhBCXwjh6cBLSCMZJC0QIYT2EEI30Aa0hRC6QwjtwNeAJ4UQXp47/i7gjqIJyz8DvCOEsCqEsA34A+BTuWPTnStp/v09cD7w4hjjcFG9z7a0CIUQ1oUQrgsh9IcQ2kII1wK/A3wfn2tpsfoYKZC/OFc+AlwPXEuaHuTFIYSrcsH9e4CvxhiP1/CzdsVzG/nlJNXHog8Ic94M9ABPAP8A/FGM0X+NlBaWdwDDwNuB1+T23xFj3A+8HPgb0ippTwOuKzrv/yRNYv4o8CPgAzHG7wDUcK6keRRC2Ay8ifTDxr4QwkCuvNpnW1q0Iul14l2k5+9vgT+NMf6zz7W0OMUYh2KM+/KF9GrwiRjj/tzPzX9ICvueIM0f+Oai0yv+rF3DuZIWkRBjbHYfJEmSJEmSJDXJUhlBKEmSJEmSJGkWDAglSZIkSZKkFmZAKEmSJEmSJLUwA0JJkiRJkiSphRnyRzBFAAAEO0lEQVQQSpIkSZIkSS3MgFCSJEmSJElqYQaEkiRJapgQwrtDCHc1ux+SJEkqCDHGZvdBkiRJ8yCE8ClgbYzxN4r3G3TvLcDDwBUxxluK6vuBrhjjwUb0Q5IkSdNrb3YHJEmStHiEENqBiTjLf2WOMQ4AA/XtlSRJkubCV4wlSZKWuBDCu4HXAS8KIcRcuTp3bGMI4YshhMO5cn0IYWvxuSGEu0IIrw8hPAiMAH0hhBeEEG7KnXMohPCvIYTzi277cG57c+5+Pyy+XtH1MyGEd4YQdoYQRkIId4YQXlJ0fEvu/JeHEP4thDAUQrgnhPC8ojYdIYQPhhD25K6xM4TwP+r+CylJkrREGRBKkiQtfX8L/BPwPWBDrvw0hNAL3ACcAJ4F/DqwF/he7ljemcCrgFcAF+Xa9wH/C3gqcDVwFPhmCKEzd85Tc9sX5O73WxX69ifAnwF/ATwZ+Brw1RDCxSXt/gb4YO7+NwNfzL2uDPCfgZcB1wFbgVcC90//yyJJkiTwFWNJkqQlL8Y4EEIYBkZijPvy9SGE1wAB+L38K8MhhDcBTwC/QQoVATqB18YYHy+67FeK7xFC+D3gGCkY/DGwP3foYPE9y3gb8Lcxxi/kPr8rhPDMXP1ritr9XYzxm7l7/SXwu8DFuXttBrYDN+W+x2PAT6v/qkiSJCnPEYSSJEmt6zLS6MDjIYSBEMIAaSTgKuDsona7SsJBQghnhxC+EEJ4MIRwDHic9HfLTbXePISwHDgN+EnJoR8DF5TU3VG0vye3XZfbfooUFm4PIXwohPCiEIJ/z5UkSaqRIwglSZJaVwb4FenV3FKHivYHyxz/FrALeBOwGxgH7iGNNqyH0kVQxk4eiDGGECD3j90xxltzqyZfCzwH+DRwewjheTHGbJ36I0mStGQZEEqSJLWGUaCtpO5W4HeAAzHGI7VeKISwBtgGvDnGeEOu7lIm/91yNLctvedJMcZjIYQ9wNOB7xcdegYpbKxZjPE48GXgyyGETwE/B84hvXosSZKkKgwIJUmSWsMjwAtDCOcBB0mvEn+eNNffN0II7yLN3XcG8BLgIzHGHRWudRg4APxBCGEnsBH4AGkUYd4TwDBwbQjhEeBEjPFomWt9AHhPCGEH8EvSvINXAZfW+sVCCG8lLa7yK9JIw1eR5kPcVes1JEmSWplzs0iSJLWG/w+4F7iFtIDI02OMQ8AzgYeALwH3kV7PXUUKAcvKvbb7SuApwF3Ah4B3AiNFbcZJqwu/gTRn4DcqXO6DpJDwf+au9TLg5THG22fw3Y6TVkL+BWlU5MXAC3PfT5IkSdMIuQXrJEmSJEmSJLUgRxBKkiRJkiRJLcyAUJIkSZIkSWphBoSSJEmSJElSCzMglCRJkiRJklqYAaEkSZIkSZLUwgwIJUmSJEmSpBZmQChJkiRJkiS1MANCSZIkSZIkqYX9/0u0JZkWtetlAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1296x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's get predictions from our model for the training, validation, and testing\n",
        "# datasets\n",
        "y_hat_train = (predict(X_train, coeffs)>=.5).astype(int)\n",
        "y_hat_valid = (predict(X_valid, coeffs)>=.5).astype(int)\n",
        "y_hat_test = (predict(X_test, coeffs)>=.5).astype(int)\n",
        "\n",
        "y_sets = [ [y_hat_train, y_train],\n",
        "           [y_hat_valid, y_valid],\n",
        "           [y_hat_test, y_test] ]\n",
        "\n",
        "def accuracy_score(y_hat, y):\n",
        "    assert(y_hat.size==y.size)\n",
        "    return (y_hat == y).sum()/y.size\n",
        "\n",
        "[accuracies.append(accuracy_score(y_set[0],y_set[1])) for y_set in y_sets]\n",
        "\n",
        "printout= (f'Training Accuracy:{accuracies[0]:.1%} \\n'\n",
        "           f'Validation Accuracy:{accuracies[1]:.1%} \\n')\n",
        "\n",
        "# Add the testing accuracy only once you're sure that your model works!\n",
        "\n",
        "\n",
        "print(printout)"
      ],
      "metadata": {
        "id": "9HX5KVcyH9gz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations on training a logistic regression algorithm from scratch! Once you're done with the upcoming environmental science applications notebook, feel free to come back to take a look at the challenges ðŸ˜€"
      ],
      "metadata": {
        "id": "4zfXs8M8Osie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenges\n",
        "\n",
        "* **C1)** Add L2 Regularization to training function \n",
        "\n",
        "* **C2)** Add early stopping to the training algorithm! Stop training when the accuracy is >=90%\n",
        "\n",
        "* **C3)** Implement a softmax regression model (It's multiclass logistic regression ðŸ™‚)"
      ],
      "metadata": {
        "id": "VAa4bzT7PHRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a=np.array([1,2,6,4,7])\n",
        "b=a<4\n",
        "print(b)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRiGpaNC46Z-",
        "outputId": "a9e1210b-0078-4046-e387-e31d4a9ca270"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ True  True False False False]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LjyopYkn5MLI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}